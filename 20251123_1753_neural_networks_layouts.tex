\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}

% Color definitions from template
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}

% Additional colors
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}

\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}

% Clean itemize/enumerate
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]

% Reduce margins for more content space
\setbeamersize{text margin left=5mm,text margin right=5mm}

% Command for bottom annotation
\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

% Title page information
\title{Introduction to Neural Networks}
\subtitle{From Brain to Business: How Machines Learn to Predict}
\author{Neural Networks for Business Applications}
\date{\today}

\begin{document}

\begin{frame}[plain]
\titlepage

\vspace{0.2cm}
\begin{block}{By the End of This Lecture, You Will Be Able To:}
\small
\begin{itemize}
\item \textbf{Explain} how biological neurons inspire artificial neural networks
\item \textbf{Calculate} the output of an artificial neuron given inputs and weights
\item \textbf{Design} a simple multilayer network architecture for a business problem
\item \textbf{Trace} information flow through forward propagation
\item \textbf{Describe} how networks learn by minimizing prediction errors
\item \textbf{Evaluate} when neural networks are appropriate for business predictions
\item \textbf{Assess} the ethical implications of automated prediction systems
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[t]{The Prediction Challenge: Can We Predict Markets?}

\textbf{The Business Question:}
\begin{itemize}
\item Can we predict if a stock price will rise or fall tomorrow?
\item Traditional methods: Statistical analysis, expert intuition, rule-based systems
\item Challenge: Markets are \textcolor{mlred}{complex, non-linear systems}
\item Many interacting factors: price history, volume, sentiment, volatility, interest rates
\end{itemize}

\vspace{0.4cm}
\textbf{Why This Matters:}
\begin{itemize}
\item Better investment decisions ($\rightarrow$ higher returns)
\item Risk management ($\rightarrow$ protect capital)
\item Portfolio optimization ($\rightarrow$ balanced exposure)
\item Automated trading strategies ($\rightarrow$ scalability)
\end{itemize}

\bottomnote{Our journey begins with understanding how nature solved similar prediction problems}
\end{frame}

\begin{frame}[t]{What We Need: A Learning System}

\begin{block}{Requirements for Market Prediction System}
A system that can:
\begin{enumerate}
\item Process multiple inputs simultaneously
\item Learn patterns from historical data
\item Handle \textbf{non-linear} relationships
\item Improve predictions over time
\item Generalize to new market conditions
\end{enumerate}
\end{block}

\vspace{0.5cm}
\begin{alertblock}{Inspiration from Nature}
The human brain solves complex pattern recognition tasks every day by learning from experience.
\vspace{0.3cm}

\textbf{Can we mimic this for business predictions?}
\end{alertblock}

\vspace{0.5cm}
\begin{center}
\Large \textcolor{mlpurple}{$\blacktriangleright$ Let's explore how brains work...}
\end{center}

\bottomnote{Next: Understanding biological neurons as the foundation}
\end{frame}

\begin{frame}[t]{Nature's Computer: How Your Brain Makes Predictions}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Biological Neuron Structure}

\begin{itemize}
\item \textbf{Dendrites:} Receive signals from other neurons
\item \textbf{Soma:} Integrates incoming signals with weights
\item \textbf{Axon:} Transmits output to next neurons
\item \textbf{Synapses:} Connection points with varying strengths
\end{itemize}

\vspace{0.3em}
\textbf{Key Principle}

Neurons process multiple weighted inputs and fire when threshold is exceeded.

\column{0.48\textwidth}
\textbf{Business AI Insights}

\begin{enumerate}
\item \textcolor{mlblue}{Multiple inputs combined}\\
$\rightarrow$ Consider many market factors
\item \textcolor{mlblue}{Weighted connections}\\
$\rightarrow$ Some factors matter more
\item \textcolor{mlblue}{Non-linear activation}\\
$\rightarrow$ Threshold effects (tipping points)
\item \textcolor{mlblue}{Layered processing}\\
$\rightarrow$ Abstract reasoning emerges
\end{enumerate}

\vspace{0.3em}
\textit{We can create mathematical models that learn the same way!}
\end{columns}

\bottomnote{Next: See the visual comparison of biological vs artificial neurons}
\end{frame}

\begin{frame}[t]{From Biology to Artificial Intelligence}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{01_biological_neuron/biological_vs_artificial.pdf}
\end{center}

\bottomnote{The artificial neuron mathematically mimics biological signal integration and activation}
\end{frame}

\begin{frame}[t]{The Artificial Neuron: Mathematical Model}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Step 1: Weighted Sum}

Mimics soma integration:
$$z = \sum_{i=1}^{n} w_i x_i + b$$

\begin{itemize}
\item $x_i$: Input features (market data)
\item $w_i$: Weights (\textcolor{mlred}{learned from data})
\item $b$: Bias term (baseline adjustment)
\end{itemize}

\vspace{0.3em}
\textit{Like dendrites weighting signals differently}

\column{0.48\textwidth}
\textbf{Step 2: Activation Function}

Mimics axon firing:
$$y = f(z) = \frac{1}{1+e^{-z}}$$

\begin{itemize}
\item $f$: Activation function (non-linearity)
\item Output: probability between 0 and 1
\item Mimics neuron ``firing'' at threshold
\end{itemize}

\vspace{0.3em}
\textbf{Complete Formula:}
$$y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right)$$
\end{columns}

\bottomnote{Next: See a concrete example with real market numbers}
\end{frame}

\begin{frame}[t]{Single Neuron Computation: Step-by-Step Example}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{02_single_neuron_function/single_neuron_computation.pdf}
\end{center}

\bottomnote{With market inputs (price=100, volume=85, sentiment=120), the neuron predicts 100\% probability of price increase}
\end{frame}

\begin{frame}[t]{Activation Functions: Why Non-Linearity Matters}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Problem}

Without activation functions:
\begin{itemize}
\item Neural networks = fancy linear regression
\item Real business relationships are \textcolor{mlred}{non-linear}!
\end{itemize}

\vspace{0.3em}
\textbf{Three Common Functions}

\begin{itemize}
\item \textbf{Sigmoid:} Smooth (0,1) range\\
Perfect for probabilities
\item \textbf{ReLU:} Fast, efficient\\
Most popular in modern networks
\item \textbf{Tanh:} Zero-centered (-1,1)\\
Alternative to sigmoid
\end{itemize}

\column{0.48\textwidth}
\textbf{Business Non-Linearity Examples}

\begin{enumerate}
\item \textbf{Diminishing returns}\\
Doubling marketing spend doesn't double sales
\item \textbf{Threshold effects}\\
Sentiment must reach tipping point
\item \textbf{Saturation}\\
Engagement plateaus beyond certain point
\item \textbf{Network effects}\\
Value increases non-linearly with users
\end{enumerate}

\vspace{0.3em}
\textit{Activation functions let networks capture these patterns!}
\end{columns}

\bottomnote{Next: Visual comparison of these three activation functions}
\end{frame}

\begin{frame}[t]{Activation Functions: Visual Comparison}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{03_activation_functions/activation_functions.pdf}
\end{center}

\bottomnote{Each activation function has different properties: Sigmoid for probabilities, ReLU for speed, Tanh for zero-centered outputs}
\end{frame}

\begin{frame}[t]{The Limitation: Why One Neuron Is Not Enough}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What One Neuron Can Do}

\begin{itemize}
\item Draw a single straight line (hyperplane)
\item Separate \textcolor{mlgreen}{linearly separable} patterns
\item Example: ``Buy if price $>$ 100 AND volume $>$ 50''
\end{itemize}

\vspace{0.3em}
\textbf{Business Analogy}

One simple rule for decision-making

\vspace{0.3em}
\textbf{Limitation}

Only linear decision boundaries possible

\column{0.48\textwidth}
\textbf{What One Neuron Cannot Do}

\begin{itemize}
\item Complex, curved decision boundaries
\item XOR-like patterns: ``Buy when high price XOR high volume''
\item \textcolor{mlred}{Real-world market patterns!}
\end{itemize}

\vspace{0.3em}
\textbf{Business Reality}

Multiple interacting factors, non-linear relationships, conditional dependencies

\vspace{0.5em}
\begin{center}
\Large \textcolor{mlgreen}{\textbf{Solution: Multiple Layers!}}
\end{center}
\end{columns}

\bottomnote{Next: See the XOR problem that proves one neuron's limitation}
\end{frame}

\begin{frame}[t]{Visual Proof: The XOR Problem}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{04_linear_limitation/linear_limitation.pdf}
\end{center}

\bottomnote{Left: Linearly separable (one neuron works). Right: XOR pattern (one neuron fails, need hidden layers)}
\end{frame}

\begin{frame}[t]{Building the Network: Layers of Intelligence}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Multi-Layer Architecture}

\begin{itemize}
\item \textbf{Input Layer:} Raw market features\\
No computation, like sensory neurons
\item \textbf{Hidden Layer(s):} Pattern detection\\
Feature combinations, like association cortex\\
Learns ``high volume + rising price = momentum''
\item \textbf{Output Layer:} Final prediction\\
Probability output, like motor neurons
\end{itemize}

\vspace{0.3em}
\textbf{Result:} Buy/Sell decision

\column{0.48\textwidth}
\textbf{Hierarchical Learning Principle}

\begin{itemize}
\item \textcolor{mlblue}{Layer 1:} Detects simple patterns\\
``price rising'', ``volume high''
\item \textcolor{mlblue}{Layer 2:} Combines into complex patterns\\
``strong momentum'', ``weak support''
\item \textcolor{mlblue}{Layer 3:} Makes strategic decisions\\
``high probability buy signal''
\end{itemize}

\vspace{0.3em}
\textbf{Key Insight}

Each layer builds on previous layer's abstractions
\end{columns}

\bottomnote{Next: See the full network architecture with all connections}
\end{frame}

\begin{frame}[t]{Neural Network Architecture Diagram}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{05_network_architecture/network_architecture.pdf}
\end{center}

\bottomnote{5 inputs $\rightarrow$ 6 hidden neurons $\rightarrow$ 1 output. Total: 36 weights to learn from data}
\end{frame}

\begin{frame}[t]{Forward Propagation: How Networks Make Predictions}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Forward Pass Process}

\begin{enumerate}
\item \textbf{Input:} Feed market features
\item \textbf{Hidden Layer:}
$$a^{(1)} = \sigma(W^{(1)}x + b^{(1)})$$
Detects patterns like ``rising trend''
\item \textbf{Output Layer:}
$$y = \sigma(W^{(2)}a^{(1)} + b^{(2)})$$
\end{enumerate}

\vspace{0.3em}
\textbf{Efficiency}

All neurons in layer compute in parallel!

\column{0.48\textwidth}
\textbf{Concrete Example}

\textbf{Input:} price=105.2, volume=0.75, sentiment=0.62

\textbf{Hidden Layer:} Detects patterns

\textbf{Output:} $y = 0.742$

\vspace{0.3em}
\textbf{Interpretation:}
\begin{itemize}
\item 74.2\% confidence price will rise
\item If $y > 0.5$ $\rightarrow$ \textcolor{mlgreen}{\textbf{BUY}}
\item If $y < 0.5$ $\rightarrow$ \textcolor{mlred}{\textbf{SELL}}
\end{itemize}
\end{columns}

\bottomnote{Next: See forward propagation with actual numbers and calculations}
\end{frame}

\begin{frame}[t]{Forward Propagation: Detailed Example}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{06_forward_propagation/forward_propagation.pdf}
\end{center}

\bottomnote{Data flows left to right: inputs (105.2, 0.75, 0.62) $\rightarrow$ hidden activations $\rightarrow$ output (0.742) = 74\% buy confidence}
\end{frame}

\begin{frame}[t]{Learning from Mistakes: The Training Process}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Learning Process Steps}

\begin{enumerate}
\item \textbf{Predict} with random weights
\item \textbf{Measure error} (Loss Function):
$$L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$
\item \textbf{Adjust weights} (Gradient Descent):
$$w_{new} = w_{old} - \eta \frac{\partial L}{\partial w}$$
\item \textbf{Repeat} thousands of times
\end{enumerate}

\vspace{0.2em}
$\eta$ = learning rate (how fast we learn)

\column{0.48\textwidth}
\textbf{Concrete Example}

\textbf{Initial:} Network predicts 55\% price rise

\textbf{Actual:} Price fell ($y=0$)

\textbf{Error:} $(0-0.55)^2 = 0.30$

\vspace{0.3em}
\textbf{Learning Step:}
\begin{itemize}
\item Calculate gradient (direction of error)
\item Move weights opposite direction
\item Error decreases each iteration
\end{itemize}

\vspace{0.2em}
\textit{Like a trader learning from past mistakes}
\end{columns}

\bottomnote{Next: Visualize the loss landscape that we're trying to navigate}
\end{frame}

\begin{frame}[t]{Loss Landscape: The Error Surface}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{07_loss_landscape/loss_landscape.pdf}
\end{center}

\bottomnote{Goal: Find weights (red star) that minimize loss. Different starting points converge to same optimum through gradient descent}
\end{frame}

\begin{frame}[t]{Gradient Descent: Learning by Stepping Downhill}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Algorithm Steps}

\begin{enumerate}
\item \textbf{Calculate gradient:}
$$\frac{\partial L}{\partial w} = \text{slope of loss}$$
\item \textbf{Step opposite direction:}
$$w_{new} = w_{old} - \eta \times \text{gradient}$$
\item \textbf{Repeat until convergence}
\end{enumerate}

\vspace{0.3em}
\textbf{Learning Rate Trade-offs}
\begin{itemize}
\item \textcolor{mlgreen}{Too small:} Slow learning
\item \textcolor{mlred}{Too large:} Unstable, overshoots
\item \textcolor{mlblue}{Just right:} Steady progress
\end{itemize}

\column{0.48\textwidth}
\textbf{Business Analogy}

Like a trader learning from mistakes:

\begin{itemize}
\item \textbf{Fast learning phase}\\
Rapid improvement from obvious patterns
\item \textbf{Steady progress}\\
Fine-tuning strategy
\item \textbf{Convergence}\\
Optimal trading rules learned
\end{itemize}

\vspace{0.3em}
\textbf{Key Insight}

Gradient tells us which direction reduces error fastest
\end{columns}

\bottomnote{Next: See how loss decreases over training iterations}
\end{frame}

\begin{frame}[t]{Gradient Descent: Optimization in Action}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{08_gradient_descent/gradient_descent.pdf}
\end{center}

\bottomnote{Left: 1D visualization showing steps toward minimum. Right: Loss decreasing over 100 training iterations}
\end{frame}

\begin{frame}[t]{Putting It Together: Market Prediction Case Study}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Business Application}

\begin{itemize}
\item \textbf{Goal:} Predict if stock price rises/falls
\item \textbf{Data:} 60 days historical market data
\item \textbf{Features:} 4 input variables per day
\end{itemize}

\vspace{0.3em}
\textbf{Input Features}
\begin{enumerate}
\item Historical Stock Price
\item Trading Volume (normalized)
\item Market Sentiment (0-1)
\item Volatility Index (0-1)
\end{enumerate}

\column{0.48\textwidth}
\textbf{Target Variable}

Binary output:
\begin{itemize}
\item 1 = price increased
\item 0 = price decreased
\end{itemize}

Network outputs: $p(\text{price rise})$

\vspace{0.3em}
\textbf{Training Setup}
\begin{itemize}
\item Training: First 45 days (learn)
\item Test: Last 15 days (evaluate)
\item Network: 4 $\rightarrow$ 6 $\rightarrow$ 1
\end{itemize}
\end{columns}

\bottomnote{Next: See the actual market data used for training}
\end{frame}

\begin{frame}[t]{Market Data: Input Features for Neural Network}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{09_market_prediction_data/market_prediction_data.pdf}
\end{center}

\bottomnote{60 days of market data: price trend (up/down markers), volume bars, sentiment score, volatility index}
\end{frame}

\begin{frame}[t]{Training Results: Before vs After}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Experiment}

\begin{itemize}
\item \textbf{Before training:}\\
Random weights $\rightarrow$ coin flip predictions
\item \textbf{After training:}\\
Learned weights $\rightarrow$ intelligent predictions
\item \textbf{Test set:} 30 days unseen data
\end{itemize}

\vspace{0.3em}
\textbf{Key Results}
\begin{itemize}
\item \textcolor{mlred}{Before:} $\approx$50\% accuracy
\item \textcolor{mlgreen}{After:} $\approx$70\% accuracy
\item \textbf{Improvement:} +20 percentage points
\end{itemize}

\column{0.48\textwidth}
\textbf{What the Network Learned}

\begin{itemize}
\item High volume + rising price + positive sentiment = likely rise
\item Low volume + high volatility = uncertain
\item Sentiment lags price but confirms trends
\end{itemize}

\textit{Discovered from data alone!}

\vspace{0.3em}
\textbf{Reality Check}
\begin{itemize}
\item 70\% is \textcolor{mlgreen}{good} for markets
\item 100\% is \textcolor{mlred}{impossible}
\item 70\% over many trades = profit
\end{itemize}
\end{columns}

\bottomnote{Next: See detailed before/after comparison with prediction accuracy}
\end{frame}

\begin{frame}[t]{Prediction Results: Before vs After Training}
\begin{center}
\vspace{0.3em}
\includegraphics[width=0.95\textwidth,height=0.80\textheight,keepaspectratio]{10_prediction_results/prediction_results.pdf}
\end{center}

\bottomnote{Top: Actual prices. Middle: Before training (random, 50\% accuracy). After training (learned patterns, 70\% accuracy)}
\end{frame}

\begin{frame}[t]{Summary: From Neurons to Predictions}

\textbf{Our Journey (Concept $\rightarrow$ Visualization):}
\begin{enumerate}
\item \textcolor{mlpurple}{Biology:} Neurons integrate weighted signals $\rightarrow$ Chart
\item \textcolor{mlpurple}{Mathematics:} $y = f(\sum w_ix_i + b)$ $\rightarrow$ Chart
\item \textcolor{mlpurple}{Non-linearity:} Activation functions enable complexity $\rightarrow$ Chart
\item \textcolor{mlpurple}{Limitation:} One neuron fails on XOR $\rightarrow$ Chart
\item \textcolor{mlpurple}{Architecture:} Layers learn hierarchical patterns $\rightarrow$ Chart
\item \textcolor{mlpurple}{Forward prop:} Making predictions $\rightarrow$ Chart
\item \textcolor{mlpurple}{Learning:} Loss landscape $\rightarrow$ Chart
\item \textcolor{mlpurple}{Optimization:} Gradient descent $\rightarrow$ Chart
\item \textcolor{mlpurple}{Application:} Market data $\rightarrow$ Chart
\item \textcolor{mlpurple}{Results:} 50\% $\rightarrow$ 70\% accuracy $\rightarrow$ Chart
\end{enumerate}

\vspace{0.3cm}
\textbf{Key Insight:}\\
Neural networks \textbf{learn from data}, not explicit programming!

\bottomnote{The separation of concept and visualization helped you see both theory and practice}
\end{frame}

\begin{frame}[t]{When to Use Neural Networks}

\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Use Neural Networks When:}
\begin{itemize}
\item \textcolor{mlgreen}{Large dataset} (thousands+ examples)
\item \textcolor{mlgreen}{Complex patterns} (non-linear relationships)
\item \textcolor{mlgreen}{Difficult to specify rules} (too many cases)
\item \textcolor{mlgreen}{Pattern recognition} (images, speech, text)
\item \textcolor{mlgreen}{Acceptable black-box} (don't need to explain every decision)
\item \textcolor{mlgreen}{Computational resources available}
\end{itemize}

\vspace{0.3cm}
\textbf{Example Applications:}
\begin{itemize}
\item Customer churn prediction
\item Fraud detection
\item Recommendation systems
\item Image recognition
\item Natural language processing
\end{itemize}

\column{0.48\textwidth}
\textbf{Do NOT Use When:}
\begin{itemize}
\item \textcolor{mlred}{Small dataset} (few hundred examples)
\item \textcolor{mlred}{Simple linear relationships}
\item \textcolor{mlred}{Need interpretability} (regulatory requirements)
\item \textcolor{mlred}{Rules are known} (use rule-based system instead)
\item \textcolor{mlred}{High stakes, low data} (medical diagnosis with small samples)
\item \textcolor{mlred}{Real-time constraints} (millisecond decisions)
\end{itemize}

\vspace{0.3cm}
\textbf{Better Alternatives:}
\begin{itemize}
\item Linear/logistic regression (interpretable)
\item Decision trees (explainable)
\item Expert systems (known rules)
\item Statistical models (small data)
\end{itemize}
\end{columns}

\bottomnote{Choose the right tool for the problem - neural networks are powerful but not always appropriate}
\end{frame}

\begin{frame}[t]{Important Limitations \& Ethical Responsibilities}

\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Technical Limitations:}
\begin{itemize}
\item \textcolor{mlred}{\textbf{Data hungry:}} Need thousands of examples
\item \textcolor{mlred}{\textbf{Black box:}} Hard to explain decisions to regulators
\item \textcolor{mlred}{\textbf{Overfitting:}} May memorize training data, fail on new data
\item \textcolor{mlred}{\textbf{No guarantees:}} Markets are inherently unpredictable
\item \textcolor{mlred}{\textbf{Computational cost:}} Training requires GPUs, time, energy
\item \textcolor{mlred}{\textbf{Hyperparameter sensitivity:}} Architecture choices matter
\end{itemize}

\column{0.48\textwidth}
\textbf{Ethical Responsibilities:}
\begin{itemize}
\item \textbf{Fairness:} Biased data $\rightarrow$ biased predictions
  \begin{itemize}
  \item Historical hiring data may encode discrimination
  \item Loan approval systems may disadvantage minorities
  \end{itemize}
\item \textbf{Transparency:} Can you explain decisions to stakeholders?
  \begin{itemize}
  \item GDPR requires ``right to explanation''
  \item Use interpretability tools (SHAP, LIME)
  \end{itemize}
\item \textbf{Accountability:} Who is responsible when AI fails?
  \begin{itemize}
  \item Flash crash: Automated trading gone wrong
  \item Wrong medical diagnosis: Who pays?
  \end{itemize}
\item \textbf{Societal impact:} Unintended consequences
  \begin{itemize}
  \item Job displacement through automation
  \item Algorithmic trading destabilizes markets
  \end{itemize}
\end{itemize}
\end{columns}

\vspace{0.2cm}
\begin{center}
\Large \textcolor{mlpurple}{\textit{With great predictive power comes great responsibility!}}
\end{center}

\bottomnote{Always consider ethical implications before deploying AI systems in real-world business contexts}
\end{frame}

\begin{frame}[t]{Appendix: Mathematical Details (Backpropagation)}

\textbf{How Gradient Descent Works: The Chain Rule}

For a simple 2-layer network, we compute gradients layer-by-layer working backwards:

\vspace{0.3cm}
\textbf{Output Layer Gradient:}
\begin{align*}
\frac{\partial L}{\partial w^{(2)}} &= \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial w^{(2)}} \\
&= (\hat{y} - y) \cdot \sigma'(z^{(2)}) \cdot a^{(1)}
\end{align*}

\textbf{Hidden Layer Gradient (using chain rule):}
\begin{align*}
\frac{\partial L}{\partial w^{(1)}} &= \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial a^{(1)}} \cdot \frac{\partial a^{(1)}}{\partial z^{(1)}} \cdot \frac{\partial z^{(1)}}{\partial w^{(1)}} \\
&= (\hat{y} - y) \cdot \sigma'(z^{(2)}) \cdot w^{(2)} \cdot \sigma'(z^{(1)}) \cdot x
\end{align*}

\vspace{0.3cm}
\textbf{Common Loss Functions:}
\begin{itemize}
\item \textbf{Mean Squared Error (Regression):} $L = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$
\item \textbf{Binary Cross-Entropy (Classification):} $L = -\frac{1}{n}\sum_{i=1}^{n}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$
\end{itemize}

\bottomnote{Backpropagation efficiently computes how each weight contributed to the final error}
\end{frame}

\begin{frame}[t]{Appendix: Practical Tips \& Resources}

\textbf{Practical Tips for Building Business Neural Networks:}
\begin{itemize}
\item \textbf{Start simple:} Try linear/logistic regression first as baseline
\item \textbf{Feature engineering matters:} Good inputs $>$ complex architecture
\item \textbf{Avoid overfitting:} Use validation sets, regularization (L1/L2), dropout
\item \textbf{Hyperparameter tuning:} Learning rate, architecture, batch size, epochs
\item \textbf{Interpretability tools:} SHAP values, attention weights, feature importance
\item \textbf{Monitor training:} Plot loss curves, check for convergence
\item \textbf{Cross-validation:} Don't rely on single train/test split
\end{itemize}

\vspace{0.3cm}
\textbf{Recommended Resources:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Books:}
\begin{itemize}
\item Deep Learning (Goodfellow et al.)
\item Neural Networks and Deep Learning (Nielsen)
\item Hands-On Machine Learning (Geron)
\end{itemize}

\textbf{Courses:}
\begin{itemize}
\item Andrew Ng - Machine Learning (Coursera)
\item Fast.ai - Deep Learning for Coders
\item MIT 6.S191 - Intro to Deep Learning
\end{itemize}

\column{0.48\textwidth}
\textbf{Tools \& Frameworks:}
\begin{itemize}
\item PyTorch (flexible, research-friendly)
\item TensorFlow/Keras (production-ready)
\item scikit-learn (simpler models)
\end{itemize}

\textbf{Practice Datasets:}
\begin{itemize}
\item Kaggle competitions
\item Yahoo Finance API
\item UCI ML Repository
\end{itemize}
\end{columns}

\bottomnote{Best way to learn: Build real projects with real data!}
\end{frame}

\begin{frame}[t]{Appendix: Practice Problem for Business Students}

\textbf{Design Challenge:} You are a data scientist at a retail company.

\vspace{0.2cm}
\textbf{Problem:} Predict customer churn (will customer leave next month?)

\vspace{0.2cm}
\textbf{Available Data:}
\begin{itemize}
\item Customer demographics (age, location, income)
\item Purchase history (frequency, recency, monetary value)
\item Customer service interactions (calls, complaints, resolutions)
\item Website engagement (visits, time spent, pages viewed)
\end{itemize}

\vspace{0.2cm}
\textbf{Your Tasks (Work in Groups):}
\begin{enumerate}
\item Design a neural network architecture: How many layers? How many neurons per layer?
\item What would be your input features? Raw data or engineered features?
\item What activation functions would you use and where?
\item What loss function is appropriate for this problem?
\item How would you evaluate model performance? (accuracy, precision, recall, F1?)
\item What are potential ethical concerns with automated churn prediction?
\item How would you explain predictions to business stakeholders?
\item When would you NOT use a neural network for this problem?
\end{enumerate}

\bottomnote{Discuss in groups and present your design - there's no single right answer!}
\end{frame}

\end{document}