\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]
\setbeamersize{text margin left=5mm,text margin right=5mm}

\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

\title{Module 3: Learning from Mistakes}
\subtitle{Training Neural Networks with Backpropagation (1986-2012)}
\author{Neural Networks for Finance}
\institute{BSc Lecture Series}
\date{\today}

\begin{document}

% ==================== SECTION 1: OPENING (Slides 1-5) ====================
\section{Opening}

% Slide 1: Title
\begin{frame}[plain]
\titlepage
\end{frame}

% Slide 2: The Central Question
\begin{frame}[t]{The Central Question}
\begin{center}
\Large
\textit{``We have the architecture. How does it LEARN?''}
\end{center}
\bottomnote{The fundamental challenge of neural network training}
\end{frame}

% Slide 3: The Trading Desk Analogy
\begin{frame}[t]{Finance Parallel: The Trading Desk}
% TODO: Content - How traders improve by analyzing mistakes
\bottomnote{How does a trader improve? By analyzing what went wrong.}
\end{frame}

% Slide 4: Module 3 Roadmap
\begin{frame}[t]{Module 3 Roadmap}
% TODO: Content - Loss -> Gradient Descent -> Backprop -> Training
\bottomnote{From measuring error to updating weights}
\end{frame}

% Slide 5: The Learning Problem
\begin{frame}[t]{The Learning Problem}
% TODO: Content - Given data and architecture, find best weights
\bottomnote{Thousands of weights to tune - how do we find the right values?}
\end{frame}

% ==================== SECTION 2: HISTORICAL CONTEXT (Slides 6-10) ====================
\section{Historical Context: 1986-2012}

% Slide 6: 1989 - LeNet
\begin{frame}[t]{1989: LeNet and Practical Success}
% TODO: Content - Yann LeCun, handwriting recognition
% Chart: timeline_1986_2012.py
\bottomnote{Yann LeCun: First commercially deployed neural network}
\end{frame}

% Slide 7: 1991 - Vanishing Gradients
\begin{frame}[t]{1991: The Vanishing Gradient Problem}
% TODO: Content - Hochreiter identifies the problem
\bottomnote{Deep networks couldn't learn - gradients disappeared}
\end{frame}

% Slide 8: 1997 - LSTM
\begin{frame}[t]{1997: LSTM Networks}
% TODO: Content - Solution to vanishing gradients for sequences
\bottomnote{Hochreiter and Schmidhuber: Long Short-Term Memory}
\end{frame}

% Slide 9: 2012 - ImageNet Moment
\begin{frame}[t]{2012: The ImageNet Moment}
% TODO: Content - AlexNet wins by huge margin
% Chart: imagenet_error_rates.py
\bottomnote{AlexNet: Deep learning proves its superiority}
\end{frame}

% Slide 10: What Changed?
\begin{frame}[t]{What Changed Between 1990 and 2012?}
% TODO: Content - Data, compute, algorithms
\bottomnote{Big data + GPUs + ReLU + dropout = breakthrough}
\end{frame}

% ==================== SECTION 3: LOSS FUNCTIONS (Slides 11-18) ====================
\section{Loss Functions: Measuring Mistakes}

% Slide 11: What Does "Wrong" Mean?
\begin{frame}[t]{What Does ``Wrong'' Mean?}
% TODO: Content - Need to quantify error
% Chart: loss_function_concept.py
\bottomnote{We need a way to measure how wrong our predictions are}
\end{frame}

% Slide 12: Finance Analogy - P&L
\begin{frame}[t]{Finance Analogy: Profit and Loss}
% TODO: Content - P&L measures trading success
\bottomnote{P\&L is the loss function of trading}
\end{frame}

% Slide 13: Loss Function Definition
\begin{frame}[t]{Loss Function: Error Measurement}
% TODO: Content - Lower loss = better predictions
\bottomnote{The loss function quantifies prediction error}
\end{frame}

% Slide 14: MSE Intuition
\begin{frame}[t]{Mean Squared Error: Intuition}
% TODO: Content - How far off, on average?
\bottomnote{``How far off were we, on average?''}
\end{frame}

% Slide 15: MSE Visually
\begin{frame}[t]{MSE: Visual Interpretation}
% TODO: Content - Squared errors visualization
% Chart: mse_gradient.py
\bottomnote{Squaring emphasizes large errors}
\end{frame}

% Slide 16: Finance Application
\begin{frame}[t]{Finance Application: Return Prediction}
% TODO: Content - Predicted +5%, actual +2%, loss = ?
\bottomnote{Worked example with stock returns}
\end{frame}

% Slide 17: Discussion Question 1
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``Why might we want to penalize large errors more than small ones in stock prediction?''}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 18: Loss Landscape Revisited
\begin{frame}[t]{The Loss Landscape}
% TODO: Content - Loss as function of all weights
\bottomnote{Finding the minimum of a high-dimensional function}
\end{frame}

% ==================== SECTION 4: GRADIENT DESCENT (Slides 19-28) ====================
\section{Gradient Descent: Finding the Minimum}

% Slide 19: The Optimization Problem
\begin{frame}[t]{The Optimization Problem}
% TODO: Content - Where is the lowest point?
\bottomnote{How do we find the weights that minimize loss?}
\end{frame}

% Slide 20: The Blind Hiker Analogy
\begin{frame}[t]{The Blind Hiker Analogy}
% TODO: Content - Blindfolded on a mountain
\bottomnote{``You're blindfolded on a mountain. How do you find the valley?''}
\end{frame}

% Slide 21: Answer - Feel the Slope
\begin{frame}[t]{Answer: Feel the Slope}
% TODO: Content - Take steps in the downward direction
\bottomnote{Move in the direction that goes down}
\end{frame}

% Slide 22: Gradient Definition
\begin{frame}[t]{The Gradient: Direction of Steepest Ascent}
% TODO: Content - Gradient vector explanation
\bottomnote{The gradient tells us which way is ``up''}
\end{frame}

% Slide 23: Gradient Descent Intuition
\begin{frame}[t]{Gradient Descent: Move Downhill}
% TODO: Content - Move opposite to gradient
% Chart: gradient_descent_contour.py
\bottomnote{Step in the negative gradient direction}
\end{frame}

% Slide 24: Finance Parallel
\begin{frame}[t]{Finance Parallel: Portfolio Optimization}
% TODO: Content - Adjusting positions to minimize risk
\bottomnote{Similar to iterative portfolio rebalancing}
\end{frame}

% Slide 25: The Learning Rate
\begin{frame}[t]{The Learning Rate: Step Size}
% TODO: Content - How big are our steps?
% Chart: learning_rate_comparison.py
\bottomnote{Learning rate controls how far we move each step}
\end{frame}

% Slide 26: Learning Rate Too High
\begin{frame}[t]{Learning Rate Too High}
% TODO: Content - Overshoot, oscillate
\bottomnote{Too big = overshoot the minimum}
\end{frame}

% Slide 27: Learning Rate Too Low
\begin{frame}[t]{Learning Rate Too Low}
% TODO: Content - Slow convergence
\bottomnote{Too small = converge too slowly}
\end{frame}

% Slide 28: Discussion Question 2
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``In trading, what's analogous to learning rate? What happens if you adjust positions too aggressively or too conservatively?''}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% ==================== SECTION 5: BACKPROPAGATION (Slides 29-38) ====================
\section{Backpropagation: Credit Assignment}

% Slide 29: The Attribution Problem
\begin{frame}[t]{The Attribution Problem}
% TODO: Content - Which weights are to blame?
\bottomnote{The output was wrong. Which weights caused it?}
\end{frame}

% Slide 30: Finance Analogy
\begin{frame}[t]{Finance Analogy: Post-Trade Analysis}
% TODO: Content - Which decisions led to this loss?
\bottomnote{``Which decisions led to this P\&L?''}
\end{frame}

% Slide 31: Backpropagation Definition
\begin{frame}[t]{Backpropagation: Blame Assignment}
% TODO: Content - Trace error backward
% Chart: backprop_flow_diagram.py
\bottomnote{Propagating error backward through the network}
\end{frame}

% Slide 32: The Chain Rule Intuition
\begin{frame}[t]{The Chain Rule: Intuition}
% TODO: Content - If A affects B and B affects C...
% Chart: chain_rule_visualization.py
\bottomnote{``If A affects B and B affects C, how does A affect C?''}
\end{frame}

% Slide 33: Finance Chain Example
\begin{frame}[t]{Finance Chain Example}
% TODO: Content - Interest rates -> housing -> banks -> portfolio
\bottomnote{Effects propagate through chains of influence}
\end{frame}

% Slide 34: Output Layer Gradients
\begin{frame}[t]{Backprop: Output Layer}
% TODO: Content - Error at output is direct
% Chart: error_propagation_layers.py
\bottomnote{At the output, error attribution is straightforward}
\end{frame}

% Slide 35: Hidden Layer Gradients
\begin{frame}[t]{Backprop: Hidden Layers}
% TODO: Content - Error flows back through weights
\bottomnote{Hidden layer gradients require the chain rule}
\end{frame}

% Slide 36: The Complete Picture
\begin{frame}[t]{The Complete Training Loop}
% TODO: Content - Forward -> Loss -> Backward -> Update
% Chart: credit_assignment.py
\bottomnote{Forward pass, compute loss, backward pass, update weights}
\end{frame}

% Slide 37: Why "Backpropagation"?
\begin{frame}[t]{Why ``Backpropagation''?}
% TODO: Content - Error propagates backward
\bottomnote{Error information flows from output to input}
\end{frame}

% Slide 38: Discussion Question 3
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``Why do deeper networks make training harder? What happens to gradients as they flow backward through many layers?''}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% ==================== SECTION 6: TRAINING DYNAMICS (Slides 39-46) ====================
\section{Training Dynamics}

% Slide 39: Batch Gradient Descent
\begin{frame}[t]{Batch Gradient Descent}
% TODO: Content - Use all data
% Chart: batch_vs_stochastic.py
\bottomnote{Compute gradient using the entire dataset}
\end{frame}

% Slide 40: Stochastic Gradient Descent
\begin{frame}[t]{Stochastic Gradient Descent (SGD)}
% TODO: Content - One sample at a time
\bottomnote{Update after each single example}
\end{frame}

% Slide 41: Mini-Batch SGD
\begin{frame}[t]{Mini-Batch: The Sweet Spot}
% TODO: Content - Small batches
% Chart: mini_batch_visualization.py
\bottomnote{Balance between efficiency and noise}
\end{frame}

% Slide 42: Epochs
\begin{frame}[t]{Epochs: Full Passes Through Data}
% TODO: Content - One epoch = seeing all data once
\bottomnote{Training typically requires multiple epochs}
\end{frame}

% Slide 43: Training Curves
\begin{frame}[t]{Training Curves}
% TODO: Content - Loss vs epochs
% Chart: training_curve.py
\bottomnote{Monitoring progress during training}
\end{frame}

% Slide 44: Worked Example
\begin{frame}[t]{Worked Example: Training a Simple Network}
% TODO: Content - 2-2-1 network trace
% Chart: worked_backprop_example.py
\bottomnote{Following the numbers through one training step}
\end{frame}

% Slide 45: Vanishing Gradients
\begin{frame}[t]{The Vanishing Gradient Problem}
% TODO: Content - Gradients shrink through layers
% Chart: vanishing_gradient.py
\bottomnote{Deep networks: gradients can become vanishingly small}
\end{frame}

% Slide 46: Appendix Reference
\begin{frame}[t]{Full Mathematical Derivation}
% TODO: Content - Reference to appendix
\bottomnote{See Appendix B for complete backpropagation derivation}
\end{frame}

% ==================== SECTION 7: OVERFITTING (Slides 47-54) ====================
\section{Overfitting: The Enemy of Generalization}

% Slide 47: What Is Overfitting?
\begin{frame}[t]{What Is Overfitting?}
% TODO: Content - Memorizing vs learning
\bottomnote{When your model memorizes instead of learns}
\end{frame}

% Slide 48: Training vs Validation
\begin{frame}[t]{Training vs Validation Loss}
% TODO: Content - Diverging curves
% Chart: overfitting_curves.py
\bottomnote{Training loss decreases but validation increases}
\end{frame}

% Slide 49: The Backtest Trap
\begin{frame}[t]{Finance: The Backtest Trap}
% TODO: Content - Looks good on historical data
% Chart: backtest_trap.py
\bottomnote{``Every strategy looks good on historical data''}
\end{frame}

% Slide 50: Why Finance Overfits Easily
\begin{frame}[t]{Why Finance Overfits So Easily}
% TODO: Content - Few samples, noisy data
\bottomnote{Limited data, high noise, non-stationary markets}
\end{frame}

% Slide 51: Detecting Overfitting
\begin{frame}[t]{Detecting Overfitting}
% TODO: Content - Train/validation split
\bottomnote{Always monitor out-of-sample performance}
\end{frame}

% Slide 52: Discussion Question 4
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``How would you know if your stock prediction model is overfitting? What specific symptoms would you look for?''}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 53: Preview: Regularization
\begin{frame}[t]{Preview: Fighting Overfitting}
% TODO: Content - Regularization techniques
\bottomnote{Module 4 will cover solutions: regularization, dropout, early stopping}
\end{frame}

% Slide 54: Module 3 Summary Diagram
\begin{frame}[t]{Training Pipeline Overview}
% TODO: Content - Complete pipeline
% Chart: module3_summary.py
\bottomnote{The complete neural network training process}
\end{frame}

% ==================== SECTION 8: SUMMARY (Slides 55-58) ====================
\section{Summary and Preview}

% Slide 55: Module 3 Key Takeaways
\begin{frame}[t]{Module 3: Key Takeaways}
% TODO: Content - Loss, gradient descent, backprop
\bottomnote{From measuring error to updating weights}
\end{frame}

% Slide 56: What We've Built
\begin{frame}[t]{What We've Built So Far}
% TODO: Content - Architecture + Training
\bottomnote{Modules 1-3: The complete neural network foundation}
\end{frame}

% Slide 57: Discussion Questions Review
\begin{frame}[t]{Discussion Questions for Reflection}
% TODO: Content - Deep questions about training
\bottomnote{Reflect on the learning process}
\end{frame}

% Slide 58: Preview of Module 4
\begin{frame}[t]{Preview: Module 4}
\begin{center}
\Large
\textit{``Theory meets practice. How do we actually use neural networks in finance?''}
\end{center}
% TODO: Content - Teaser for applications
\bottomnote{Next: Regularization, case studies, and modern developments}
\end{frame}

\end{document}
