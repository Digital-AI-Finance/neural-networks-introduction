\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]
\setbeamersize{text margin left=5mm,text margin right=5mm}

\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

\title{Module 1: The Birth of Neural Computing}
\subtitle{From Biological Inspiration to the Perceptron (1943-1969)}
\author{Neural Networks for Finance}
\institute{BSc Lecture Series}
\date{\today}

\begin{document}

%% ============================================================================
%% MODULE1 PERCEPTRON
%% ============================================================================



% ==================== SECTION 1: OPENING (Slides 1-5) ====================
\section{Opening}

% Slide 1: Title
\begin{frame}[plain]
\titlepage
\end{frame}

% Slide 2: The Investment Committee Analogy
\begin{frame}[t]{The Investment Committee}
\begin{columns}[T]
\column{0.55\textwidth}
\textbf{How Does a Committee Make Decisions?}

Imagine an investment committee evaluating a stock:
\begin{itemize}
\item \textbf{Analyst A}: ``Strong earnings growth'' \textcolor{mlgreen}{(+1 vote)}
\item \textbf{Analyst B}: ``High debt levels'' \textcolor{mlred}{(-1 vote)}
\item \textbf{Analyst C}: ``Good momentum'' \textcolor{mlgreen}{(+1 vote)}
\item \textbf{Senior Partner}: ``Market risk is elevated'' \textcolor{mlred}{(-2 votes)}
\end{itemize}

\vspace{0.5em}
\textbf{The Decision Process:}
\begin{enumerate}
\item Gather evidence from each analyst
\item Weight opinions by seniority/expertise
\item Sum the weighted votes
\item If total $>$ threshold: \textbf{Buy}
\end{enumerate}

\column{0.42\textwidth}
\begin{center}
\textbf{Weighted Voting}

\vspace{1em}
\begin{tabular}{lcc}
\toprule
\textbf{Analyst} & \textbf{Vote} & \textbf{Weight} \\
\midrule
Analyst A & +1 & 1.0 \\
Analyst B & -1 & 1.0 \\
Analyst C & +1 & 1.0 \\
Senior Partner & -1 & 2.0 \\
\midrule
\textbf{Weighted Sum} & & \textbf{-1.0} \\
\bottomrule
\end{tabular}

\vspace{1em}
\textcolor{mlred}{\textbf{Decision: Don't Buy}}
\end{center}
\end{columns}
\bottomnote{Finance Hook: This is exactly how a perceptron works!}
\end{frame}

% Slide 3: What If Machines Could Decide?
\begin{frame}[t]{What If Machines Could Decide?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Central Question}

In 1943, scientists asked:

\begin{center}
\textit{``Can we build a machine that learns to make decisions like a brain?''}
\end{center}

\vspace{0.5em}
\textbf{Why This Matters for Finance:}
\begin{itemize}
\item Humans are slow and biased
\item Markets process millions of data points
\item Pattern recognition at scale
\item Consistent, emotionless decisions
\end{itemize}

\column{0.48\textwidth}
\textbf{The Promise}

If we could capture how neurons compute:
\begin{itemize}
\item Automatic stock screening
\item Risk assessment at scale
\item Pattern detection in market data
\item Learning from historical decisions
\end{itemize}

\vspace{0.5em}
\textbf{The Challenge}

How do we translate biological processes into mathematical operations?

\vspace{0.5em}
\textcolor{mlpurple}{\textit{This module tells the story of how scientists attempted this translation.}}
\end{columns}
\bottomnote{The fundamental question that started neural network research}
\end{frame}

% Slide 4: Module Roadmap
\begin{frame}[t]{Module 1 Roadmap}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Complete Journey (4 Modules)}

\begin{enumerate}
\item \textcolor{mlpurple}{\textbf{The Perceptron (Today)}}
\begin{itemize}
\item Single neuron foundations
\item 1943-1969 history
\end{itemize}
\item Multi-Layer Perceptrons
\begin{itemize}
\item Stacking layers, activation functions
\end{itemize}
\item Training Neural Networks
\begin{itemize}
\item Backpropagation, optimization
\end{itemize}
\item Applications in Finance
\begin{itemize}
\item Stock prediction case study
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{Today's Module Structure}

\begin{enumerate}
\item \textbf{Historical Context} (1943-1969)
\begin{itemize}
\item McCulloch-Pitts, Hebb, Rosenblatt
\end{itemize}
\item \textbf{Biological Inspiration}
\begin{itemize}
\item From neurons to mathematics
\end{itemize}
\item \textbf{The Perceptron}
\begin{itemize}
\item Intuition, then math
\end{itemize}
\item \textbf{Learning Algorithm}
\begin{itemize}
\item How it adjusts weights
\end{itemize}
\item \textbf{Limitations}
\begin{itemize}
\item XOR problem, AI Winter
\end{itemize}
\end{enumerate}
\end{columns}
\bottomnote{Your journey through neural network fundamentals}
\end{frame}

% Slide 5: Learning Objectives
\begin{frame}[t]{Learning Objectives}
\textbf{By the end of this module, you will be able to:}

\vspace{0.5em}
\begin{columns}[T]
\column{0.48\textwidth}
\begin{enumerate}
\item \textbf{Understand biological inspiration}
\begin{itemize}
\item How real neurons inspired artificial ones
\item What we kept and what we simplified
\end{itemize}

\vspace{0.3em}
\item \textbf{Master the perceptron model}
\begin{itemize}
\item Inputs, weights, sum, activation
\item The decision-making unit
\end{itemize}

\vspace{0.3em}
\item \textbf{Interpret decision boundaries}
\begin{itemize}
\item Geometric meaning of weights
\item Linear separability concept
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\begin{enumerate}
\setcounter{enumi}{3}
\item \textbf{Apply the learning algorithm}
\begin{itemize}
\item Weight update rule
\item Convergence conditions
\end{itemize}

\vspace{0.3em}
\item \textbf{Recognize limitations}
\begin{itemize}
\item XOR problem
\item Why single layers are not enough
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Finance Connection:}} Throughout, we'll use stock classification as our running example.
\end{columns}
\bottomnote{By the end of this module, you will be able to...}
\end{frame}

% ==================== SECTION 2: HISTORICAL CONTEXT (Slides 6-12) ====================
\section{Historical Context: 1943-1969}

% Slide 6: 1943 - The Spark
\begin{frame}[t]{1943: The Mathematical Neuron}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Warren McCulloch \& Walter Pitts}

In 1943, a neurophysiologist and a logician asked:

\begin{center}
\textit{``Can we describe what neurons do using mathematics?''}
\end{center}

\vspace{0.5em}
Their paper: ``A Logical Calculus of Ideas Immanent in Nervous Activity''

\vspace{0.5em}
\textbf{Key Insight:}
\begin{itemize}
\item Neurons have binary states (fire or not)
\item This is like TRUE/FALSE in logic
\item Networks of neurons can compute any logical function
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{module1_perceptron/charts/mcculloch_pitts_diagram/mcculloch_pitts_diagram.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/mcculloch_pitts_diagram}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/mcculloch_pitts_diagram}{\includegraphics[width=0.6cm]{module1_perceptron/charts/mcculloch_pitts_diagram/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/mcculloch_pitts_diagram}{\tiny\texttt{\textcolor{gray}{mcculloch\_pitts\_diagram}}}
};
\end{tikzpicture}

\bottomnote{Warren McCulloch and Walter Pitts: ``A Logical Calculus of Ideas Immanent in Nervous Activity''}
\end{frame}

% Slide 7: The Big Idea
\begin{frame}[t]{The Big Idea: Computation in the Brain}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What McCulloch \& Pitts Proposed}

The brain performs computation through:
\begin{enumerate}
\item \textbf{Binary Signals}
\begin{itemize}
\item Neurons either fire (1) or don't (0)
\item Like bits in a computer
\end{itemize}
\item \textbf{Threshold Logic}
\begin{itemize}
\item Sum of inputs exceeds threshold $\rightarrow$ fire
\item Otherwise $\rightarrow$ stay quiet
\end{itemize}
\item \textbf{Network Composition}
\begin{itemize}
\item Complex behaviors from simple units
\item AND, OR, NOT gates from neurons
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{Logical Operations with Neurons}

\vspace{0.5em}
\textbf{AND Gate} (threshold = 2):
\begin{itemize}
\item Both inputs = 1 $\rightarrow$ output = 1
\item Otherwise $\rightarrow$ output = 0
\end{itemize}

\textbf{OR Gate} (threshold = 1):
\begin{itemize}
\item Any input = 1 $\rightarrow$ output = 1
\item All inputs = 0 $\rightarrow$ output = 0
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Implication:}} If neurons compute logic, and computers compute logic, then we can build artificial brains!
\end{columns}
\bottomnote{If neurons compute, can we build artificial ones?}
\end{frame}

% Slide 8: 1949 - Hebb's Learning Rule
\begin{frame}[t]{1949: Hebbian Learning}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Donald Hebb's Insight}

McCulloch-Pitts neurons were fixed. But how does the brain \textit{learn}?

\vspace{0.5em}
\textbf{Hebb's Rule (1949):}
\begin{center}
\textit{``Neurons that fire together, wire together.''}
\end{center}

\vspace{0.5em}
\textbf{In Plain Terms:}
\begin{itemize}
\item If neuron A consistently activates neuron B
\item The connection A $\rightarrow$ B grows stronger
\item Repeated patterns reinforce pathways
\end{itemize}

\vspace{0.5em}
\textbf{Finance Analogy:}

An analyst who repeatedly identifies winning stocks gains more influence in the committee.

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{module1_perceptron/charts/hebb_learning_visualization/hebb_learning_visualization.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/hebb_learning_visualization}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/hebb_learning_visualization}{\includegraphics[width=0.6cm]{module1_perceptron/charts/hebb_learning_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/hebb_learning_visualization}{\tiny\texttt{\textcolor{gray}{hebb\_learning\_visualization}}}
};
\end{tikzpicture}

\bottomnote{Donald Hebb: ``Neurons that fire together, wire together''}
\end{frame}

% Slide 9: 1958 - The Perceptron
\begin{frame}[t]{1958: The Perceptron is Born}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Frank Rosenblatt at Cornell}

Combined McCulloch-Pitts neurons with Hebbian learning into a machine that could \textit{learn from examples}.

\vspace{0.5em}
\textbf{The Perceptron:}
\begin{itemize}
\item A single artificial neuron
\item Adjustable connection weights
\item Learns to classify patterns
\item Implemented in hardware (Mark I)
\end{itemize}

\vspace{0.5em}
\textbf{Key Innovation:}

Not just fixed logic gates, but a system that \textbf{learns} the right weights from training data.

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{module1_perceptron/charts/mark1_perceptron_diagram/mark1_perceptron_diagram.pdf}
\end{center}

\vspace{0.5em}
\small
The Mark I Perceptron used 400 photocells connected to a single layer of neurons with adjustable weights.
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/mark1_perceptron_diagram}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/mark1_perceptron_diagram}{\includegraphics[width=0.6cm]{module1_perceptron/charts/mark1_perceptron_diagram/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/mark1_perceptron_diagram}{\tiny\texttt{\textcolor{gray}{mark1\_perceptron\_diagram}}}
};
\end{tikzpicture}

\bottomnote{Frank Rosenblatt creates a machine that can learn}
\end{frame}

% Slide 10: Media Hype
\begin{frame}[t]{The New York Times Headline}
\begin{columns}[T]
\column{0.55\textwidth}
\textbf{July 8, 1958 - The New York Times}

\begin{center}
\textit{``New Navy Device Learns By Doing; Psychologist Shows Embryo of Computer Designed to Read and Grow Wiser''}
\end{center}

\vspace{0.5em}
\textbf{The Promises Made:}
\begin{itemize}
\item Machines that recognize faces
\item Automatic translation of languages
\item Systems that ``perceive'' like humans
\item The Navy predicted: walking, talking, self-reproducing machines
\end{itemize}

\vspace{0.5em}
\textbf{The Reality:}

The perceptron could classify simple patterns, but the gap between promise and capability was vast.

\column{0.42\textwidth}
\textbf{Lessons for Today}

\vspace{0.5em}
\textcolor{mlorange}{\textbf{Sound Familiar?}}
\begin{itemize}
\item ``AI will replace all jobs''
\item ``Machines will be smarter than humans by 20XX''
\item ``This changes everything''
\end{itemize}

\vspace{0.5em}
\textbf{Pattern:}
\begin{enumerate}
\item Genuine breakthrough
\item Media amplification
\item Overpromising
\item Disappointment
\item ``AI Winter''
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textit{History repeats...}}
\end{columns}
\bottomnote{``New Navy Device Learns By Doing'' - The hype cycle begins}
\end{frame}

% Slide 11: Timeline 1943-1958
\begin{frame}[t]{Timeline: The Early Years}
\begin{center}
\includegraphics[width=0.78\textwidth]{module1_perceptron/charts/timeline_1943_1969/timeline_1943_1969.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/timeline_1943_1969}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/timeline_1943_1969}{\includegraphics[width=0.6cm]{module1_perceptron/charts/timeline_1943_1969/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/timeline_1943_1969}{\tiny\texttt{\textcolor{gray}{timeline\_1943\_1969}}}
};
\end{tikzpicture}

\bottomnote{From theory to hardware in 15 years}
\end{frame}

% Slide 12: Discussion Question 1
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``The perceptron was funded by the US Navy for military applications. How does funding source shape research direction? Are there parallels in modern AI development?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\begin{itemize}
\item Military vs. commercial vs. academic funding
\item What problems get prioritized?
\item Open vs. closed research
\end{itemize}

\column{0.48\textwidth}
\begin{itemize}
\item Today: Tech giants fund most AI research
\item Government initiatives (CHIPS Act, etc.)
\item Startup ecosystem influence
\end{itemize}
\end{columns}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% ==================== SECTION 3: BIOLOGICAL INSPIRATION (Slides 13-18) ====================
\section{Biological Inspiration}

% Slide 13: The Real Neuron
\begin{frame}[t]{The Biological Neuron}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Anatomy of a Real Neuron}

\begin{enumerate}
\item \textbf{Dendrites} (Input)
\begin{itemize}
\item Tree-like branches
\item Receive signals from other neurons
\item Thousands of connections
\end{itemize}

\item \textbf{Cell Body (Soma)} (Processing)
\begin{itemize}
\item Integrates incoming signals
\item Contains the nucleus
\item Determines if neuron fires
\end{itemize}

\item \textbf{Axon} (Output)
\begin{itemize}
\item Long fiber carrying output signal
\item Connects to other neurons
\item All-or-nothing signal
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{How It Works}

\vspace{0.5em}
\begin{enumerate}
\item Signals arrive at dendrites
\item Soma sums the inputs
\item If sum exceeds threshold: neuron \textbf{fires}
\item Action potential travels down axon
\item Signal reaches next neurons
\end{enumerate}

\vspace{0.5em}
\textbf{Key Numbers:}
\begin{itemize}
\item Human brain: $\sim$86 billion neurons
\item Each neuron: $\sim$7,000 connections
\item Total synapses: $\sim$100 trillion
\end{itemize}
\end{columns}
\bottomnote{Dendrites receive, soma processes, axon transmits}
\end{frame}

% Slide 14: The Artificial Neuron
\begin{frame}[t]{The Artificial Neuron}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Mathematical Abstraction}

\vspace{0.5em}
\begin{enumerate}
\item \textbf{Inputs} ($x_1, x_2, \ldots, x_n$)
\begin{itemize}
\item Numerical values (features)
\item Replace dendrites
\end{itemize}

\item \textbf{Weights} ($w_1, w_2, \ldots, w_n$)
\begin{itemize}
\item Importance of each input
\item Replace synapse strength
\end{itemize}

\item \textbf{Weighted Sum}
\begin{itemize}
\item $z = \sum_{i=1}^{n} w_i x_i + b$
\item Replace soma integration
\end{itemize}

\item \textbf{Activation Function}
\begin{itemize}
\item $y = f(z)$
\item Replace firing decision
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{The Complete Model}

\vspace{0.5em}
$$y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)$$

\vspace{1em}
\textbf{Components:}
\begin{itemize}
\item $x_i$: Input features
\item $w_i$: Learnable weights
\item $b$: Bias (threshold adjustment)
\item $f$: Activation function
\item $y$: Output (prediction)
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Key Point:}} The weights are what the network \textit{learns}.
\end{columns}
\bottomnote{From biology to mathematics: the abstraction trade-off}
\end{frame}

% Slide 15: Side-by-Side Comparison
\begin{frame}[t]{Biological vs. Artificial: Side by Side}
\begin{center}
\includegraphics[width=0.95\textwidth]{module1_perceptron/charts/biological_vs_artificial_neuron/biological_vs_artificial_neuron.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/biological_vs_artificial_neuron}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/biological_vs_artificial_neuron}{\includegraphics[width=0.6cm]{module1_perceptron/charts/biological_vs_artificial_neuron/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/biological_vs_artificial_neuron}{\tiny\texttt{\textcolor{gray}{biological\_vs\_artificial\_neuron}}}
};
\end{tikzpicture}

\bottomnote{What did we keep? What did we simplify?}
\end{frame}

% Slide 16: The Finance Analyst Analogy
\begin{frame}[t]{Finance Analogy: The Analyst}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{A Financial Analyst as a Neuron}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Biology} & \textbf{Finance} \\
\midrule
Dendrites & Market data feeds \\
Synapses & Data reliability weights \\
Soma & Analyst's judgment \\
Threshold & Conviction level \\
Axon & ``Buy'' recommendation \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{The Process:}
\begin{enumerate}
\item Receive multiple data points
\item Weight by source quality
\item Aggregate into overall view
\item If conviction $>$ threshold: recommend
\end{enumerate}

\column{0.48\textwidth}
\textbf{Example: Stock Screening}

\vspace{0.5em}
\textbf{Inputs (Data):}
\begin{itemize}
\item $x_1$: P/E ratio = 15
\item $x_2$: Revenue growth = 20\%
\item $x_3$: Debt/Equity = 0.5
\end{itemize}

\textbf{Weights (Importance):}
\begin{itemize}
\item $w_1 = 0.3$ (value focus)
\item $w_2 = 0.5$ (growth priority)
\item $w_3 = -0.2$ (debt penalty)
\end{itemize}

\textbf{Decision:}
$$z = 0.3(15) + 0.5(20) - 0.2(0.5) = 14.4$$

If $z > 10$: \textcolor{mlgreen}{\textbf{Buy}}
\end{columns}
\bottomnote{Inputs (data) -> Weights (importance) -> Decision (output)}
\end{frame}

% Slide 17: What We Gained
\begin{frame}[t]{What We Gained from Abstraction}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Benefits of Simplification}

\begin{enumerate}
\item \textbf{Mathematical Tractability}
\begin{itemize}
\item We can write equations
\item Analyze behavior formally
\item Prove theorems
\end{itemize}

\item \textbf{Computability}
\begin{itemize}
\item Easy to implement in code
\item Fast computation
\item Scales to millions of units
\end{itemize}

\item \textbf{Trainability}
\begin{itemize}
\item Can adjust weights systematically
\item Gradient-based optimization
\item Learn from data
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{What We Can Now Do}

\vspace{0.5em}
\begin{itemize}
\item Define learning algorithms
\item Compute exact outputs
\item Train on historical data
\item Make predictions on new data
\item Analyze decision boundaries
\end{itemize}

\vspace{0.5em}
\textbf{Scale Comparison:}

\begin{tabular}{lcc}
\toprule
& \textbf{Brain} & \textbf{GPU} \\
\midrule
Operations/sec & $10^{16}$ & $10^{15}$ \\
Power & 20W & 300W \\
Training time & Years & Hours \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\small
Different trade-offs, different capabilities.
\end{columns}
\bottomnote{Simplification enables computation}
\end{frame}

% Slide 18: What We Lost
\begin{frame}[t]{What We Lost from Abstraction}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Biological Complexity We Ignored}

\begin{enumerate}
\item \textbf{Temporal Dynamics}
\begin{itemize}
\item Real neurons have timing
\item Spike patterns carry information
\item We use static activations
\end{itemize}

\item \textbf{Structural Complexity}
\begin{itemize}
\item Dendrites have local computation
\item Different neuron types
\item We use uniform units
\end{itemize}

\item \textbf{Neurochemistry}
\begin{itemize}
\item Neurotransmitters vary
\item Modulatory systems
\item We use simple multiplication
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{Implications}

\vspace{0.5em}
\textbf{What ANNs Cannot Do (Well):}
\begin{itemize}
\item Energy efficiency of brain
\item One-shot learning
\item Continuous adaptation
\item Common sense reasoning
\end{itemize}

\vspace{0.5em}
\textbf{The Trade-off:}

\begin{center}
\begin{tabular}{cc}
\textcolor{mlgreen}{Tractability} & \textcolor{mlred}{Realism} \\
$\uparrow$ & $\downarrow$ \\
\end{tabular}
\end{center}

\vspace{0.5em}
\textcolor{mlpurple}{\textit{Artificial neurons are inspired by biology, not copies of it.}}
\end{columns}
\bottomnote{The brain does far more than our models capture}
\end{frame}

% ==================== SECTION 4: PERCEPTRON INTUITION (Slides 19-28) ====================
\section{The Perceptron: Intuition First}

% Slide 19: The Simplest Decision Maker
\begin{frame}[t]{The Simplest Decision Maker}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What is a Perceptron?}

The simplest possible neural network:
\begin{itemize}
\item One artificial neuron
\item Multiple inputs, one output
\item Binary decision: Yes or No
\end{itemize}

\vspace{0.5em}
\textbf{Think of it as:}
\begin{itemize}
\item A filter for data
\item A simple classifier
\item A linear decision maker
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Finance Application:}}

Stock screener that outputs ``Buy'' or ``Don't Buy'' based on financial metrics.

\column{0.48\textwidth}
\textbf{Real-World Examples}

\vspace{0.5em}
\textbf{Email Spam Filter:}
\begin{itemize}
\item Inputs: word frequencies
\item Output: spam or not spam
\end{itemize}

\textbf{Loan Approval:}
\begin{itemize}
\item Inputs: income, credit score, debt
\item Output: approve or reject
\end{itemize}

\textbf{Stock Screening:}
\begin{itemize}
\item Inputs: P/E, momentum, volume
\item Output: buy or pass
\end{itemize}

\vspace{0.5em}
All these are binary classification problems that a perceptron can solve (if the data is linearly separable).
\end{columns}
\bottomnote{A single perceptron is a stock screening filter}
\end{frame}

% Slide 20: Your First Neural Network
\begin{frame}[t]{Your First Neural Network}
\begin{center}
\includegraphics[width=0.66\textwidth]{module1_perceptron/charts/perceptron_architecture/perceptron_architecture.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_architecture}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_architecture}{\includegraphics[width=0.6cm]{module1_perceptron/charts/perceptron_architecture/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_architecture}{\tiny\texttt{\textcolor{gray}{perceptron\_architecture}}}
};
\end{tikzpicture}

\bottomnote{Inputs, weights, sum, activation, output}
\end{frame}

% Slide 21: Finance Scenario
\begin{frame}[t]{Finance Scenario: Buy or Sell?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Problem Setup}

You want to build a simple stock screener:
\begin{itemize}
\item \textbf{Goal}: Decide Buy or Pass
\item \textbf{Data}: Historical financial metrics
\item \textbf{Method}: Perceptron classifier
\end{itemize}

\vspace{0.5em}
\textbf{Available Features:}
\begin{enumerate}
\item P/E Ratio (valuation)
\item 6-month momentum (\%)
\item Average daily volume
\item Debt-to-Equity ratio
\item Earnings surprise (\%)
\end{enumerate}

\column{0.48\textwidth}
\textbf{The Question}

Given these features for a new stock, should we add it to our portfolio?

\vspace{0.5em}
\textbf{Example Stock:}
\begin{itemize}
\item P/E = 18
\item Momentum = +12\%
\item Volume = 2M shares
\item D/E = 0.8
\item Surprise = +5\%
\end{itemize}

\vspace{0.5em}
\textbf{Traditional Approach:}

Analyst manually weighs factors and decides.

\vspace{0.5em}
\textbf{Perceptron Approach:}

Learn the weights from historical winners/losers.
\end{columns}
\bottomnote{Given financial indicators, should we buy this stock?}
\end{frame}

% Slide 22: Inputs - The Raw Data
\begin{frame}[t]{Inputs: The Raw Data}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What Are Inputs?}

Each input $x_i$ is a numerical feature:
\begin{itemize}
\item A measurement
\item A statistic
\item A signal
\end{itemize}

\vspace{0.5em}
\textbf{In Finance:}
\begin{itemize}
\item Price-based: returns, volatility
\item Fundamental: P/E, ROE, debt ratios
\item Technical: RSI, moving averages
\item Sentiment: news scores, analyst ratings
\end{itemize}

\vspace{0.5em}
\textbf{Key Requirement:}

All inputs must be \textbf{numerical}. Categorical data needs encoding.

\column{0.48\textwidth}
\textbf{Notation}

\vspace{0.5em}
For a stock with $n$ features:

$$\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$$

\vspace{0.5em}
\textbf{Example (n=3):}

$$\mathbf{x} = \begin{pmatrix} 18 \\ 0.12 \\ 0.8 \end{pmatrix} = \begin{pmatrix} \text{P/E} \\ \text{Momentum} \\ \text{D/E} \end{pmatrix}$$

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Note:}} Features often need \textbf{normalization} (covered in Module 3).
\end{columns}
\bottomnote{What data feeds into our decision?}
\end{frame}

% Slide 23: Weights - The Importance Factors
\begin{frame}[t]{Weights: The Importance Factors}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What Are Weights?}

Each weight $w_i$ represents:
\begin{itemize}
\item Importance of input $x_i$
\item Direction of influence
\item Learned from data
\end{itemize}

\vspace{0.5em}
\textbf{Interpretation:}
\begin{itemize}
\item $w_i > 0$: Higher $x_i$ pushes toward ``Buy''
\item $w_i < 0$: Higher $x_i$ pushes toward ``Sell''
\item $|w_i|$ large: Strong influence
\item $|w_i|$ small: Weak influence
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{module1_perceptron/charts/weighted_sum_visualization/weighted_sum_visualization.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/weighted_sum_visualization}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/weighted_sum_visualization}{\includegraphics[width=0.6cm]{module1_perceptron/charts/weighted_sum_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/weighted_sum_visualization}{\tiny\texttt{\textcolor{gray}{weighted\_sum\_visualization}}}
};
\end{tikzpicture}

\bottomnote{``Not all data is equally important'' - weights encode importance}
\end{frame}

% Slide 24: Discussion Question 2
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``If you could only look at 3 metrics for a stock, which would you choose and why? How would you weight them?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Value Investor Might Choose:}
\begin{itemize}
\item P/E ratio ($w = 0.5$)
\item Book value ($w = 0.3$)
\item Dividend yield ($w = 0.2$)
\end{itemize}

\column{0.48\textwidth}
\textbf{Growth Investor Might Choose:}
\begin{itemize}
\item Revenue growth ($w = 0.5$)
\item Momentum ($w = 0.3$)
\item Market share ($w = 0.2$)
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Key Insight:}} Different investors would assign different weights. The perceptron \textit{learns} these weights from historical performance.
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 25: The Weighted Sum
\begin{frame}[t]{The Weighted Sum: Adding Up Evidence}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Computing the Weighted Sum}

$$z = \sum_{i=1}^{n} w_i x_i + b = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b$$

\vspace{0.5em}
\textbf{What This Means:}
\begin{itemize}
\item Multiply each input by its weight
\item Sum all the products
\item Add the bias term $b$
\item Result: a single ``score''
\end{itemize}

\vspace{0.5em}
\textbf{The Bias $b$:}
\begin{itemize}
\item Shifts the decision threshold
\item Like a ``base rate'' or prior
\item Can be thought of as $w_0 \cdot x_0$ where $x_0 = 1$
\end{itemize}

\column{0.48\textwidth}
\textbf{Worked Example}

\vspace{0.5em}
\textbf{Inputs:}
\begin{itemize}
\item $x_1 = 0.8$ (normalized P/E)
\item $x_2 = 0.6$ (normalized momentum)
\end{itemize}

\textbf{Weights:}
\begin{itemize}
\item $w_1 = 0.5$
\item $w_2 = 0.7$
\item $b = -0.3$
\end{itemize}

\textbf{Calculation:}
\begin{align*}
z &= w_1 x_1 + w_2 x_2 + b \\
&= (0.5)(0.8) + (0.7)(0.6) + (-0.3) \\
&= 0.4 + 0.42 - 0.3 \\
&= \textbf{0.52}
\end{align*}
\end{columns}
\bottomnote{Combine all weighted inputs into a single score}
\end{frame}

% Slide 26: The Voting Committee Analogy
\begin{frame}[t]{Analogy: The Voting Committee}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Perceptron as a Committee}

\begin{tabular}{lccc}
\toprule
\textbf{Member} & \textbf{Vote} & \textbf{Weight} & \textbf{Contribution} \\
\midrule
P/E analyst & +1 & 0.5 & +0.5 \\
Momentum & +1 & 0.7 & +0.7 \\
Bias (skeptic) & -1 & 0.3 & -0.3 \\
\midrule
\textbf{Total} & & & \textbf{+0.9} \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{If Total $>$ 0:} Committee recommends \textcolor{mlgreen}{\textbf{Buy}}

\vspace{0.5em}
\textbf{Key Insight:}

The perceptron is just a weighted voting system where the weights are learned from data.

\column{0.48\textwidth}
\textbf{Why This Works}

\vspace{0.5em}
\textbf{Traditional Committee:}
\begin{itemize}
\item Human experts set weights
\item Based on experience/intuition
\item May have biases
\item Hard to scale
\end{itemize}

\textbf{Perceptron Committee:}
\begin{itemize}
\item Weights learned from data
\item Based on historical performance
\item Consistent application
\item Scales to any volume
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Trade-off:}} Data-driven weights may not capture regime changes or rare events.
\end{columns}
\bottomnote{Some votes count more than others}
\end{frame}

% Slide 27: The Threshold Decision
\begin{frame}[t]{The Threshold: Making the Call}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Activation Function}

After computing $z$, we need a final decision.

\vspace{0.5em}
\textbf{Step Function:}
$$f(z) = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases}$$

\vspace{0.5em}
\textbf{Interpretation:}
\begin{itemize}
\item $z \geq 0$: Evidence favors ``Buy'' $\rightarrow$ output 1
\item $z < 0$: Evidence favors ``Sell'' $\rightarrow$ output 0
\end{itemize}

\vspace{0.5em}
\textbf{Why Step Function?}
\begin{itemize}
\item Binary classification needs binary output
\item Mimics neuron firing (all-or-nothing)
\item Simple to implement
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{module1_perceptron/charts/step_function/step_function.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/step_function}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/step_function}{\includegraphics[width=0.6cm]{module1_perceptron/charts/step_function/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/step_function}{\tiny\texttt{\textcolor{gray}{step\_function}}}
};
\end{tikzpicture}

\bottomnote{Above threshold = Buy, Below threshold = Sell}
\end{frame}

% Slide 28: Putting It All Together
\begin{frame}[t]{The Complete Perceptron Flow}
\begin{columns}[T]
\column{0.35\textwidth}
\textbf{The Pipeline}

\begin{enumerate}
\item \textbf{Input}: Receive features $\mathbf{x}$
\item \textbf{Weight}: Multiply by $\mathbf{w}$
\item \textbf{Sum}: Add all products + bias
\item \textbf{Activate}: Apply step function
\item \textbf{Output}: Return prediction
\end{enumerate}

\vspace{0.5em}
\textbf{Compact Notation:}
$$y = f(\mathbf{w}^T \mathbf{x} + b)$$

where $\mathbf{w}^T \mathbf{x} = \sum_i w_i x_i$

\column{0.62\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module1_perceptron/charts/perceptron_architecture/perceptron_architecture.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_architecture}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_architecture}{\includegraphics[width=0.6cm]{module1_perceptron/charts/perceptron_architecture/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_architecture}{\tiny\texttt{\textcolor{gray}{perceptron\_architecture}}}
};
\end{tikzpicture}

\bottomnote{Inputs -> Weights -> Sum -> Threshold -> Decision}
\end{frame}

% ==================== SECTION 5: PERCEPTRON MATHEMATICS (Slides 29-36) ====================
\section{The Perceptron: Mathematical Formulation}

% Slide 29: Transition to Math
\begin{frame}[t]{Now Let's Formalize}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What You Already Know}

From the intuition section:
\begin{itemize}
\item Inputs are weighted
\item Weights encode importance
\item Sum is compared to threshold
\item Output is binary
\end{itemize}

\vspace{0.5em}
\textbf{What's Next}

\begin{itemize}
\item Precise mathematical notation
\item Geometric interpretation
\item Foundation for learning algorithm
\end{itemize}

\column{0.48\textwidth}
\textbf{Why Math Matters}

\vspace{0.5em}
\textbf{Without Math:}
\begin{itemize}
\item ``The network kind of learns''
\item ``Adjust weights somehow''
\item ``It works, probably''
\end{itemize}

\textbf{With Math:}
\begin{itemize}
\item Precise learning rules
\item Convergence guarantees
\item Understanding of limitations
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textit{The next 8 slides formalize what you already understand intuitively.}}
\end{columns}
\bottomnote{You understand the intuition. Let's write it precisely.}
\end{frame}

% Slide 30: The Perceptron Equation
\begin{frame}[t]{The Perceptron Equation}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Scalar Form}

$$y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)$$

where $f$ is the step function:
$$f(z) = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{otherwise} \end{cases}$$

\vspace{0.5em}
\textbf{Vector Form}

$$y = f(\mathbf{w}^T \mathbf{x} + b)$$

where:
\begin{itemize}
\item $\mathbf{w} = (w_1, \ldots, w_n)^T$
\item $\mathbf{x} = (x_1, \ldots, x_n)^T$
\end{itemize}

\column{0.48\textwidth}
\textbf{Alternative Notation}

We can absorb the bias into weights:

$$\tilde{\mathbf{w}} = \begin{pmatrix} b \\ w_1 \\ \vdots \\ w_n \end{pmatrix}, \quad \tilde{\mathbf{x}} = \begin{pmatrix} 1 \\ x_1 \\ \vdots \\ x_n \end{pmatrix}$$

Then:
$$y = f(\tilde{\mathbf{w}}^T \tilde{\mathbf{x}})$$

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Note:}} This ``bias trick'' simplifies notation but they are equivalent.
\end{columns}
\bottomnote{The complete mathematical model}
\end{frame}

% Slide 31: Unpacking the Math
\begin{frame}[t]{Unpacking the Mathematics}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Term by Term}

\vspace{0.5em}
\begin{tabular}{cl}
\toprule
\textbf{Symbol} & \textbf{Meaning} \\
\midrule
$x_i$ & Input feature $i$ \\
$w_i$ & Weight for feature $i$ \\
$b$ & Bias (threshold shift) \\
$z$ & Weighted sum (pre-activation) \\
$f$ & Activation function \\
$y$ & Output prediction \\
$n$ & Number of features \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Dimensions:}
\begin{itemize}
\item $\mathbf{x} \in \mathbb{R}^n$
\item $\mathbf{w} \in \mathbb{R}^n$
\item $b, z, y \in \mathbb{R}$
\end{itemize}

\column{0.48\textwidth}
\textbf{What Gets Learned?}

\vspace{0.5em}
\textcolor{mlgreen}{\textbf{Learned (trainable):}}
\begin{itemize}
\item Weights $w_1, \ldots, w_n$
\item Bias $b$
\end{itemize}

\textcolor{mlred}{\textbf{Fixed (architecture):}}
\begin{itemize}
\item Number of inputs $n$
\item Activation function $f$
\end{itemize}

\textcolor{mlblue}{\textbf{Given (data):}}
\begin{itemize}
\item Input values $x_1, \ldots, x_n$
\item Target labels (for training)
\end{itemize}

\vspace{0.5em}
\textbf{Total Parameters:} $n + 1$

(For a 3-feature perceptron: 4 parameters)
\end{columns}
\bottomnote{Each symbol has a meaning}
\end{frame}

% Slide 32: The Bias Term
\begin{frame}[t]{The Bias Term}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What Does Bias Do?}

Without bias ($b = 0$):
$$z = \mathbf{w}^T \mathbf{x}$$

The decision boundary passes through origin.

\vspace{0.5em}
With bias ($b \neq 0$):
$$z = \mathbf{w}^T \mathbf{x} + b$$

The decision boundary can be anywhere.

\vspace{0.5em}
\textbf{Interpretation:}
\begin{itemize}
\item $b > 0$: Default toward ``Buy''
\item $b < 0$: Default toward ``Sell''
\item Like a prior belief
\end{itemize}

\column{0.48\textwidth}
\textbf{Finance Analogy}

\vspace{0.5em}
\textbf{Without Bias:}

``I have no opinion until I see data''

\vspace{0.5em}
\textbf{With Positive Bias:}

``I'm generally bullish; you need to convince me to sell''

\vspace{0.5em}
\textbf{With Negative Bias:}

``I'm skeptical by default; you need strong evidence to buy''

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Key Point:}} Bias shifts the ``bar'' that evidence must clear. It's learned from data just like weights.
\end{columns}
\bottomnote{Bias shifts the decision threshold}
\end{frame}

% Slide 33: The Step Activation Function
\begin{frame}[t]{The Step Activation Function}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Formal Definition}

The Heaviside step function:

$$f(z) = \mathbf{1}_{z \geq 0} = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases}$$

\vspace{0.5em}
\textbf{Properties:}
\begin{itemize}
\item Output $\in \{0, 1\}$
\item Discontinuous at $z = 0$
\item Not differentiable (problem for gradient-based learning!)
\end{itemize}

\vspace{0.5em}
\textbf{Variants:}
\begin{itemize}
\item Sign function: outputs $\{-1, +1\}$
\item Same idea, different labels
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.95\textwidth]{module1_perceptron/charts/step_function/step_function.pdf}
\end{center}

\vspace{0.5em}
\small
\textcolor{mlpurple}{\textbf{Preview:}} The non-differentiability of the step function is why we'll need smoother activations (sigmoid, ReLU) in later modules.
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/step_function}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/step_function}{\includegraphics[width=0.6cm]{module1_perceptron/charts/step_function/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/step_function}{\tiny\texttt{\textcolor{gray}{step\_function}}}
};
\end{tikzpicture}

\bottomnote{Binary output: yes or no}
\end{frame}

% Slide 34: Geometric Interpretation
\begin{frame}[t]{Geometric Interpretation: The Decision Boundary}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Perceptron as a Hyperplane}

The equation $\mathbf{w}^T \mathbf{x} + b = 0$ defines a hyperplane:
\begin{itemize}
\item In 2D: a line
\item In 3D: a plane
\item In $n$D: a hyperplane
\end{itemize}

\vspace{0.5em}
\textbf{Regions:}
\begin{itemize}
\item $\mathbf{w}^T \mathbf{x} + b > 0$: Class 1 (Buy)
\item $\mathbf{w}^T \mathbf{x} + b < 0$: Class 0 (Sell)
\item $\mathbf{w}^T \mathbf{x} + b = 0$: Decision boundary
\end{itemize}

\vspace{0.5em}
\textbf{Weight Vector Direction:}

$\mathbf{w}$ is perpendicular to the decision boundary, pointing toward the positive class.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.85\textwidth]{module1_perceptron/charts/decision_boundary_2d/decision_boundary_2d.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/decision_boundary_2d}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/decision_boundary_2d}{\includegraphics[width=0.6cm]{module1_perceptron/charts/decision_boundary_2d/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/decision_boundary_2d}{\tiny\texttt{\textcolor{gray}{decision\_boundary\_2d}}}
};
\end{tikzpicture}

\bottomnote{The perceptron draws a line between classes}
\end{frame}

% Slide 35: Finance Example: Stock Classification
\begin{frame}[t]{Finance Example: Classifying Stocks}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Two-Feature Stock Screener}

Features:
\begin{itemize}
\item $x_1$: P/E ratio (normalized)
\item $x_2$: 6-month momentum (\%)
\end{itemize}

\vspace{0.5em}
\textbf{Classes:}
\begin{itemize}
\item \textcolor{mlgreen}{Green}: Outperformed (Buy)
\item \textcolor{mlred}{Red}: Underperformed (Sell)
\end{itemize}

\vspace{0.5em}
\textbf{Goal:}

Find $w_1, w_2, b$ such that:
$$w_1 \cdot \text{P/E} + w_2 \cdot \text{Momentum} + b = 0$$

separates the classes.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.85\textwidth]{module1_perceptron/charts/stock_features_scatter/stock_features_scatter.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/stock_features_scatter}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/stock_features_scatter}{\includegraphics[width=0.6cm]{module1_perceptron/charts/stock_features_scatter/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/stock_features_scatter}{\tiny\texttt{\textcolor{gray}{stock\_features\_scatter}}}
};
\end{tikzpicture}

\bottomnote{Separating ``good'' stocks from ``bad'' stocks}
\end{frame}

% Slide 36: Decision Boundary Formula
\begin{frame}[t]{The Decision Boundary Formula}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{In 2D: The Line Equation}

From $w_1 x_1 + w_2 x_2 + b = 0$:

$$x_2 = -\frac{w_1}{w_2} x_1 - \frac{b}{w_2}$$

\vspace{0.5em}
\textbf{This is a line with:}
\begin{itemize}
\item Slope: $-\frac{w_1}{w_2}$
\item Intercept: $-\frac{b}{w_2}$
\end{itemize}

\vspace{0.5em}
\textbf{Example:}

If $w_1 = 2, w_2 = 1, b = -3$:
$$x_2 = -2x_1 + 3$$

Stocks above this line: Buy

Stocks below this line: Sell

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module1_perceptron/charts/finance_decision_boundary/finance_decision_boundary.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/finance_decision_boundary}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/finance_decision_boundary}{\includegraphics[width=0.6cm]{module1_perceptron/charts/finance_decision_boundary/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/finance_decision_boundary}{\tiny\texttt{\textcolor{gray}{finance\_decision\_boundary}}}
};
\end{tikzpicture}

\bottomnote{The line that separates buy from sell}
\end{frame}

% ==================== SECTION 6: PERCEPTRON LEARNING (Slides 37-44) ====================
\section{The Perceptron Learning Algorithm}

% Slide 37: How Does It Learn?
\begin{frame}[t]{How Does the Perceptron Learn?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Learning Problem}

\textbf{Given:}
\begin{itemize}
\item Training data: $\{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^{m}$
\item Each $\mathbf{x}^{(i)}$: feature vector
\item Each $y^{(i)} \in \{0, 1\}$: true label
\end{itemize}

\textbf{Find:}
\begin{itemize}
\item Weights $\mathbf{w}$
\item Bias $b$
\item Such that predictions match labels
\end{itemize}

\vspace{0.5em}
\textbf{The Approach:}

Start with random weights, then iteratively adjust based on mistakes.

\column{0.48\textwidth}
\textbf{The Core Idea}

\vspace{0.5em}
\textbf{If prediction is correct:}

Do nothing. Weights are fine.

\vspace{0.5em}
\textbf{If prediction is wrong:}

Adjust weights to make this example more likely to be correct next time.

\vspace{0.5em}
\textbf{Repeat:}

Keep cycling through training data until no mistakes (or convergence).

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Key Insight:}} Learning = adjusting weights based on errors.
\end{columns}
\bottomnote{Learning = adjusting weights based on mistakes}
\end{frame}

% Slide 38: Learning from Mistakes
\begin{frame}[t]{Learning from Mistakes}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Two Types of Errors}

\textbf{False Negative} ($\hat{y} = 0$, $y = 1$):
\begin{itemize}
\item Predicted Sell, should be Buy
\item The score $z$ was too low
\item Need to \textit{increase} score for this $\mathbf{x}$
\item Solution: Add $\mathbf{x}$ to $\mathbf{w}$
\end{itemize}

\vspace{0.5em}
\textbf{False Positive} ($\hat{y} = 1$, $y = 0$):
\begin{itemize}
\item Predicted Buy, should be Sell
\item The score $z$ was too high
\item Need to \textit{decrease} score for this $\mathbf{x}$
\item Solution: Subtract $\mathbf{x}$ from $\mathbf{w}$
\end{itemize}

\column{0.48\textwidth}
\textbf{Visual Intuition}

\vspace{0.5em}
\textbf{Before update:}

Point is on wrong side of boundary.

\vspace{0.5em}
\textbf{After update:}

Boundary moves to include the point on the correct side.

\vspace{0.5em}
\textbf{The Update Rule:}

$$\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} + (y - \hat{y}) \cdot \mathbf{x}$$

\vspace{0.5em}
\textbf{Check:}
\begin{itemize}
\item If $y = 1, \hat{y} = 0$: add $\mathbf{x}$
\item If $y = 0, \hat{y} = 1$: subtract $\mathbf{x}$
\item If $y = \hat{y}$: no change
\end{itemize}
\end{columns}
\bottomnote{Each mistake is a learning opportunity}
\end{frame}

% Slide 39: The Learning Rule (Intuition)
\begin{frame}[t]{The Learning Rule: Intuition}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Why Adding $\mathbf{x}$ Works}

For a false negative (missed Buy):
\begin{itemize}
\item Current: $\mathbf{w}^T \mathbf{x} + b < 0$
\item After adding $\mathbf{x}$ to $\mathbf{w}$:
\item New score: $(\mathbf{w} + \mathbf{x})^T \mathbf{x} + b$
\item $= \mathbf{w}^T \mathbf{x} + \mathbf{x}^T \mathbf{x} + b$
\item $= \mathbf{w}^T \mathbf{x} + \|\mathbf{x}\|^2 + b$
\end{itemize}

Since $\|\mathbf{x}\|^2 > 0$, the new score is higher!

\vspace{0.5em}
\textbf{Geometrically:}

Adding $\mathbf{x}$ rotates the decision boundary toward classifying $\mathbf{x}$ correctly.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module1_perceptron/charts/perceptron_learning_animation/perceptron_learning_animation.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_learning_animation}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_learning_animation}{\includegraphics[width=0.6cm]{module1_perceptron/charts/perceptron_learning_animation/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/perceptron_learning_animation}{\tiny\texttt{\textcolor{gray}{perceptron\_learning\_animation}}}
};
\end{tikzpicture}

\bottomnote{If wrong, move the boundary}
\end{frame}

% Slide 40: The Learning Rule (Math)
\begin{frame}[t]{The Perceptron Learning Rule}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Update Equations}

For each training example $(\mathbf{x}, y)$:

\vspace{0.5em}
\textbf{Weight update:}
$$\mathbf{w} \leftarrow \mathbf{w} + \eta (y - \hat{y}) \mathbf{x}$$

\textbf{Bias update:}
$$b \leftarrow b + \eta (y - \hat{y})$$

where:
\begin{itemize}
\item $\eta > 0$ is the learning rate
\item $\hat{y} = f(\mathbf{w}^T \mathbf{x} + b)$ is prediction
\item $y$ is true label
\end{itemize}

\column{0.48\textwidth}
\textbf{The Complete Algorithm}

\begin{enumerate}
\item Initialize $\mathbf{w} = \mathbf{0}$, $b = 0$
\item \textbf{repeat}:
\begin{enumerate}
\item[a.] For each $(\mathbf{x}^{(i)}, y^{(i)})$ in training set:
\item[b.] Compute $\hat{y}^{(i)} = f(\mathbf{w}^T \mathbf{x}^{(i)} + b)$
\item[c.] If $\hat{y}^{(i)} \neq y^{(i)}$:
\item[] $\mathbf{w} \leftarrow \mathbf{w} + \eta (y^{(i)} - \hat{y}^{(i)}) \mathbf{x}^{(i)}$
\item[] $b \leftarrow b + \eta (y^{(i)} - \hat{y}^{(i)})$
\end{enumerate}
\item \textbf{until} no errors (or max iterations)
\end{enumerate}
\end{columns}
\bottomnote{The mathematical update rule}
\end{frame}

% Slide 41: Learning Rate
\begin{frame}[t]{The Learning Rate}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What is $\eta$?}

The learning rate controls step size:
\begin{itemize}
\item How much weights change per update
\item Typical values: 0.01 to 1.0
\item For perceptron: often $\eta = 1$
\end{itemize}

\vspace{0.5em}
\textbf{Effects:}

\textbf{$\eta$ too small:}
\begin{itemize}
\item Very slow learning
\item Many iterations needed
\item But stable
\end{itemize}

\textbf{$\eta$ too large:}
\begin{itemize}
\item May overshoot
\item Oscillate around solution
\item But faster initially
\end{itemize}

\column{0.48\textwidth}
\textbf{For the Perceptron}

\vspace{0.5em}
\textbf{Good news:}

For linearly separable data, the perceptron converges regardless of $\eta > 0$.

\vspace{0.5em}
\textbf{Why?}

The convergence theorem (next slides) guarantees finding a solution if one exists.

\vspace{0.5em}
\textbf{In Practice:}

$\eta = 1$ is common for perceptron. Learning rate matters more for:
\begin{itemize}
\item Gradient descent (Module 3)
\item Non-separable data
\item Multi-layer networks
\end{itemize}
\end{columns}
\bottomnote{Step size matters: too big or too small both cause problems}
\end{frame}

% Slide 42: Worked Example
\begin{frame}[t]{Worked Example: Stock Classification}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Setup}

Two stocks, two features:
\begin{itemize}
\item $\mathbf{x}^{(1)} = (0.5, 0.8)$, $y^{(1)} = 1$ (Buy)
\item $\mathbf{x}^{(2)} = (0.2, 0.3)$, $y^{(2)} = 0$ (Sell)
\end{itemize}

Initialize: $\mathbf{w} = (0, 0)$, $b = 0$, $\eta = 1$

\vspace{0.5em}
\textbf{Iteration 1:} Example 1
\begin{itemize}
\item $z = 0 \cdot 0.5 + 0 \cdot 0.8 + 0 = 0$
\item $\hat{y} = f(0) = 1$ (threshold at 0)
\item $y = 1$, correct! No update.
\end{itemize}

\vspace{0.5em}
\textbf{Iteration 1:} Example 2
\begin{itemize}
\item $z = 0$, $\hat{y} = 1$
\item $y = 0$, wrong!
\item $\mathbf{w} \leftarrow (0,0) + 1(0-1)(0.2,0.3) = (-0.2, -0.3)$
\item $b \leftarrow 0 + 1(0-1) = -1$
\end{itemize}

\column{0.48\textwidth}
\textbf{Iteration 2:} Example 1
\begin{itemize}
\item $z = -0.2(0.5) - 0.3(0.8) - 1 = -1.34$
\item $\hat{y} = 0$
\item $y = 1$, wrong!
\item $\mathbf{w} \leftarrow (-0.2,-0.3) + (0.5,0.8) = (0.3, 0.5)$
\item $b \leftarrow -1 + 1 = 0$
\end{itemize}

\textbf{Iteration 2:} Example 2
\begin{itemize}
\item $z = 0.3(0.2) + 0.5(0.3) + 0 = 0.21$
\item $\hat{y} = 1$, $y = 0$, wrong!
\item $\mathbf{w} \leftarrow (0.3,0.5) - (0.2,0.3) = (0.1, 0.2)$
\item $b \leftarrow 0 - 1 = -1$
\end{itemize}

\textbf{Continue until convergence...}
\end{columns}
\bottomnote{Following the math with real numbers}
\end{frame}

% Slide 43: Convergence
\begin{frame}[t]{Convergence: Does It Always Work?}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Perceptron Convergence Theorem}

\textbf{Theorem (Rosenblatt, 1962):}

If the training data is \textbf{linearly separable}, the perceptron learning algorithm will find a separating hyperplane in a \textbf{finite} number of updates.

\vspace{0.5em}
\textbf{Key Conditions:}
\begin{itemize}
\item Data must be linearly separable
\item Learning rate $\eta > 0$
\item Cycling through all examples
\end{itemize}

\vspace{0.5em}
\textbf{Bound on Updates:}
$$\text{mistakes} \leq \frac{R^2}{\gamma^2}$$

where $R$ = max norm, $\gamma$ = margin

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module1_perceptron/charts/convergence_plot/convergence_plot.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/convergence_plot}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/convergence_plot}{\includegraphics[width=0.6cm]{module1_perceptron/charts/convergence_plot/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/convergence_plot}{\tiny\texttt{\textcolor{gray}{convergence\_plot}}}
};
\end{tikzpicture}

\bottomnote{The perceptron convergence theorem guarantees finding a solution IF one exists}
\end{frame}

% Slide 44: Discussion Question 3
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``What happens when data isn't linearly separable in financial markets? Can you think of examples?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Examples of Non-Separable Data:}
\begin{itemize}
\item High P/E growth stocks AND low P/E value stocks both outperform
\item Medium-risk investments underperform both conservative and aggressive
\item ``Buy the rumor, sell the news'' patterns
\end{itemize}

\column{0.48\textwidth}
\textbf{What Happens to the Perceptron?}
\begin{itemize}
\item Never converges
\item Oscillates forever
\item Best we can do: minimize errors
\item Need something more powerful...
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Foreshadowing:}} This is exactly why we need \textbf{multi-layer} networks (Module 2).
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% ==================== SECTION 7: LIMITATIONS (Slides 45-48) ====================
\section{Limitations and the First AI Winter}

% Slide 45: The XOR Problem
\begin{frame}[t]{The XOR Problem}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Exclusive OR Function}

\begin{center}
\begin{tabular}{cc|c}
\toprule
$x_1$ & $x_2$ & XOR \\
\midrule
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.5em}
\textbf{In Words:}

Output is 1 if inputs are \textit{different}, 0 if inputs are \textit{same}.

\vspace{0.5em}
\textbf{The Challenge:}

Try to draw a single line that separates the 1s from the 0s...

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module1_perceptron/charts/xor_problem/xor_problem.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/xor_problem}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/xor_problem}{\includegraphics[width=0.6cm]{module1_perceptron/charts/xor_problem/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/xor_problem}{\tiny\texttt{\textcolor{gray}{xor\_problem}}}
};
\end{tikzpicture}

\bottomnote{Some patterns cannot be separated by a single line}
\end{frame}

% Slide 46: Why XOR Fails
\begin{frame}[t]{Why XOR Cannot Be Solved}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Geometric Impossibility}

\vspace{0.5em}
\textbf{Perceptron decision boundary:}

$w_1 x_1 + w_2 x_2 + b = 0$

This is always a \textbf{straight line}.

\vspace{0.5em}
\textbf{XOR requires:}

A boundary that curves or has multiple segments.

\vspace{0.5em}
\textbf{Linear vs Non-Linear}

\textbf{Linearly Separable:}
\begin{itemize}
\item AND, OR, NAND, NOR
\item One line can separate
\end{itemize}

\textbf{Not Linearly Separable:}
\begin{itemize}
\item XOR, XNOR
\item No single line works
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module1_perceptron/charts/linear_vs_nonlinear_patterns/linear_vs_nonlinear_patterns.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/linear_vs_nonlinear_patterns}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/linear_vs_nonlinear_patterns}{\includegraphics[width=0.6cm]{module1_perceptron/charts/linear_vs_nonlinear_patterns/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/linear_vs_nonlinear_patterns}{\tiny\texttt{\textcolor{gray}{linear\_vs\_nonlinear\_patterns}}}
};
\end{tikzpicture}

\bottomnote{No single hyperplane can separate XOR}
\end{frame}

% Slide 47: 1969 - Minsky and Papert
\begin{frame}[t]{1969: The Critique That Changed Everything}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Minsky and Papert's Book}

``Perceptrons: An Introduction to Computational Geometry'' (1969)

\vspace{0.5em}
\textbf{Key Arguments:}
\begin{enumerate}
\item Single-layer perceptrons cannot compute XOR
\item Many important functions are non-linear
\item No known training algorithm for multi-layer networks
\item Scaling limitations
\end{enumerate}

\vspace{0.5em}
\textbf{The Impact:}

The book was rigorous and influential. It convinced funding agencies that neural networks were a dead end.

\column{0.48\textwidth}
\textbf{The Controversy}

\vspace{0.5em}
\textbf{Valid Points:}
\begin{itemize}
\item Single layers \textit{are} limited
\item XOR problem is real
\item No training algorithm existed (then)
\end{itemize}

\textbf{Overstated Points:}
\begin{itemize}
\item ``Neural networks can't work''
\item Implied multi-layer networks wouldn't help
\item Discouraged research for 15+ years
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Lesson:}} Valid criticism of current methods shouldn't stop research into future improvements.
\end{columns}
\bottomnote{Marvin Minsky and Seymour Papert: ``Perceptrons'' book}
\end{frame}

% Slide 48: The First AI Winter
\begin{frame}[t]{The First AI Winter Begins}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Collapse}

After 1969:
\begin{itemize}
\item Funding dried up
\item Researchers left the field
\item ``Neural networks don't work''
\item Symbolic AI took over
\end{itemize}

\vspace{0.5em}
\textbf{Duration:} 1969 to $\sim$1982

\vspace{0.5em}
\textbf{What Survived:}
\begin{itemize}
\item A few dedicated researchers
\item Theoretical work continued quietly
\item Hopfield networks (1982)
\item Backpropagation (1986)
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module1_perceptron/charts/ai_winter_timeline/ai_winter_timeline.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/ai_winter_timeline}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/ai_winter_timeline}{\includegraphics[width=0.6cm]{module1_perceptron/charts/ai_winter_timeline/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/ai_winter_timeline}{\tiny\texttt{\textcolor{gray}{ai\_winter\_timeline}}}
};
\end{tikzpicture}

\bottomnote{1969-1982: The dark ages of neural network research}
\end{frame}

% ==================== SECTION 8: SUMMARY (Slides 49-50) ====================
\section{Summary and Preview}

% Slide 49: Module 1 Summary
\begin{frame}[t]{Module 1: Key Takeaways}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{What We Learned}

\begin{enumerate}
\item \textbf{Historical Foundation}
\begin{itemize}
\item McCulloch-Pitts (1943): neurons compute
\item Hebb (1949): learning strengthens connections
\item Rosenblatt (1958): perceptron learns
\end{itemize}

\item \textbf{The Perceptron Model}
\begin{itemize}
\item Weighted sum + threshold
\item Linear decision boundary
\item Learns from mistakes
\end{itemize}

\item \textbf{Limitations}
\begin{itemize}
\item Only linearly separable problems
\item XOR is impossible
\item Led to AI Winter
\end{itemize}
\end{enumerate}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module1_perceptron/charts/module1_summary_diagram/module1_summary_diagram.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/module1_summary_diagram}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/module1_summary_diagram}{\includegraphics[width=0.6cm]{module1_perceptron/charts/module1_summary_diagram/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module1_perceptron/charts/module1_summary_diagram}{\tiny\texttt{\textcolor{gray}{module1\_summary\_diagram}}}
};
\end{tikzpicture}

\bottomnote{From biological inspiration to mathematical limitation}
\end{frame}

% Slide 50: Preview of Module 2
\begin{frame}[t]{Preview: Module 2}
\begin{center}
\Large
\textit{``What if we stack multiple perceptrons?''}
\end{center}

\vspace{1em}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Problem We Face}

Single perceptrons can only solve linearly separable problems. Real financial data is rarely that simple.

\vspace{0.5em}
\textbf{The Solution Preview:}
\begin{itemize}
\item Add ``hidden'' layers
\item Non-linear activation functions
\item Multi-Layer Perceptrons (MLPs)
\end{itemize}

\column{0.48\textwidth}
\textbf{Coming in Module 2:}
\begin{itemize}
\item How XOR gets solved
\item MLP architecture
\item Activation functions (sigmoid, ReLU)
\item Universal Approximation Theorem
\item Loss functions
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Spoiler:}} Adding just one hidden layer changes everything.
\end{columns}

\vspace{1em}
\textbf{Mathematical details for this module: See Appendix A (Perceptron Convergence Proof)}
\bottomnote{Next: Solving XOR with Multi-Layer Perceptrons}
\end{frame}



%% ============================================================================
%% MODULE2 MLP
%% ============================================================================



% ==================== SECTION 1: OPENING (Slides 1-5) ====================
\section{Opening}

% Slide 1: Title
\begin{frame}[plain]
\titlepage
\end{frame}

% Slide 2: Recap
\begin{frame}[t]{Where We Left Off}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Module 1 Summary}

We learned that a single perceptron:
\begin{itemize}
\item Takes weighted inputs
\item Applies a threshold
\item Outputs a binary decision
\item Can only draw \textbf{linear} boundaries
\end{itemize}

\vspace{0.5em}
\textbf{The Perceptron Equation:}
$$y = f\left(\sum_{i=1}^n w_i x_i + b\right)$$

\column{0.48\textwidth}
\textbf{The Problem}

The perceptron cannot solve XOR or any non-linearly separable problem.

\vspace{0.5em}
\textbf{The AI Winter:}
\begin{itemize}
\item Minsky-Papert (1969) critique
\item Funding dried up
\item ``Neural networks don't work''
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Today's Question:}}

What if we stack multiple perceptrons together?
\end{columns}
\bottomnote{The perceptron: powerful but limited}
\end{frame}

% Slide 3: The XOR Problem Revisited
\begin{frame}[t]{The XOR Problem Revisited}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Why One Line Isn't Enough}

\begin{center}
\begin{tabular}{cc|c}
\toprule
$x_1$ & $x_2$ & XOR \\
\midrule
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 0 \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.5em}
\textbf{The Geometry:}
\begin{itemize}
\item Opposite corners have same label
\item No single line can separate them
\item We need \textit{multiple} boundaries
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/xor_solution_mlp/xor_solution_mlp.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/xor_solution_mlp}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/xor_solution_mlp}{\includegraphics[width=0.6cm]{module2_mlp/charts/xor_solution_mlp/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/xor_solution_mlp}{\tiny\texttt{\textcolor{gray}{xor\_solution\_mlp}}}
};
\end{tikzpicture}

\bottomnote{Some patterns require more than a single line}
\end{frame}

% Slide 4: The Finance Parallel
\begin{frame}[t]{The Finance Parallel}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Single Analyst (Perceptron)}

One junior analyst screening stocks:
\begin{itemize}
\item Looks at a few metrics
\item Applies simple rules
\item Makes direct decisions
\item Limited perspective
\end{itemize}

\vspace{0.5em}
\textbf{Limitation:}

``Buy if P/E $<$ 15 AND momentum $>$ 0''

This is a single linear rule.

\column{0.48\textwidth}
\textbf{Investment Team (MLP)}

A hierarchical team:
\begin{itemize}
\item Junior analysts find patterns
\item Senior analysts synthesize
\item CIO makes final call
\item Complex reasoning emerges
\end{itemize}

\vspace{0.5em}
\textbf{Capability:}

``Consider value metrics, momentum signals, AND market regime together''

Multiple non-linear patterns.
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Key Insight:}} Hierarchical processing enables complex pattern recognition.
\bottomnote{A single analyst sees simple patterns. A team sees complex ones.}
\end{frame}

% Slide 5: Module 2 Roadmap
\begin{frame}[t]{Module 2 Roadmap}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What We'll Cover}

\begin{enumerate}
\item \textbf{Historical Context}
\begin{itemize}
\item AI Winter survival
\item Backprop rediscovery (1986)
\end{itemize}
\item \textbf{MLP Architecture}
\begin{itemize}
\item Intuition: The firm analogy
\item Math: Matrix notation
\end{itemize}
\item \textbf{Activation Functions}
\begin{itemize}
\item Why non-linearity matters
\item Sigmoid, Tanh, ReLU
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\begin{enumerate}
\setcounter{enumi}{3}
\item \textbf{Universal Approximation}
\begin{itemize}
\item The fundamental theorem
\item Implications and limits
\end{itemize}
\item \textbf{Loss Functions}
\begin{itemize}
\item MSE for regression
\item Cross-entropy for classification
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textbf{Learning Objectives:}
\begin{itemize}
\item Understand MLP architecture
\item Master matrix notation
\item Know when to use which activation
\item Appreciate universal approximation
\end{itemize}
\end{columns}
\bottomnote{From single perceptron to universal function approximation}
\end{frame}

% ==================== SECTION 2: HISTORICAL CONTEXT (Slides 6-12) ====================
\section{Historical Context: 1969-1986}

% Slide 6: The AI Winter
\begin{frame}[t]{The AI Winter (1969-1982)}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{After Minsky-Papert}

The neural network winter:
\begin{itemize}
\item Government funding cut
\item Researchers moved to other fields
\item ``Connectionism is dead''
\item Symbolic AI dominated
\end{itemize}

\vspace{0.5em}
\textbf{The Mood:}
\begin{itemize}
\item Perceptrons can't solve XOR
\item Multi-layer networks exist but...
\item No efficient training algorithm
\item Why bother?
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/timeline_1969_1986/timeline_1969_1986.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/timeline_1969_1986}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/timeline_1969_1986}{\includegraphics[width=0.6cm]{module2_mlp/charts/timeline_1969_1986/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/timeline_1969_1986}{\tiny\texttt{\textcolor{gray}{timeline\_1969\_1986}}}
};
\end{tikzpicture}

\bottomnote{After Minsky-Papert, neural network research nearly died}
\end{frame}

% Slide 7: Underground Progress
\begin{frame}[t]{Underground Progress}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Paul Werbos (1974)}

PhD thesis at Harvard:
\begin{itemize}
\item Derived backpropagation
\item For general non-linear systems
\item Applied to neural networks
\item \textcolor{mlred}{Largely ignored}
\end{itemize}

\vspace{0.5em}
\textbf{Why Ignored?}
\begin{itemize}
\item Published in economics, not CS
\item AI winter was at its coldest
\item No computational power to test
\item No community to spread ideas
\end{itemize}

\column{0.48\textwidth}
\textbf{Parallel Discoveries}

\vspace{0.5em}
\textbf{1970s:}
\begin{itemize}
\item Linnainmaa: automatic differentiation
\item Control theory: similar ideas
\end{itemize}

\textbf{1980s:}
\begin{itemize}
\item Parker (1982): rediscovery
\item LeCun (1985): independent work
\item Rumelhart/Hinton/Williams (1986): fame
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Lesson:}} Good ideas can be discovered multiple times before they ``take off.''
\end{columns}
\bottomnote{The key ideas existed but were ignored}
\end{frame}

% Slide 8: 1982 - Hopfield
\begin{frame}[t]{1982: Hopfield Networks}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{John Hopfield}

A physicist (not AI researcher) revived interest:
\begin{itemize}
\item Connected neural networks to physics
\item Energy-based formulation
\item Published in PNAS (prestigious)
\item Showed neural nets could store memories
\end{itemize}

\vspace{0.5em}
\textbf{The Impact:}
\begin{itemize}
\item Legitimized neural network research
\item Attracted physicists to the field
\item New mathematical tools
\item Funding started returning
\end{itemize}

\column{0.48\textwidth}
\textbf{Why Physics Helped}

\vspace{0.5em}
\textbf{Physics Connection:}
\begin{itemize}
\item Neurons $\leftrightarrow$ spins in magnets
\item Learning $\leftrightarrow$ energy minimization
\item Networks $\leftrightarrow$ statistical mechanics
\end{itemize}

\vspace{0.5em}
\textbf{Finance Parallel:}

Physicists would later apply similar ideas to:
\begin{itemize}
\item Option pricing
\item Market dynamics
\item Risk modeling
\item Quantitative finance
\end{itemize}
\end{columns}
\bottomnote{John Hopfield: Physicist rediscovers neural networks}
\end{frame}

% Slide 9: 1986 - The Breakthrough
\begin{frame}[t]{1986: The Backpropagation Paper}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Paper That Changed Everything}

Rumelhart, Hinton, Williams in Nature (1986):

``Learning representations by back-propagating errors''

\vspace{0.5em}
\textbf{Key Contributions:}
\begin{itemize}
\item Clear algorithm presentation
\item Demonstrated on real problems
\item Published in high-impact journal
\item Well-communicated to broad audience
\end{itemize}

\vspace{0.5em}
\textbf{The Result:}

Neural network renaissance begins.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/rumelhart_hinton_williams/rumelhart_hinton_williams.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/rumelhart_hinton_williams}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/rumelhart_hinton_williams}{\includegraphics[width=0.6cm]{module2_mlp/charts/rumelhart_hinton_williams/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/rumelhart_hinton_williams}{\tiny\texttt{\textcolor{gray}{rumelhart\_hinton\_williams}}}
};
\end{tikzpicture}

\bottomnote{Nature paper: ``Learning representations by back-propagating errors''}
\end{frame}

% Slide 10: What Made It Different?
\begin{frame}[t]{What Made 1986 Different?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Werbos (1974)}

\begin{itemize}
\item[\textcolor{mlgreen}{+}] Correct algorithm
\item[\textcolor{mlgreen}{+}] General framework
\item[\textcolor{mlred}{-}] Wrong field (economics)
\item[\textcolor{mlred}{-}] No demonstrations
\item[\textcolor{mlred}{-}] No community
\item[\textcolor{mlred}{-}] No computers
\end{itemize}

\column{0.48\textwidth}
\textbf{Rumelhart et al. (1986)}

\begin{itemize}
\item[\textcolor{mlgreen}{+}] Correct algorithm
\item[\textcolor{mlgreen}{+}] Clear presentation
\item[\textcolor{mlgreen}{+}] Compelling demos
\item[\textcolor{mlgreen}{+}] High-profile venue (Nature)
\item[\textcolor{mlgreen}{+}] Growing community
\item[\textcolor{mlgreen}{+}] Computers available
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Lesson for Researchers:}

Being right isn't enough. You need:
\begin{itemize}
\item The right timing
\item The right communication
\item The right audience
\item The right technology
\end{itemize}
\bottomnote{The right idea at the right time with the right people}
\end{frame}

% Slide 11: Discussion Question 1
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``Backpropagation was discovered multiple times (1974, 1982, 1986). Why do some discoveries get ignored while others take off? What role did timing play?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\begin{itemize}
\item Publication venue matters
\item Community readiness
\item Computational infrastructure
\item Demonstration quality
\end{itemize}

\column{0.48\textwidth}
\begin{itemize}
\item Today: transformers (2017) exploded
\item LSTMs existed since 1997
\item What changed?
\end{itemize}
\end{columns}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 12: The Renaissance Begins
\begin{frame}[t]{The Neural Network Renaissance}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{After 1986}

Neural networks were back:
\begin{itemize}
\item Funding returned
\item New conferences (NIPS, now NeurIPS)
\item ``Connectionism'' movement
\item Real applications emerged
\end{itemize}

\vspace{0.5em}
\textbf{Key Milestones:}
\begin{itemize}
\item 1989: LeNet for digit recognition
\item 1990s: Speech recognition
\item 1990s: Financial applications begin
\end{itemize}

\column{0.48\textwidth}
\textbf{But Challenges Remained}

\vspace{0.5em}
Not everything worked:
\begin{itemize}
\item Deep networks hard to train
\item Vanishing gradients
\item Limited compute power
\item Another ``winter'' in 2000s
\end{itemize}

\vspace{0.5em}
\textbf{True Revolution:} 2012

AlexNet on ImageNet marked the deep learning era. (Module 4)

\vspace{0.5em}
\textcolor{mlpurple}{\textit{But first, we need to understand the architecture...}}
\end{columns}
\bottomnote{Neural networks are back - and this time they can learn}
\end{frame}

% ==================== SECTION 3: MLP ARCHITECTURE - INTUITION (Slides 13-22) ====================
\section{MLP Architecture: Intuition}

% Slide 13: The Investment Firm Analogy
\begin{frame}[t]{The Investment Firm Analogy}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Hierarchical Decision Making}

\textbf{Level 1: Junior Analysts (Hidden Layer 1)}
\begin{itemize}
\item Look at raw data
\item Find basic patterns
\item ``This looks like a value stock''
\item ``This has momentum''
\end{itemize}

\textbf{Level 2: Senior Analysts (Hidden Layer 2)}
\begin{itemize}
\item Combine junior reports
\item Higher-level synthesis
\item ``Value + momentum = quality''
\end{itemize}

\column{0.48\textwidth}
\textbf{Level 3: CIO (Output Layer)}
\begin{itemize}
\item Final buy/sell decision
\item Combines all analyses
\item Single decision point
\end{itemize}

\vspace{0.5em}
\textbf{Key Properties:}
\begin{enumerate}
\item Information flows upward
\item Each level adds abstraction
\item Later layers see patterns in patterns
\item Final layer integrates everything
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{This is an MLP!}}
\end{columns}
\bottomnote{Hierarchical decision making}
\end{frame}

% Slide 14: Layer 1 - Data Gatherers
\begin{frame}[t]{Input Layer: The Data Gatherers}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Input Layer}

What it does:
\begin{itemize}
\item Receives raw data
\item One neuron per feature
\item No computation
\item Just passes data forward
\end{itemize}

\vspace{0.5em}
\textbf{In Finance:}
\begin{itemize}
\item P/E ratio
\item Momentum (returns)
\item Volume
\item Volatility
\item Sector indicators
\item Market cap
\end{itemize}

\column{0.48\textwidth}
\textbf{Notation}

$$\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$$

where:
\begin{itemize}
\item $n$ = number of features
\item $x_i$ = value of feature $i$
\end{itemize}

\vspace{0.5em}
\textbf{Example (n=4):}
$$\mathbf{x} = \begin{pmatrix} 15 \\ 0.08 \\ 1.2M \\ 0.25 \end{pmatrix} = \begin{pmatrix} \text{P/E} \\ \text{Return} \\ \text{Volume} \\ \text{Vol} \end{pmatrix}$$
\end{columns}
\bottomnote{The input layer receives raw information}
\end{frame}

% Slide 15: Hidden Layers - Pattern Finders
\begin{frame}[t]{Hidden Layers: The Pattern Finders}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{What Hidden Layers Do}

They discover intermediate patterns:
\begin{itemize}
\item Not explicitly programmed
\item Emerge from training
\item Often uninterpretable
\item But highly useful
\end{itemize}

\vspace{0.5em}
\textbf{Each Hidden Neuron:}
\begin{itemize}
\item Receives weighted inputs
\item Applies activation function
\item Outputs a single number
\item ``Detects'' a specific pattern
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/hidden_layer_representations/hidden_layer_representations.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/hidden_layer_representations}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/hidden_layer_representations}{\includegraphics[width=0.6cm]{module2_mlp/charts/hidden_layer_representations/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/hidden_layer_representations}{\tiny\texttt{\textcolor{gray}{hidden\_layer\_representations}}}
};
\end{tikzpicture}

\bottomnote{``They see things in the data you didn't explicitly ask for''}
\end{frame}

% Slide 16: What Hidden Layers Find
\begin{frame}[t]{Finance Example: What Hidden Layers Find}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Hypothetical Hidden Neurons}

\textbf{Hidden Neuron 1:} ``Value Detector''
\begin{itemize}
\item Positive weight on low P/E
\item Positive weight on high book value
\item Activates for value stocks
\end{itemize}

\textbf{Hidden Neuron 2:} ``Momentum Detector''
\begin{itemize}
\item Positive weight on recent returns
\item Positive weight on volume
\item Activates for trending stocks
\end{itemize}

\textbf{Hidden Neuron 3:} ``Risk Detector''
\begin{itemize}
\item Positive weight on volatility
\item Positive weight on debt
\item Activates for risky stocks
\end{itemize}

\column{0.48\textwidth}
\textbf{The Output Layer}

Combines hidden neuron outputs:

$$\text{Buy} = f(w_1 \cdot \text{Value} + w_2 \cdot \text{Momentum} - w_3 \cdot \text{Risk})$$

\vspace{0.5em}
\textbf{Key Insight:}

We never told the network what ``value'' or ``momentum'' means. It \textit{discovered} these concepts from data.

\vspace{0.5em}
\textbf{Caveat:}

Real hidden neurons may not be this interpretable. They might detect patterns we can't name.
\end{columns}
\bottomnote{Hidden neurons learn abstract concepts}
\end{frame}

% Slide 17: Output Layer - The Final Call
\begin{frame}[t]{Output Layer: The Final Decision}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Output Layer}

Takes hidden representations and produces:
\begin{itemize}
\item Classification: probability of class
\item Regression: continuous prediction
\item Multiple outputs possible
\end{itemize}

\vspace{0.5em}
\textbf{For Binary Classification:}

Single output neuron with sigmoid:
$$\hat{y} = \sigma(w^T h + b)$$

Output $\in (0, 1)$ interpreted as probability.

\vspace{0.5em}
\textbf{For Regression:}

Single output neuron with no activation (or linear):
$$\hat{y} = w^T h + b$$

Output is predicted value.

\column{0.48\textwidth}
\textbf{Finance Examples}

\vspace{0.5em}
\textbf{Buy/Sell Classification:}
\begin{itemize}
\item Output: $P(\text{Buy})$
\item If $> 0.5$: recommend Buy
\item If $< 0.5$: recommend Sell
\end{itemize}

\textbf{Return Prediction:}
\begin{itemize}
\item Output: predicted return
\item Could be next-day, next-month
\item Continuous value
\end{itemize}

\textbf{Multi-Class (Sector):}
\begin{itemize}
\item Multiple output neurons
\item Softmax activation
\item Each output = probability of sector
\end{itemize}
\end{columns}
\bottomnote{The output layer synthesizes everything into a decision}
\end{frame}

% Slide 18: The Full Architecture
\begin{frame}[t]{The Full MLP Architecture}
\begin{center}
\includegraphics[width=0.78\textwidth]{module2_mlp/charts/mlp_architecture_2_3_1/mlp_architecture_2_3_1.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/mlp_architecture_2_3_1}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/mlp_architecture_2_3_1}{\includegraphics[width=0.6cm]{module2_mlp/charts/mlp_architecture_2_3_1/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/mlp_architecture_2_3_1}{\tiny\texttt{\textcolor{gray}{mlp\_architecture\_2\_3\_1}}}
};
\end{tikzpicture}

\bottomnote{A complete multi-layer perceptron}
\end{frame}

% Slide 19: Why "Hidden"?
\begin{frame}[t]{Why Are They Called ``Hidden''?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{We Don't Observe Them Directly}

\textbf{Observable:}
\begin{itemize}
\item Input layer: the features we provide
\item Output layer: the prediction we get
\end{itemize}

\textbf{Hidden:}
\begin{itemize}
\item Internal representations
\item Not directly specified
\item Learned automatically
\item ``Hidden'' from us
\end{itemize}

\column{0.48\textwidth}
\textbf{We Don't Tell Them What to Learn}

\vspace{0.5em}
\textbf{Traditional ML:}

``Here are features: P/E, momentum, volume''

We engineer the features.

\vspace{0.5em}
\textbf{Deep Learning Philosophy:}

``Here is raw data. Find useful patterns.''

Network discovers features.

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Trade-off:}}

More automatic, but less interpretable.
\end{columns}
\bottomnote{Hidden layers discover features automatically}
\end{frame}

% Slide 20: Solving XOR
\begin{frame}[t]{How MLPs Solve XOR}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Two-Hidden-Neuron Solution}

\textbf{Hidden Neuron 1:}

Learns: ``Is it in the upper-right region?''

$h_1 = \sigma(w_{11}x_1 + w_{12}x_2 + b_1)$

\vspace{0.5em}
\textbf{Hidden Neuron 2:}

Learns: ``Is it in the lower-left region?''

$h_2 = \sigma(w_{21}x_1 + w_{22}x_2 + b_2)$

\vspace{0.5em}
\textbf{Output Neuron:}

Combines: ``If $h_1$ XOR $h_2$, output 1''

Each hidden neuron draws \textit{one} line. Together, they create a non-linear boundary.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/xor_solution_mlp/xor_solution_mlp.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/xor_solution_mlp}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/xor_solution_mlp}{\includegraphics[width=0.6cm]{module2_mlp/charts/xor_solution_mlp/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/xor_solution_mlp}{\tiny\texttt{\textcolor{gray}{xor\_solution\_mlp}}}
};
\end{tikzpicture}

\bottomnote{Multiple decision boundaries working together}
\end{frame}

% Slide 21: Discussion Question 2
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``If hidden layers find features automatically, why do we still need feature engineering in finance?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Arguments for Feature Engineering:}
\begin{itemize}
\item Domain knowledge helps
\item Less data needed
\item More interpretable
\item Faster training
\end{itemize}

\column{0.48\textwidth}
\textbf{Arguments Against:}
\begin{itemize}
\item Human biases
\item Miss non-obvious patterns
\item Deep learning works on raw data
\item ImageNet revolution
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Reality:}} In finance, hybrid approaches often work best.
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 22: Universal Approximation Preview
\begin{frame}[t]{Universal Approximation: The Big Promise}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{A Remarkable Theorem}

With just \textit{one} hidden layer and enough neurons, an MLP can approximate \textbf{any} continuous function to arbitrary accuracy.

\vspace{0.5em}
\textbf{Implications:}
\begin{itemize}
\item MLPs are universal function approximators
\item No pattern is too complex (in theory)
\item The architecture is not the bottleneck
\end{itemize}

\vspace{0.5em}
\textbf{Caveats:}
\begin{itemize}
\item ``Enough neurons'' may be exponential
\item Finding the right weights is hard
\item Theory vs practice gap
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/universal_approximation_demo/universal_approximation_demo.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/universal_approximation_demo}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/universal_approximation_demo}{\includegraphics[width=0.6cm]{module2_mlp/charts/universal_approximation_demo/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/universal_approximation_demo}{\tiny\texttt{\textcolor{gray}{universal\_approximation\_demo}}}
};
\end{tikzpicture}

\bottomnote{MLPs can learn ANY pattern (in theory)}
\end{frame}

% ==================== SECTION 4: MLP ARCHITECTURE - MATH (Slides 23-32) ====================
\section{MLP Architecture: Mathematical Formulation}

% Slide 23: Transition to Math
\begin{frame}[t]{Now Let's Formalize}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What You Already Know}

From the intuition section:
\begin{itemize}
\item Layers process sequentially
\item Each layer transforms its input
\item Hidden layers find patterns
\item Output layer makes predictions
\end{itemize}

\vspace{0.5em}
\textbf{What's Next}

\begin{itemize}
\item Matrix notation for efficiency
\item Precise forward pass equations
\item Parameter counting
\item Worked numerical examples
\end{itemize}

\column{0.48\textwidth}
\textbf{Why Matrix Notation?}

\vspace{0.5em}
\textbf{Without Matrices:}

Write $n \times m$ separate equations for each weight.

\vspace{0.5em}
\textbf{With Matrices:}

$$\mathbf{h} = f(\mathbf{W}\mathbf{x} + \mathbf{b})$$

One equation captures everything.

\vspace{0.5em}
\textbf{Benefits:}
\begin{itemize}
\item Compact notation
\item Efficient computation (GPUs)
\item Easier to implement
\item Clearer understanding
\end{itemize}
\end{columns}
\bottomnote{You understand the intuition. Let's write it precisely.}
\end{frame}

% Slide 24: Matrix Notation Introduction
\begin{frame}[t]{Matrix Notation: Why Matrices?}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Single Neuron (Scalar)}

$$h = f(w_1 x_1 + w_2 x_2 + w_3 x_3 + b)$$

\vspace{0.5em}
\textbf{As Dot Product:}

$$h = f(\mathbf{w}^T \mathbf{x} + b)$$

where $\mathbf{w}, \mathbf{x} \in \mathbb{R}^3$

\vspace{0.5em}
\textbf{Multiple Neurons (Matrix):}

$$\mathbf{h} = f(\mathbf{W}\mathbf{x} + \mathbf{b})$$

where $\mathbf{W} \in \mathbb{R}^{m \times n}$

Each \textit{row} of $\mathbf{W}$ is the weights for one hidden neuron.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/matrix_multiplication_visual/matrix_multiplication_visual.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/matrix_multiplication_visual}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/matrix_multiplication_visual}{\includegraphics[width=0.6cm]{module2_mlp/charts/matrix_multiplication_visual/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/matrix_multiplication_visual}{\tiny\texttt{\textcolor{gray}{matrix\_multiplication\_visual}}}
};
\end{tikzpicture}

\bottomnote{Matrices make neural network math elegant}
\end{frame}

% Slide 25: Weight Matrix Definition
\begin{frame}[t]{The Weight Matrix}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Weight Matrix $\mathbf{W}^{(l)}$}

For layer $l$:

$$\mathbf{W}^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}$$

where:
\begin{itemize}
\item $n_l$ = neurons in layer $l$
\item $n_{l-1}$ = neurons in layer $l-1$
\end{itemize}

\vspace{0.5em}
\textbf{Entry $W_{ij}^{(l)}$:}

Weight from neuron $j$ in layer $l-1$ to neuron $i$ in layer $l$.

\column{0.48\textwidth}
\textbf{Bias Vector $\mathbf{b}^{(l)}$}

$$\mathbf{b}^{(l)} \in \mathbb{R}^{n_l}$$

One bias per neuron in layer $l$.

\vspace{0.5em}
\textbf{Example: 4-3 Layer}

Input: 4 neurons, Hidden: 3 neurons

$$\mathbf{W}^{(1)} = \begin{pmatrix} w_{11} & w_{12} & w_{13} & w_{14} \\ w_{21} & w_{22} & w_{23} & w_{24} \\ w_{31} & w_{32} & w_{33} & w_{34} \end{pmatrix}$$

Size: $3 \times 4$ (12 weights)

$\mathbf{b}^{(1)} \in \mathbb{R}^3$ (3 biases)
\end{columns}
\bottomnote{Each layer has its own weight matrix}
\end{frame}

% Slide 26: Forward Pass - Layer by Layer
\begin{frame}[t]{Forward Pass: Layer by Layer}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{One Layer Computation}

$$\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$$
$$\mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})$$

where:
\begin{itemize}
\item $\mathbf{z}^{(l)}$: pre-activation (weighted sum)
\item $\mathbf{a}^{(l)}$: activation (after $f$)
\item $\mathbf{a}^{(0)} = \mathbf{x}$: input
\end{itemize}

\vspace{0.5em}
\textbf{The Steps:}
\begin{enumerate}
\item Matrix multiply: $\mathbf{W}^{(l)} \mathbf{a}^{(l-1)}$
\item Add bias: $+ \mathbf{b}^{(l)}$
\item Apply activation: $f(\cdot)$
\end{enumerate}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/layer_by_layer_computation/layer_by_layer_computation.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/layer_by_layer_computation}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/layer_by_layer_computation}{\includegraphics[width=0.6cm]{module2_mlp/charts/layer_by_layer_computation/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/layer_by_layer_computation}{\tiny\texttt{\textcolor{gray}{layer\_by\_layer\_computation}}}
};
\end{tikzpicture}

\bottomnote{Computing outputs one layer at a time}
\end{frame}

% Slide 27: Full Forward Pass
\begin{frame}[t]{The Complete Forward Pass}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{For an L-Layer Network}

\textbf{Input:}
$$\mathbf{a}^{(0)} = \mathbf{x}$$

\textbf{Hidden Layers} ($l = 1, \ldots, L-1$):
$$\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$$
$$\mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})$$

\textbf{Output Layer:}
$$\mathbf{z}^{(L)} = \mathbf{W}^{(L)} \mathbf{a}^{(L-1)} + \mathbf{b}^{(L)}$$
$$\hat{\mathbf{y}} = g(\mathbf{z}^{(L)})$$

where $g$ may differ from $f$.

\column{0.48\textwidth}
\textbf{Example: 2-Layer Network}

\vspace{0.5em}
\textbf{Layer 1 (hidden):}
$$\mathbf{z}^{(1)} = \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}$$
$$\mathbf{a}^{(1)} = \text{ReLU}(\mathbf{z}^{(1)})$$

\textbf{Layer 2 (output):}
$$\mathbf{z}^{(2)} = \mathbf{W}^{(2)} \mathbf{a}^{(1)} + \mathbf{b}^{(2)}$$
$$\hat{y} = \sigma(\mathbf{z}^{(2)})$$

\vspace{0.5em}
\textbf{Compact Form:}
$$\hat{y} = \sigma(\mathbf{W}^{(2)} \text{ReLU}(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}) + \mathbf{b}^{(2)})$$
\end{columns}
\bottomnote{Chaining layer computations together}
\end{frame}

% Slide 28: Dimensions Matter
\begin{frame}[t]{Dimensions Matter}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Dimension Checking}

For $\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$:

\vspace{0.5em}
\begin{tabular}{ll}
$\mathbf{W}$: & $(n_{\text{out}} \times n_{\text{in}})$ \\
$\mathbf{x}$: & $(n_{\text{in}} \times 1)$ \\
$\mathbf{Wx}$: & $(n_{\text{out}} \times 1)$ \\
$\mathbf{b}$: & $(n_{\text{out}} \times 1)$ \\
$\mathbf{z}$: & $(n_{\text{out}} \times 1)$ \\
\end{tabular}

\vspace{0.5em}
\textbf{Rule:}

Inner dimensions must match.

$(m \times \mathbf{n}) \times (\mathbf{n} \times p) = (m \times p)$

\column{0.48\textwidth}
\textbf{Example: 4-3-1 Network}

\vspace{0.5em}
\textbf{Layer 1:}
\begin{itemize}
\item $\mathbf{W}^{(1)}$: $3 \times 4$
\item $\mathbf{x}$: $4 \times 1$
\item $\mathbf{z}^{(1)}$: $3 \times 1$
\end{itemize}

\textbf{Layer 2:}
\begin{itemize}
\item $\mathbf{W}^{(2)}$: $1 \times 3$
\item $\mathbf{a}^{(1)}$: $3 \times 1$
\item $\mathbf{z}^{(2)}$: $1 \times 1$ (scalar)
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Common Error:}} Transposed matrices. Always check dimensions!
\end{columns}
\bottomnote{Matrix dimensions must be compatible}
\end{frame}

% Slide 29: Worked Example
\begin{frame}[t]{Worked Example: 2-3-1 Network}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Network Setup}

Input: $\mathbf{x} = \begin{pmatrix} 0.5 \\ 0.8 \end{pmatrix}$

\vspace{0.3em}
Layer 1 weights:
$$\mathbf{W}^{(1)} = \begin{pmatrix} 0.2 & 0.4 \\ 0.3 & 0.1 \\ 0.5 & 0.2 \end{pmatrix}$$

$\mathbf{b}^{(1)} = \begin{pmatrix} 0.1 \\ -0.1 \\ 0.0 \end{pmatrix}$

\vspace{0.3em}
Layer 2 weights:
$$\mathbf{W}^{(2)} = \begin{pmatrix} 0.6 & 0.3 & 0.4 \end{pmatrix}$$

$b^{(2)} = -0.2$

\column{0.48\textwidth}
\textbf{Forward Pass}

\textbf{Layer 1:}
$$\mathbf{z}^{(1)} = \begin{pmatrix} 0.2(0.5) + 0.4(0.8) + 0.1 \\ 0.3(0.5) + 0.1(0.8) - 0.1 \\ 0.5(0.5) + 0.2(0.8) + 0.0 \end{pmatrix} = \begin{pmatrix} 0.52 \\ 0.13 \\ 0.41 \end{pmatrix}$$

$\mathbf{a}^{(1)} = \text{ReLU}(\mathbf{z}^{(1)}) = \begin{pmatrix} 0.52 \\ 0.13 \\ 0.41 \end{pmatrix}$

\textbf{Layer 2:}
$$z^{(2)} = 0.6(0.52) + 0.3(0.13) + 0.4(0.41) - 0.2 = 0.315$$

$\hat{y} = \sigma(0.315) = 0.578$

\textcolor{mlgreen}{\textbf{Output: 57.8\% probability of class 1}}
\end{columns}
\bottomnote{Following the numbers through the network}
\end{frame}

% Slide 30: Parameter Counting
\begin{frame}[t]{Counting Parameters}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Parameters per Layer}

For layer $l$ with $n_{l-1}$ inputs and $n_l$ outputs:

\vspace{0.5em}
\textbf{Weights:} $n_l \times n_{l-1}$

\textbf{Biases:} $n_l$

\textbf{Total:} $n_l \times n_{l-1} + n_l = n_l(n_{l-1} + 1)$

\vspace{0.5em}
\textbf{Network Total:}

$$\text{Params} = \sum_{l=1}^{L} n_l(n_{l-1} + 1)$$

\column{0.48\textwidth}
\textbf{Example: 4-10-5-1 Network}

\vspace{0.5em}
\textbf{Layer 1} (4 $\rightarrow$ 10):
$$10 \times 4 + 10 = 50$$

\textbf{Layer 2} (10 $\rightarrow$ 5):
$$5 \times 10 + 5 = 55$$

\textbf{Layer 3} (5 $\rightarrow$ 1):
$$1 \times 5 + 1 = 6$$

\textbf{Total: 111 parameters}

\vspace{0.5em}
For 100 training samples: $<2$ samples per parameter. Risk of overfitting!
\end{columns}
\bottomnote{How many weights does your network have?}
\end{frame}

% Slide 31: Discussion Question 3
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``A 4-10-5-1 network has how many parameters? Calculate and discuss: is this a lot or a little for stock prediction?''}
\end{center}

\vspace{1em}
\textbf{Answer: 111 parameters}

\vspace{0.5em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Stock Data Context:}
\begin{itemize}
\item Daily data: $\sim$252 days/year
\item 10 years = 2,520 samples
\item 111 params: 23 samples/param
\item Seems okay...
\end{itemize}

\column{0.48\textwidth}
\textbf{But Also Consider:}
\begin{itemize}
\item Financial regimes change
\item Not all data equally relevant
\item Need train/val/test split
\item Model complexity vs data size
\end{itemize}
\end{columns}
\bottomnote{Exercise: 3 minutes}
\end{frame}

% Slide 32: Finance MLP Architecture
\begin{frame}[t]{Finance Example: Multi-Factor Stock Prediction}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{A Realistic Setup}

\textbf{Input Features (10):}
\begin{itemize}
\item P/E, P/B, EV/EBITDA (value)
\item 1m, 3m, 6m returns (momentum)
\item 20d volatility (risk)
\item Volume ratio (liquidity)
\item Sector one-hot (2 features)
\end{itemize}

\textbf{Architecture:}
\begin{itemize}
\item Hidden 1: 20 neurons (ReLU)
\item Hidden 2: 10 neurons (ReLU)
\item Output: 1 neuron (sigmoid)
\end{itemize}

\textbf{Total: 441 parameters}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/finance_mlp_architecture/finance_mlp_architecture.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/finance_mlp_architecture}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/finance_mlp_architecture}{\includegraphics[width=0.6cm]{module2_mlp/charts/finance_mlp_architecture/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/finance_mlp_architecture}{\tiny\texttt{\textcolor{gray}{finance\_mlp\_architecture}}}
};
\end{tikzpicture}

\bottomnote{Multiple factors combined through hidden layers}
\end{frame}

% ==================== SECTION 5: ACTIVATION FUNCTIONS (Slides 33-42) ====================
\section{Activation Functions}

% Slide 33: Why Non-Linearity?
\begin{frame}[t]{Why Non-Linearity?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Core Problem}

Without activation functions:

$$\mathbf{a}^{(1)} = \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}$$
$$\hat{\mathbf{y}} = \mathbf{W}^{(2)} \mathbf{a}^{(1)} + \mathbf{b}^{(2)}$$

Substituting:
$$\hat{\mathbf{y}} = \mathbf{W}^{(2)}(\mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}) + \mathbf{b}^{(2)}$$
$$= (\mathbf{W}^{(2)}\mathbf{W}^{(1)}) \mathbf{x} + (\mathbf{W}^{(2)}\mathbf{b}^{(1)} + \mathbf{b}^{(2)})$$
$$= \mathbf{W}' \mathbf{x} + \mathbf{b}'$$

\textcolor{mlred}{\textbf{Result:}} A single linear transformation!

\column{0.48\textwidth}
\textbf{The Solution}

Non-linear activation functions:
$$\mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})$$

where $f$ is non-linear.

\vspace{0.5em}
\textbf{Why This Works:}
\begin{itemize}
\item Non-linearity breaks the collapse
\item Composition of non-linear functions
\item Can approximate any function
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Key Insight:}}

Non-linearity is what makes deep networks ``deep'' in a meaningful sense.
\end{columns}
\bottomnote{Non-linearity is essential for learning complex patterns}
\end{frame}

% Slide 34: Linear Networks Collapse
\begin{frame}[t]{Linear Networks Collapse}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Mathematical Proof}

For any number of linear layers:

$$\mathbf{y} = \mathbf{W}^{(L)} \mathbf{W}^{(L-1)} \cdots \mathbf{W}^{(1)} \mathbf{x}$$

Since matrix multiplication is associative:

$$= (\mathbf{W}^{(L)} \mathbf{W}^{(L-1)} \cdots \mathbf{W}^{(1)}) \mathbf{x}$$

$$= \mathbf{W}^{\text{eff}} \mathbf{x}$$

\vspace{0.5em}
\textbf{Conclusion:}

100 linear layers = 1 linear layer.

No benefit from depth without non-linearity.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/linear_collapse_proof/linear_collapse_proof.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/linear_collapse_proof}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/linear_collapse_proof}{\includegraphics[width=0.6cm]{module2_mlp/charts/linear_collapse_proof/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/linear_collapse_proof}{\tiny\texttt{\textcolor{gray}{linear\_collapse\_proof}}}
};
\end{tikzpicture}

\bottomnote{Stacked linear layers = single linear layer}
\end{frame}

% Slide 35: The Sigmoid Function
\begin{frame}[t]{The Sigmoid Function}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Definition}

$$\sigma(z) = \frac{1}{1+e^{-z}}$$

\vspace{0.5em}
\textbf{Properties:}
\begin{itemize}
\item Range: $(0, 1)$
\item Smooth and differentiable
\item $\sigma(0) = 0.5$
\item Symmetric: $\sigma(-z) = 1 - \sigma(z)$
\end{itemize}

\textbf{Derivative:}
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$

\vspace{0.5em}
\textbf{Use Cases:}
\begin{itemize}
\item Binary classification (output)
\item Probability interpretation
\item Historical (hidden layers)
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/sigmoid_function/sigmoid_function.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/sigmoid_function}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/sigmoid_function}{\includegraphics[width=0.6cm]{module2_mlp/charts/sigmoid_function/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/sigmoid_function}{\tiny\texttt{\textcolor{gray}{sigmoid\_function}}}
};
\end{tikzpicture}

\bottomnote{The classic activation: squashes to probability}
\end{frame}

% Slide 36: Sigmoid Properties
\begin{frame}[t]{Sigmoid: Properties and Problems}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Advantages}
\begin{itemize}
\item[\textcolor{mlgreen}{+}] Bounded output $(0, 1)$
\item[\textcolor{mlgreen}{+}] Smooth gradient
\item[\textcolor{mlgreen}{+}] Probability interpretation
\item[\textcolor{mlgreen}{+}] Historically important
\end{itemize}

\vspace{0.5em}
\textbf{Disadvantages}
\begin{itemize}
\item[\textcolor{mlred}{-}] \textbf{Vanishing gradients}
\item[] For $|z| > 4$: $\sigma'(z) \approx 0$
\item[] Gradients become tiny
\item[] Deep networks can't learn
\item[\textcolor{mlred}{-}] Not zero-centered
\item[] All positive outputs
\item[] Zig-zag weight updates
\item[\textcolor{mlred}{-}] Computationally expensive
\item[] Requires $\exp$ function
\end{itemize}

\column{0.48\textwidth}
\textbf{The Vanishing Gradient Problem}

\vspace{0.5em}
When $z$ is very positive or negative:

\begin{center}
\begin{tabular}{cc}
$z$ & $\sigma'(z)$ \\
\midrule
0 & 0.25 \\
2 & 0.10 \\
4 & 0.018 \\
6 & 0.0025 \\
\end{tabular}
\end{center}

\vspace{0.5em}
Gradients shrink exponentially through layers!

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Result:}} Early layers learn very slowly in deep networks. This limited deep learning until ReLU.
\end{columns}
\bottomnote{Smooth and bounded, but gradients can vanish}
\end{frame}

% Slide 37: The Tanh Function
\begin{frame}[t]{The Tanh Function}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Definition}

$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\sigma(2z) - 1$$

\vspace{0.5em}
\textbf{Properties:}
\begin{itemize}
\item Range: $(-1, 1)$
\item Zero-centered
\item $\tanh(0) = 0$
\item Odd function: $\tanh(-z) = -\tanh(z)$
\end{itemize}

\textbf{Derivative:}
$$\tanh'(z) = 1 - \tanh^2(z)$$

\vspace{0.5em}
\textbf{Advantage over Sigmoid:}

Zero-centered outputs lead to more stable gradient updates.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/tanh_function/tanh_function.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/tanh_function}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/tanh_function}{\includegraphics[width=0.6cm]{module2_mlp/charts/tanh_function/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/tanh_function}{\tiny\texttt{\textcolor{gray}{tanh\_function}}}
};
\end{tikzpicture}

\bottomnote{Zero-centered: range (-1, 1)}
\end{frame}

% Slide 38: The ReLU Function
\begin{frame}[t]{ReLU: Rectified Linear Unit}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Definition}

$$\text{ReLU}(z) = \max(0, z) = \begin{cases} z & z > 0 \\ 0 & z \leq 0 \end{cases}$$

\vspace{0.5em}
\textbf{Properties:}
\begin{itemize}
\item Range: $[0, \infty)$
\item Not bounded above
\item Not differentiable at $z=0$
\item Piecewise linear
\end{itemize}

\textbf{Derivative:}
$$\text{ReLU}'(z) = \begin{cases} 1 & z > 0 \\ 0 & z \leq 0 \end{cases}$$

\vspace{0.5em}
\textbf{The Modern Default} for hidden layers.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/relu_function/relu_function.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/relu_function}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/relu_function}{\includegraphics[width=0.6cm]{module2_mlp/charts/relu_function/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/relu_function}{\tiny\texttt{\textcolor{gray}{relu\_function}}}
};
\end{tikzpicture}

\bottomnote{Simple but powerful: the modern default}
\end{frame}

% Slide 39: Why ReLU Works
\begin{frame}[t]{Why ReLU Works So Well}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Advantages}
\begin{itemize}
\item[\textcolor{mlgreen}{+}] \textbf{No vanishing gradient}
\item[] Gradient is 1 for $z > 0$
\item[] Signal propagates through layers
\item[\textcolor{mlgreen}{+}] \textbf{Computationally cheap}
\item[] Just comparison and assignment
\item[] No exponentials
\item[] 6x faster than sigmoid
\item[\textcolor{mlgreen}{+}] \textbf{Sparse activation}
\item[] Many neurons output 0
\item[] Efficient representation
\item[\textcolor{mlgreen}{+}] \textbf{Biological plausibility}
\item[] Neurons can be ``off''
\end{itemize}

\column{0.48\textwidth}
\textbf{Disadvantages}
\begin{itemize}
\item[\textcolor{mlred}{-}] \textbf{``Dying ReLU'' problem}
\item[] If $z < 0$ always: gradient = 0
\item[] Neuron never updates
\item[] Can ``die'' permanently
\item[\textcolor{mlred}{-}] Not zero-centered
\item[\textcolor{mlred}{-}] Unbounded (can explode)
\end{itemize}

\vspace{0.5em}
\textbf{Variants:}
\begin{itemize}
\item Leaky ReLU: $\max(0.01z, z)$
\item ELU: $z$ if $z>0$, $\alpha(e^z-1)$ otherwise
\item GELU: used in transformers
\end{itemize}
\end{columns}
\bottomnote{Cheap to compute, gradients don't vanish (for positive inputs)}
\end{frame}

% Slide 40: Activation Comparison
\begin{frame}[t]{Activation Functions: Comparison}
\begin{center}
\includegraphics[width=0.55\textwidth]{module2_mlp/charts/activation_comparison/activation_comparison.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/activation_comparison}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/activation_comparison}{\includegraphics[width=0.6cm]{module2_mlp/charts/activation_comparison/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/activation_comparison}{\tiny\texttt{\textcolor{gray}{activation\_comparison}}}
};
\end{tikzpicture}

\bottomnote{Different functions for different problems}
\end{frame}

% Slide 41: Discussion Question 4
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``Which activation function would you use for: (a) predicting stock returns, (b) buy/sell classification? Why?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{(a) Stock Returns (Regression)}
\begin{itemize}
\item Output: continuous value
\item Can be positive or negative
\item Hidden: ReLU or tanh
\item Output: \textcolor{mlgreen}{\textbf{Linear (none)}}
\item Returns are unbounded
\end{itemize}

\column{0.48\textwidth}
\textbf{(b) Buy/Sell (Classification)}
\begin{itemize}
\item Output: probability $\in (0,1)$
\item Two mutually exclusive classes
\item Hidden: ReLU
\item Output: \textcolor{mlgreen}{\textbf{Sigmoid}}
\item Or softmax for multi-class
\end{itemize}
\end{columns}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 42: Choosing Activation Functions
\begin{frame}[t]{Choosing the Right Activation}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Hidden Layer Guidelines}

\vspace{0.5em}
\textbf{Default:} ReLU
\begin{itemize}
\item Works well in most cases
\item Fast and stable
\end{itemize}

\textbf{If dying ReLU:} Leaky ReLU
\begin{itemize}
\item Small negative slope
\item Prevents dead neurons
\end{itemize}

\textbf{For RNNs:} Tanh
\begin{itemize}
\item Bounded outputs help stability
\item Zero-centered
\end{itemize}

\column{0.48\textwidth}
\textbf{Output Layer Guidelines}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Task} & \textbf{Activation} \\
\midrule
Binary class & Sigmoid \\
Multi-class & Softmax \\
Regression & Linear \\
Bounded regression & Sigmoid/tanh \\
Positive only & ReLU \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Finance Examples:}
\begin{itemize}
\item Return prediction: Linear
\item Direction prediction: Sigmoid
\item Sector classification: Softmax
\item Volatility: ReLU or Softplus
\end{itemize}
\end{columns}
\bottomnote{Output layer choice depends on your problem type}
\end{frame}

% ==================== SECTION 6: UNIVERSAL APPROXIMATION (Slides 43-48) ====================
\section{Universal Approximation}

% Slide 43: The Fundamental Question
\begin{frame}[t]{The Fundamental Question}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{How Powerful Are Neural Networks?}

We've seen that MLPs can:
\begin{itemize}
\item Solve XOR (non-linear patterns)
\item Combine features hierarchically
\item Learn from data
\end{itemize}

\vspace{0.5em}
\textbf{But a Deeper Question:}

Are there functions that MLPs fundamentally \textit{cannot} represent?

\vspace{0.5em}
Or can they approximate \textit{anything}?

\column{0.48\textwidth}
\textbf{Why This Matters}

\vspace{0.5em}
\textbf{If MLPs are limited:}
\begin{itemize}
\item Need to check if problem is solvable
\item Architecture constraints matter
\item Some patterns impossible
\end{itemize}

\textbf{If MLPs are universal:}
\begin{itemize}
\item Architecture is not the bottleneck
\item Challenges are elsewhere (data, training)
\item Theoretical guarantee of capability
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Spoiler:}} MLPs are universal approximators!
\end{columns}
\bottomnote{Just how powerful are neural networks?}
\end{frame}

% Slide 44: The Theorem Statement
\begin{frame}[t]{Universal Approximation Theorem}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Theorem (Informal)}

A feedforward network with:
\begin{itemize}
\item One hidden layer
\item Sufficient hidden neurons
\item Non-linear activation (e.g., sigmoid)
\end{itemize}

can approximate any continuous function on a compact domain to arbitrary accuracy.

\vspace{0.5em}
\textbf{Key Contributors:}
\begin{itemize}
\item Cybenko (1989): sigmoid
\item Hornik (1991): general activations
\item Further extensions since
\end{itemize}

\column{0.48\textwidth}
\textbf{Formal Statement}

Let $f: [0,1]^n \rightarrow \mathbb{R}$ be continuous.

For any $\epsilon > 0$, there exists an MLP $\hat{f}$ with:

$$|\hat{f}(\mathbf{x}) - f(\mathbf{x})| < \epsilon$$

for all $\mathbf{x} \in [0,1]^n$.

\vspace{0.5em}
\textbf{In Plain English:}

No matter how complex the pattern, an MLP with enough hidden neurons can match it as closely as you want.
\end{columns}
\bottomnote{With enough hidden neurons, you can approximate any continuous function}
\end{frame}

% Slide 45: What It Means
\begin{frame}[t]{What Universal Approximation Means}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Good News}

\begin{itemize}
\item No function is ``too complex''
\item MLPs are theoretically complete
\item Architecture is not the limit
\item One hidden layer is enough (in theory)
\end{itemize}

\vspace{0.5em}
\textbf{Visual Intuition:}

Each hidden neuron contributes a ``bump'' or ``step.'' With enough bumps, you can approximate any shape.

\vspace{0.5em}
Think of it like approximating a curve with many small line segments.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/universal_approximation_demo/universal_approximation_demo.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/universal_approximation_demo}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/universal_approximation_demo}{\includegraphics[width=0.6cm]{module2_mlp/charts/universal_approximation_demo/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/universal_approximation_demo}{\tiny\texttt{\textcolor{gray}{universal\_approximation\_demo}}}
};
\end{tikzpicture}

\bottomnote{More neurons = better approximation}
\end{frame}

% Slide 46: What It Doesn't Mean
\begin{frame}[t]{What It Doesn't Mean}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Common Misconceptions}

\textbf{``Any network can learn anything''}
\begin{itemize}
\item[\textcolor{mlred}{No.}] Need enough neurons
\item May need exponentially many
\end{itemize}

\textbf{``Training will find the solution''}
\begin{itemize}
\item[\textcolor{mlred}{No.}] Theorem is about existence
\item Says nothing about finding weights
\item Optimization may fail
\end{itemize}

\textbf{``One layer is always enough''}
\begin{itemize}
\item[\textcolor{mlred}{Technically yes, practically no.}]
\item Deep networks often more efficient
\item Fewer parameters for same accuracy
\end{itemize}

\column{0.48\textwidth}
\textbf{The Gap: Existence vs Construction}

\vspace{0.5em}
\textbf{The theorem says:}

``A good approximation exists.''

\textbf{It does NOT say:}
\begin{itemize}
\item How many neurons you need
\item How to find the right weights
\item How much data is required
\item How long training takes
\item Whether it will generalize
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Analogy:}}

``There exists a needle in this haystack'' doesn't help you find it.
\end{columns}
\bottomnote{Existence of a solution does not mean we can find it}
\end{frame}

% Slide 47: Theory vs Practice
\begin{frame}[t]{Theory vs Practice}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Theoretical Guarantees}

Universal approximation says:
\begin{itemize}
\item Given infinite neurons: perfect fit
\item Given infinite data: find the function
\item Given infinite compute: optimize
\end{itemize}

\vspace{0.5em}
\textbf{Practical Reality}

We have:
\begin{itemize}
\item Finite neurons: limited capacity
\item Finite data: must generalize
\item Finite compute: approximate solutions
\end{itemize}

\column{0.48\textwidth}
\textbf{What Matters More in Practice}

\vspace{0.5em}
\begin{enumerate}
\item \textbf{Data quality and quantity}
\begin{itemize}
\item More important than architecture
\end{itemize}
\item \textbf{Regularization}
\begin{itemize}
\item Prevent overfitting
\end{itemize}
\item \textbf{Optimization}
\begin{itemize}
\item Finding good weights
\end{itemize}
\item \textbf{Generalization}
\begin{itemize}
\item Performance on new data
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Module 3}} will address these practical challenges.
\end{columns}
\bottomnote{Universal approximation is necessary but not sufficient}
\end{frame}

% Slide 48: Implications for Finance
\begin{frame}[t]{Implications for Finance}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Optimistic View}

If markets have patterns, MLPs can learn them:
\begin{itemize}
\item Non-linear relationships? Possible.
\item Complex interactions? Possible.
\item Hidden factors? Possible.
\end{itemize}

\vspace{0.5em}
\textbf{Theoretical Capability:}

``An MLP could, in principle, capture any market pattern.''

\column{0.48\textwidth}
\textbf{The Realistic View}

\vspace{0.5em}
\textbf{Challenges Remain:}
\begin{itemize}
\item Signal-to-noise ratio is low
\item Markets are non-stationary
\item Past patterns may not repeat
\item Data is limited (especially for crashes)
\item Overfitting is easy
\end{itemize}

\vspace{0.5em}
\textbf{The EMH Counterargument:}

If markets are efficient, there's nothing systematic to learn.

\vspace{0.5em}
\textcolor{mlpurple}{\textit{Module 4 will explore this tension.}}
\end{columns}
\bottomnote{In theory, yes. In practice, many challenges remain.}
\end{frame}

% ==================== SECTION 7: LOSS FUNCTIONS (Slides 49-53) ====================
\section{Loss Functions}

% Slide 49: Why Loss Functions?
\begin{frame}[t]{Why Loss Functions?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Learning Requires an Objective}

To train a neural network, we need:
\begin{enumerate}
\item A way to measure errors
\item A number that decreases as we improve
\item A signal for weight updates
\end{enumerate}

\vspace{0.5em}
\textbf{The Loss Function:}

$\mathcal{L}(\hat{\mathbf{y}}, \mathbf{y})$ measures how wrong our predictions are.

\vspace{0.5em}
\textbf{Goal of Training:}

Find weights that minimize $\mathcal{L}$.

\column{0.48\textwidth}
\textbf{Finance Analogy}

\vspace{0.5em}
\textbf{Profit \& Loss (P\&L):}
\begin{itemize}
\item Measures trading performance
\item Negative P\&L = bad trades
\item Optimize to maximize P\&L
\end{itemize}

\textbf{Loss Function:}
\begin{itemize}
\item Measures prediction errors
\item High loss = bad predictions
\item Optimize to minimize loss
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Note:}} ``Loss'' is the opposite of ``profit'' -- we minimize loss!
\end{columns}
\bottomnote{To learn, we must measure mistakes}
\end{frame}

% Slide 50: Mean Squared Error
\begin{frame}[t]{Mean Squared Error (MSE)}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Definition}

$$\mathcal{L}_{\text{MSE}} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2$$

\vspace{0.5em}
\textbf{Properties:}
\begin{itemize}
\item Always non-negative
\item Zero only if perfect predictions
\item Penalizes large errors heavily
\item Differentiable everywhere
\end{itemize}

\textbf{Use Case:}
\begin{itemize}
\item Regression problems
\item Predicting continuous values
\item Stock returns, prices, etc.
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/mse_visualization/mse_visualization.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/mse_visualization}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/mse_visualization}{\includegraphics[width=0.6cm]{module2_mlp/charts/mse_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/mse_visualization}{\tiny\texttt{\textcolor{gray}{mse\_visualization}}}
};
\end{tikzpicture}

\bottomnote{The standard loss for predicting continuous values}
\end{frame}

% Slide 51: Cross-Entropy Loss
\begin{frame}[t]{Cross-Entropy Loss}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Binary Cross-Entropy}

$$\mathcal{L}_{\text{BCE}} = -\frac{1}{n}\sum_{i=1}^n [y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

\vspace{0.5em}
\textbf{Properties:}
\begin{itemize}
\item For probability outputs
\item Heavily penalizes confident wrong answers
\item Connected to information theory
\end{itemize}

\textbf{Use Case:}
\begin{itemize}
\item Classification problems
\item Buy/sell decisions
\item Any yes/no prediction
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/cross_entropy_visualization/cross_entropy_visualization.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/cross_entropy_visualization}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/cross_entropy_visualization}{\includegraphics[width=0.6cm]{module2_mlp/charts/cross_entropy_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/cross_entropy_visualization}{\tiny\texttt{\textcolor{gray}{cross\_entropy\_visualization}}}
};
\end{tikzpicture}

\bottomnote{The standard loss for classification}
\end{frame}

% Slide 52: Loss Landscape
\begin{frame}[t]{The Loss Landscape}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Loss as a Function of Weights}

$$\mathcal{L}(\mathbf{W}, \mathbf{b})$$

For every choice of weights, there's a loss value.

\vspace{0.5em}
\textbf{The Landscape:}
\begin{itemize}
\item High regions: bad weights
\item Low regions: good weights
\item Global minimum: best weights
\item Local minima: traps
\end{itemize}

\textbf{Training = }

Finding the lowest point in this landscape.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module2_mlp/charts/loss_landscape_3d/loss_landscape_3d.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/loss_landscape_3d}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/loss_landscape_3d}{\includegraphics[width=0.6cm]{module2_mlp/charts/loss_landscape_3d/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module2_mlp/charts/loss_landscape_3d}{\tiny\texttt{\textcolor{gray}{loss\_landscape\_3d}}}
};
\end{tikzpicture}

\bottomnote{Training = finding the lowest point in this landscape}
\end{frame}

% Slide 53: Finance Application
\begin{frame}[t]{Finance: Choosing Your Loss}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Task-Specific Loss Functions}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Task} & \textbf{Loss} \\
\midrule
Return prediction & MSE \\
Direction prediction & Cross-entropy \\
Volatility forecast & MSE \\
Multi-class sector & Categorical CE \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Beyond Standard Losses:}
\begin{itemize}
\item Sharpe ratio optimization
\item Asymmetric losses (penalize losses more than gains)
\item Custom finance metrics
\end{itemize}

\column{0.48\textwidth}
\textbf{Important Consideration}

\vspace{0.5em}
\textbf{MSE vs Business Metric:}

A model with low MSE may still lose money!

\vspace{0.5em}
\textbf{Example:}
\begin{itemize}
\item Predict returns with 5\% MSE
\item But wrong on big moves
\item Transaction costs eat profits
\item Risk-adjusted return is poor
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Lesson:}}

Statistical accuracy $\neq$ Trading profitability

Module 4 explores this gap.
\end{columns}
\bottomnote{Different problems, different loss functions}
\end{frame}

% ==================== SECTION 8: SUMMARY (Slides 54-55) ====================
\section{Summary and Preview}

% Slide 54: Module 2 Summary
\begin{frame}[t]{Module 2: Key Takeaways}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What We Learned}

\begin{enumerate}
\item \textbf{Historical Context}
\begin{itemize}
\item AI Winter (1969-1982)
\item Backprop renaissance (1986)
\item Right idea + right time
\end{itemize}

\item \textbf{MLP Architecture}
\begin{itemize}
\item Hidden layers find patterns
\item Matrix notation for computation
\item Parameter counting
\end{itemize}

\item \textbf{Activation Functions}
\begin{itemize}
\item Non-linearity is essential
\item ReLU for hidden, task-specific for output
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\begin{enumerate}
\setcounter{enumi}{3}
\item \textbf{Universal Approximation}
\begin{itemize}
\item MLPs can learn any function
\item But existence $\neq$ construction
\end{itemize}

\item \textbf{Loss Functions}
\begin{itemize}
\item MSE for regression
\item Cross-entropy for classification
\item Loss landscape visualization
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textbf{The Big Picture:}

We now have powerful architectures. But how do they \textit{learn}?
\end{columns}
\bottomnote{From single perceptron to universal function approximator}
\end{frame}

% Slide 55: Preview of Module 3
\begin{frame}[t]{Preview: Module 3}
\begin{center}
\Large
\textit{``We have the architecture. But how does it LEARN?''}
\end{center}

\vspace{1em}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Missing Piece}

We know:
\begin{itemize}
\item How to compute forward pass
\item What loss functions measure
\item That good weights exist
\end{itemize}

We don't know:
\begin{itemize}
\item How to find good weights
\item How errors update weights
\item How to avoid overfitting
\end{itemize}

\column{0.48\textwidth}
\textbf{Coming in Module 3:}
\begin{itemize}
\item Gradient descent (intuition)
\item Backpropagation (the magic)
\item Training dynamics
\item Overfitting and regularization
\item Practical training tips
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{The Key:}} Backpropagation -- the algorithm that made deep learning possible.
\end{columns}

\vspace{0.5em}
\textbf{Mathematical details: See Appendix B (Backpropagation Derivation)}
\bottomnote{Next: The magic of backpropagation}
\end{frame}



%% ============================================================================
%% MODULE3 TRAINING
%% ============================================================================



% ==================== SECTION 1: OPENING (Slides 1-5) ====================
\section{Opening}

% Slide 1: Title
\begin{frame}[plain]
\titlepage
\end{frame}

% Slide 2: The Central Question
\begin{frame}[t]{The Central Question}
\begin{center}
\Large
\textit{``We have the architecture. How does it LEARN?''}
\end{center}

\vspace{1em}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What We Know:}
\begin{itemize}
\item MLP architecture (Module 2)
\item Forward pass computation
\item Loss functions measure error
\item Good weights exist (universal approximation)
\end{itemize}

\column{0.48\textwidth}
\textbf{What We Don't Know:}
\begin{itemize}
\item How to find good weights
\item How errors guide updates
\item Why training sometimes fails
\item How to avoid overfitting
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{This module bridges the gap from architecture to learning.}}
\bottomnote{The fundamental challenge of neural network training}
\end{frame}

% Slide 3: The Trading Desk Analogy
\begin{frame}[t]{Finance Parallel: The Trading Desk}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{How Traders Improve}

A trader's learning process:
\begin{enumerate}
\item Make a trade (forward pass)
\item Wait for P\&L (loss function)
\item Analyze what went wrong (gradient)
\item Adjust strategy (weight update)
\item Repeat thousands of times (epochs)
\end{enumerate}

\vspace{0.5em}
\textbf{Key Insight:}

Mistakes are information. Each error tells you how to adjust.

\column{0.48\textwidth}
\textbf{Neural Network Training}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Trading} & \textbf{Neural Net} \\
\midrule
Trade execution & Forward pass \\
P\&L calculation & Loss function \\
Post-trade analysis & Backpropagation \\
Strategy adjustment & Weight update \\
Experience & Training epochs \\
\bottomrule
\end{tabular}

\vspace{0.5em}
Both learn by \textbf{iteratively correcting mistakes}.
\end{columns}
\bottomnote{How does a trader improve? By analyzing what went wrong.}
\end{frame}

% Slide 4: Module 3 Roadmap
\begin{frame}[t]{Module 3 Roadmap}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Today's Journey}

\begin{enumerate}
\item \textbf{Loss Functions (Review)}
\begin{itemize}
\item Measuring prediction error
\item MSE intuition
\end{itemize}

\item \textbf{Gradient Descent}
\begin{itemize}
\item Finding the minimum
\item Learning rate tuning
\end{itemize}

\item \textbf{Backpropagation}
\begin{itemize}
\item Credit assignment
\item Chain rule in action
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\begin{enumerate}
\setcounter{enumi}{3}
\item \textbf{Training Dynamics}
\begin{itemize}
\item Batch vs. stochastic
\item Epochs and convergence
\end{itemize}

\item \textbf{Overfitting}
\begin{itemize}
\item The enemy of generalization
\item The backtest trap
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textbf{Learning Objectives:}
\begin{itemize}
\item Understand gradient descent intuitively
\item Grasp backpropagation as ``blame assignment''
\item Recognize and prevent overfitting
\end{itemize}
\end{columns}
\bottomnote{From measuring error to updating weights}
\end{frame}

% Slide 5: The Learning Problem
\begin{frame}[t]{The Learning Problem}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Challenge}

\textbf{Given:}
\begin{itemize}
\item Training data: $\{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^m$
\item Network architecture
\item Loss function $\mathcal{L}$
\end{itemize}

\textbf{Find:}
\begin{itemize}
\item Weights $\mathbf{W}$ and biases $\mathbf{b}$
\item That minimize $\mathcal{L}$
\item And generalize to new data
\end{itemize}

\vspace{0.5em}
\textbf{Scale of the Problem:}

A 4-10-5-1 network: 111 parameters

A ResNet-50: 25 million parameters

\column{0.48\textwidth}
\textbf{Why Is This Hard?}

\vspace{0.5em}
\textbf{Dimensionality:}
\begin{itemize}
\item Millions of weights to tune
\item Exponentially many combinations
\item Can't try them all
\end{itemize}

\textbf{Non-Convexity:}
\begin{itemize}
\item Many local minima
\item Saddle points
\item Flat regions
\end{itemize}

\textbf{The Solution:}

Gradient-based optimization

``Move downhill in weight space''
\end{columns}
\bottomnote{Thousands of weights to tune - how do we find the right values?}
\end{frame}

% ==================== SECTION 2: HISTORICAL CONTEXT (Slides 6-10) ====================
\section{Historical Context: 1986-2012}

% Slide 6: 1989 - LeNet
\begin{frame}[t]{1989: LeNet and Practical Success}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Yann LeCun at Bell Labs}

First commercially deployed neural network:
\begin{itemize}
\item Handwritten digit recognition
\item Used by US Postal Service
\item Read millions of checks
\item Proved neural nets could work
\end{itemize}

\vspace{0.5em}
\textbf{Key Innovations:}
\begin{itemize}
\item Convolutional architecture
\item Shared weights
\item Backprop through convolutions
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/timeline_1986_2012/timeline_1986_2012.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/timeline_1986_2012}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/timeline_1986_2012}{\includegraphics[width=0.6cm]{module3_training/charts/timeline_1986_2012/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/timeline_1986_2012}{\tiny\texttt{\textcolor{gray}{timeline\_1986\_2012}}}
};
\end{tikzpicture}

\bottomnote{Yann LeCun: First commercially deployed neural network}
\end{frame}

% Slide 7: 1991 - Vanishing Gradients
\begin{frame}[t]{1991: The Vanishing Gradient Problem}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Discovery}

Sepp Hochreiter (1991) identified why deep networks fail:

\vspace{0.5em}
\textbf{The Problem:}
\begin{itemize}
\item Gradients multiply through layers
\item Sigmoid derivative: max 0.25
\item Through 10 layers: $0.25^{10} \approx 10^{-6}$
\item Early layers learn nothing
\end{itemize}

\vspace{0.5em}
\textbf{Symptoms:}
\begin{itemize}
\item Later layers learn quickly
\item Early layers stuck at random
\item Network never converges
\end{itemize}

\column{0.48\textwidth}
\textbf{Why Sigmoid Causes Problems}

\vspace{0.5em}
For sigmoid: $\sigma'(z) = \sigma(z)(1-\sigma(z))$

Maximum value: $\sigma'(0) = 0.25$

\vspace{0.5em}
\begin{center}
\begin{tabular}{cc}
\toprule
\textbf{Layers} & \textbf{Max Gradient} \\
\midrule
1 & 0.25 \\
5 & $10^{-3}$ \\
10 & $10^{-6}$ \\
20 & $10^{-12}$ \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Implication:}} Deep networks seemed impossible until ReLU (2010).
\end{columns}
\bottomnote{Deep networks couldn't learn - gradients disappeared}
\end{frame}

% Slide 8: 1997 - LSTM
\begin{frame}[t]{1997: LSTM Networks}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Long Short-Term Memory}

Hochreiter \& Schmidhuber solution:
\begin{itemize}
\item Designed for sequences
\item Explicit ``memory'' cells
\item Gating mechanisms
\item Gradients can flow unchanged
\end{itemize}

\vspace{0.5em}
\textbf{Key Innovation:}

The ``constant error carousel'' -- a path where gradients don't decay.

\vspace{0.5em}
\textbf{Applications:}
\begin{itemize}
\item Speech recognition
\item Machine translation
\item Time series prediction
\end{itemize}

\column{0.48\textwidth}
\textbf{Finance Relevance}

\vspace{0.5em}
LSTMs became popular for:
\begin{itemize}
\item Stock price prediction
\item Volatility forecasting
\item Sentiment analysis
\item Algorithmic trading
\end{itemize}

\vspace{0.5em}
\textbf{Why LSTM for Finance?}
\begin{itemize}
\item Financial data is sequential
\item Long-term dependencies matter
\item Regime changes persist
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Note:}} Now largely replaced by Transformers (2017).
\end{columns}
\bottomnote{Hochreiter and Schmidhuber: Long Short-Term Memory}
\end{frame}

% Slide 9: 2012 - ImageNet Moment
\begin{frame}[t]{2012: The ImageNet Moment}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{AlexNet Wins ImageNet}

Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton:
\begin{itemize}
\item 15.3\% error rate
\item Second place: 26.2\%
\item \textbf{40\% relative improvement}
\item Used GPUs for training
\end{itemize}

\vspace{0.5em}
\textbf{What Made It Work:}
\begin{enumerate}
\item ReLU activation (not sigmoid)
\item Dropout regularization
\item GPU training (60x faster)
\item Large dataset (1.2M images)
\item Data augmentation
\end{enumerate}

\column{0.48\textwidth}
\textbf{Why This Was Different}

\vspace{0.5em}
\textbf{Previous Attempts:}
\begin{itemize}
\item Shallow networks
\item Hand-crafted features
\item Small datasets
\item CPU training
\end{itemize}

\textbf{AlexNet:}
\begin{itemize}
\item 8 layers deep
\item Learned features
\item Massive data
\item GPU parallelism
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{The Result:}} Deep learning became the dominant paradigm. Every major AI company pivoted.
\end{columns}
\bottomnote{AlexNet: Deep learning proves its superiority}
\end{frame}

% Slide 10: What Changed?
\begin{frame}[t]{What Changed Between 1990 and 2012?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Ingredients for Success}

\begin{enumerate}
\item \textbf{Big Data}
\begin{itemize}
\item ImageNet: 1.2M labeled images
\item Internet made data collection possible
\item 1990: thousands of samples
\end{itemize}

\item \textbf{Compute Power}
\begin{itemize}
\item GPUs: 100x speedup
\item Moore's law compounding
\item Training in days, not years
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\begin{enumerate}
\setcounter{enumi}{2}
\item \textbf{Algorithmic Improvements}
\begin{itemize}
\item ReLU: no vanishing gradients
\item Dropout: better generalization
\item Batch normalization (2015)
\end{itemize}

\item \textbf{Open Research Culture}
\begin{itemize}
\item arXiv preprints
\item Open-source frameworks
\item Reproducibility
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Key Insight:}} The core ideas from 1986 worked -- they just needed scale and engineering.
\end{columns}
\bottomnote{Big data + GPUs + ReLU + dropout = breakthrough}
\end{frame}

% ==================== SECTION 3: LOSS FUNCTIONS (Slides 11-18) ====================
\section{Loss Functions: Measuring Mistakes}

% Slide 11: What Does "Wrong" Mean?
\begin{frame}[t]{What Does ``Wrong'' Mean?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Quantifying Prediction Error}

We need a function that:
\begin{itemize}
\item Takes predictions and labels
\item Returns a single number
\item Higher = worse predictions
\item Differentiable (for gradients)
\end{itemize}

\vspace{0.5em}
\textbf{The Loss Function:}

$$\mathcal{L}(\hat{y}, y)$$

\vspace{0.5em}
\textbf{Properties We Want:}
\begin{itemize}
\item $\mathcal{L} \geq 0$ (non-negative)
\item $\mathcal{L} = 0$ iff perfect prediction
\item Smooth (for optimization)
\end{itemize}

\column{0.48\textwidth}
\textbf{Different Tasks, Different Losses}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Task} & \textbf{Loss} \\
\midrule
Regression & MSE \\
Binary classification & Cross-entropy \\
Multi-class & Categorical CE \\
Ranking & Hinge loss \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Finance Examples:}
\begin{itemize}
\item Return prediction: MSE
\item Buy/sell: Binary CE
\item Sector classification: Categorical CE
\end{itemize}
\end{columns}
\bottomnote{We need a way to measure how wrong our predictions are}
\end{frame}

% Slide 12: Finance Analogy - P&L
\begin{frame}[t]{Finance Analogy: Profit and Loss}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{P\&L as a Loss Function}

For traders:
\begin{itemize}
\item P\&L = realized gain/loss
\item Negative P\&L = bad trades
\item Goal: maximize P\&L
\end{itemize}

\vspace{0.5em}
\textbf{Connection to ML Loss:}
\begin{itemize}
\item ML loss = prediction error
\item Higher loss = worse model
\item Goal: minimize loss
\end{itemize}

\vspace{0.5em}
\textbf{Key Difference:}

P\&L is a \textit{performance} metric.

ML loss is an \textit{optimization} target.

They may not align perfectly!

\column{0.48\textwidth}
\textbf{When P\&L $\neq$ Loss}

\vspace{0.5em}
A model might have:
\begin{itemize}
\item Low MSE (accurate predictions)
\item But low P\&L (wrong on big moves)
\end{itemize}

Or:
\begin{itemize}
\item High MSE (noisy predictions)
\item But high P\&L (right when it matters)
\end{itemize}

\vspace{0.5em}
\textbf{Implication:}

Consider using custom loss functions that better align with trading goals.

\vspace{0.5em}
\textcolor{mlpurple}{\textit{Module 4 explores this tension.}}
\end{columns}
\bottomnote{P\&L is the loss function of trading}
\end{frame}

% Slide 13: Loss Function Definition
\begin{frame}[t]{Loss Function: Error Measurement}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Total Loss Over Dataset}

For $m$ training examples:

$$\mathcal{L}(\mathbf{W}) = \frac{1}{m} \sum_{i=1}^{m} \ell(\hat{y}^{(i)}, y^{(i)})$$

where:
\begin{itemize}
\item $\ell$: loss per example
\item $\hat{y}^{(i)} = f(\mathbf{x}^{(i)}; \mathbf{W})$: prediction
\item $y^{(i)}$: true label
\item $\mathbf{W}$: all network weights
\end{itemize}

\vspace{0.5em}
\textbf{Goal:}

$$\mathbf{W}^* = \arg\min_{\mathbf{W}} \mathcal{L}(\mathbf{W})$$

\column{0.48\textwidth}
\textbf{Why Average?}

\vspace{0.5em}
\textbf{Sum vs Average:}
\begin{itemize}
\item Sum: scales with dataset size
\item Average: comparable across datasets
\item Gradient magnitude consistent
\end{itemize}

\vspace{0.5em}
\textbf{The Optimization Landscape:}

$\mathcal{L}(\mathbf{W})$ defines a surface over weight space.

\begin{itemize}
\item High regions: bad weights
\item Low regions: good weights
\item We seek the lowest point
\end{itemize}
\end{columns}
\bottomnote{The loss function quantifies prediction error}
\end{frame}

% Slide 14: MSE Intuition
\begin{frame}[t]{Mean Squared Error: Intuition}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Formula}

$$\mathcal{L}_{MSE} = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2$$

\vspace{0.5em}
\textbf{In Words:}
\begin{enumerate}
\item Compute error: $y - \hat{y}$
\item Square it: $(y - \hat{y})^2$
\item Average over all samples
\end{enumerate}

\vspace{0.5em}
\textbf{Why Squaring?}
\begin{itemize}
\item Makes all errors positive
\item Penalizes large errors heavily
\item Mathematically convenient
\end{itemize}

\column{0.48\textwidth}
\textbf{Example}

\vspace{0.5em}
\begin{tabular}{ccc}
\toprule
$y$ & $\hat{y}$ & $(y-\hat{y})^2$ \\
\midrule
5\% & 3\% & 4 \\
-2\% & 1\% & 9 \\
8\% & 7\% & 1 \\
\midrule
\textbf{MSE} & & \textbf{4.67} \\
\bottomrule
\end{tabular}

\vspace{0.5em}
Units: $\text{(percentage points)}^2$

\vspace{0.5em}
\textbf{RMSE:} $\sqrt{MSE} = 2.16\%$

``On average, we're off by about 2\%''
\end{columns}
\bottomnote{``How far off were we, on average?''}
\end{frame}

% Slide 15: MSE Visually
\begin{frame}[t]{MSE: Visual Interpretation}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Squared Errors as Areas}

Each error $(y - \hat{y})^2$ is the area of a square with side length $|y - \hat{y}|$.

\vspace{0.5em}
\textbf{MSE = Average Square Area}

\vspace{0.5em}
\textbf{Why This Matters:}
\begin{itemize}
\item Error of 4 is 16x worse than error of 1
\item Large errors dominate
\item Outliers have huge impact
\end{itemize}

\vspace{0.5em}
\textbf{Alternative: MAE}

Mean Absolute Error:
$$\mathcal{L}_{MAE} = \frac{1}{m} \sum |y - \hat{y}|$$

More robust to outliers.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/mse_visualization/mse_visualization.pdf}
\end{center}
\end{columns}
\bottomnote{Squaring emphasizes large errors}
\end{frame}

% Slide 16: Finance Application
\begin{frame}[t]{Finance Application: Return Prediction}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Worked Example}

\textbf{Predictions for 5 Stocks:}

\begin{tabular}{lccc}
\toprule
\textbf{Stock} & $\hat{y}$ & $y$ & Error$^2$ \\
\midrule
AAPL & +5\% & +2\% & 9 \\
MSFT & +3\% & +4\% & 1 \\
GOOG & -1\% & +2\% & 9 \\
AMZN & +4\% & +4\% & 0 \\
META & +2\% & -3\% & 25 \\
\midrule
\textbf{MSE} & & & \textbf{8.8} \\
\bottomrule
\end{tabular}

\vspace{0.5em}
RMSE = 2.97\%

\column{0.48\textwidth}
\textbf{Interpretation}

\vspace{0.5em}
``On average, our return predictions are off by about 3 percentage points.''

\vspace{0.5em}
\textbf{Is This Good?}

Depends on context:
\begin{itemize}
\item Market daily vol: $\sim$1\%
\item 3\% RMSE = 3 std devs
\item \textcolor{mlred}{Not very predictive}
\end{itemize}

\vspace{0.5em}
\textbf{Reality Check:}

Even small predictability (RMSE slightly $<$ volatility) can be valuable in trading.
\end{columns}
\bottomnote{Worked example with stock returns}
\end{frame}

% Slide 17: Discussion Question 1
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``Why might we want to penalize large errors more than small ones in stock prediction?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Arguments For (Use MSE):}
\begin{itemize}
\item Big errors are costlier
\item Crashes matter more than small moves
\item Position sizing affected
\item Risk management
\end{itemize}

\column{0.48\textwidth}
\textbf{Arguments Against (Use MAE):}
\begin{itemize}
\item Markets have fat tails
\item Outliers can dominate MSE
\item May optimize for rare events
\item Robustness to noise
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Reality:}} Many practitioners use MAE or Huber loss (combines both) for financial applications.
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 18: Loss Landscape Revisited
\begin{frame}[t]{The Loss Landscape}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Loss as a Function of Weights}

$$\mathcal{L}(\mathbf{W})$$

For every choice of weights, there's a loss value.

\vspace{0.5em}
\textbf{In 2D (two weights):}

A surface we can visualize.

\vspace{0.5em}
\textbf{In High Dimensions:}

A hypersurface we navigate blindly.

\vspace{0.5em}
\textbf{Features:}
\begin{itemize}
\item Global minimum (best)
\item Local minima (traps)
\item Saddle points
\item Flat regions (plateaus)
\end{itemize}

\column{0.48\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/loss_landscape_3d/loss_landscape_3d.pdf}
\end{center}
\end{columns}
\bottomnote{Finding the minimum of a high-dimensional function}
\end{frame}

% ==================== SECTION 4: GRADIENT DESCENT (Slides 19-28) ====================
\section{Gradient Descent: Finding the Minimum}

% Slide 19: The Optimization Problem
\begin{frame}[t]{The Optimization Problem}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Challenge}

Find:
$$\mathbf{W}^* = \arg\min_{\mathbf{W}} \mathcal{L}(\mathbf{W})$$

\vspace{0.5em}
\textbf{Difficulties:}
\begin{itemize}
\item Millions of dimensions
\item Non-convex landscape
\item No closed-form solution
\item Can't try all possibilities
\end{itemize}

\vspace{0.5em}
\textbf{We Need:}

An \textit{iterative} algorithm that gradually improves weights.

\column{0.48\textwidth}
\textbf{Possible Approaches}

\vspace{0.5em}
\textbf{Random Search:}
\begin{itemize}
\item Try random weights
\item Keep best so far
\item \textcolor{mlred}{Hopelessly slow}
\end{itemize}

\textbf{Grid Search:}
\begin{itemize}
\item Try all combinations
\item $10^{100}$ possibilities
\item \textcolor{mlred}{Impossible}
\end{itemize}

\textbf{Gradient-Based:}
\begin{itemize}
\item Use local slope information
\item Move toward improvement
\item \textcolor{mlgreen}{Tractable!}
\end{itemize}
\end{columns}
\bottomnote{How do we find the weights that minimize loss?}
\end{frame}

% Slide 20: The Blind Hiker Analogy
\begin{frame}[t]{The Blind Hiker Analogy}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Scenario}

Imagine you're:
\begin{itemize}
\item Blindfolded
\item On a mountainside
\item Trying to reach the valley
\item Can only feel the local slope
\end{itemize}

\vspace{0.5em}
\textbf{What Would You Do?}

\begin{enumerate}
\item Feel the ground around you
\item Determine which way is downhill
\item Take a step in that direction
\item Repeat until you reach a valley
\end{enumerate}

\column{0.48\textwidth}
\textbf{Neural Network Translation}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Hiker} & \textbf{Network} \\
\midrule
Position & Weights $\mathbf{W}$ \\
Altitude & Loss $\mathcal{L}$ \\
Slope & Gradient $\nabla \mathcal{L}$ \\
Step & Weight update \\
Valley & Minimum loss \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Key Insight:}

We don't need to see the whole landscape. Local slope is enough!
\end{columns}
\bottomnote{``You're blindfolded on a mountain. How do you find the valley?''}
\end{frame}

% Slide 21: Answer - Feel the Slope
\begin{frame}[t]{Answer: Feel the Slope}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Strategy}

\begin{enumerate}
\item Compute the slope (gradient)
\item Move opposite to the slope
\item Repeat until convergence
\end{enumerate}

\vspace{0.5em}
\textbf{Why Opposite?}
\begin{itemize}
\item Gradient points uphill
\item We want to go downhill
\item Move in negative gradient direction
\end{itemize}

\vspace{0.5em}
\textbf{The Update Rule:}

$$\mathbf{W} \leftarrow \mathbf{W} - \eta \nabla_{\mathbf{W}} \mathcal{L}$$

\column{0.48\textwidth}
\textbf{Gradient Descent Algorithm}

\vspace{0.5em}
\begin{enumerate}
\item Initialize $\mathbf{W}$ randomly
\item \textbf{repeat}:
\begin{enumerate}
\item[a.] Compute loss $\mathcal{L}(\mathbf{W})$
\item[b.] Compute gradient $\nabla \mathcal{L}$
\item[c.] Update: $\mathbf{W} \leftarrow \mathbf{W} - \eta \nabla \mathcal{L}$
\end{enumerate}
\item \textbf{until} convergence
\end{enumerate}

\vspace{0.5em}
$\eta$ = learning rate (step size)
\end{columns}
\bottomnote{Move in the direction that goes down}
\end{frame}

% Slide 22: Gradient Definition
\begin{frame}[t]{The Gradient: Direction of Steepest Ascent}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What Is the Gradient?}

The gradient $\nabla \mathcal{L}$ is a vector of partial derivatives:

$$\nabla_{\mathbf{W}} \mathcal{L} = \begin{pmatrix} \frac{\partial \mathcal{L}}{\partial w_1} \\ \frac{\partial \mathcal{L}}{\partial w_2} \\ \vdots \\ \frac{\partial \mathcal{L}}{\partial w_n} \end{pmatrix}$$

\vspace{0.5em}
\textbf{Each Component:}

$\frac{\partial \mathcal{L}}{\partial w_i}$ = How much does loss change if we change $w_i$ slightly?

\column{0.48\textwidth}
\textbf{Properties}

\vspace{0.5em}
\textbf{Direction:}
\begin{itemize}
\item Points toward steepest increase
\item $-\nabla \mathcal{L}$ points toward steepest decrease
\end{itemize}

\textbf{Magnitude:}
\begin{itemize}
\item $\|\nabla \mathcal{L}\|$ = slope steepness
\item Near minimum: gradient $\approx 0$
\end{itemize}

\textbf{At a Minimum:}
$$\nabla \mathcal{L} = \mathbf{0}$$

No direction goes further down.
\end{columns}
\bottomnote{The gradient tells us which way is ``up''}
\end{frame}

% Slide 23: Gradient Descent Intuition
\begin{frame}[t]{Gradient Descent: Move Downhill}
\begin{center}
\includegraphics[width=0.52\textwidth]{module3_training/charts/gradient_descent_contour/gradient_descent_contour.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/gradient_descent_contour}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/gradient_descent_contour}{\includegraphics[width=0.6cm]{module3_training/charts/gradient_descent_contour/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/gradient_descent_contour}{\tiny\texttt{\textcolor{gray}{gradient\_descent\_contour}}}
};
\end{tikzpicture}

\bottomnote{Step in the negative gradient direction}
\end{frame}

% Slide 24: Finance Parallel
\begin{frame}[t]{Finance Parallel: Portfolio Optimization}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Portfolio Adjustment}

Similar iterative process:
\begin{enumerate}
\item Evaluate current portfolio
\item Estimate sensitivities (``greeks'')
\item Adjust positions to reduce risk
\item Repeat periodically
\end{enumerate}

\vspace{0.5em}
\textbf{Delta Hedging:}
\begin{itemize}
\item Measure option delta
\item Adjust stock position
\item Move toward neutral
\end{itemize}

\column{0.48\textwidth}
\textbf{Comparison}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{GD} & \textbf{Portfolio} \\
\midrule
Loss & Risk/Variance \\
Weights & Positions \\
Gradient & Sensitivities \\
Learning rate & Trading aggressiveness \\
Convergence & Optimal allocation \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Key Difference:}

Markets change continuously. Portfolios must adapt.

Neural networks train once (mostly).
\end{columns}
\bottomnote{Similar to iterative portfolio rebalancing}
\end{frame}

% Slide 25: The Learning Rate
\begin{frame}[t]{The Learning Rate: Step Size}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Hyperparameter $\eta$}

$$\mathbf{W} \leftarrow \mathbf{W} - \eta \nabla \mathcal{L}$$

\vspace{0.5em}
\textbf{$\eta$ Controls:}
\begin{itemize}
\item Size of each weight update
\item Speed of convergence
\item Stability of training
\end{itemize}

\vspace{0.5em}
\textbf{Typical Values:}
\begin{itemize}
\item $10^{-4}$ to $10^{-1}$
\item Often starts at 0.01 or 0.001
\item May decrease during training
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/learning_rate_effects/learning_rate_effects.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/learning_rate_effects}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/learning_rate_effects}{\includegraphics[width=0.6cm]{module3_training/charts/learning_rate_effects/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/learning_rate_effects}{\tiny\texttt{\textcolor{gray}{learning\_rate\_effects}}}
};
\end{tikzpicture}

\bottomnote{Learning rate controls how far we move each step}
\end{frame}

% Slide 26: Learning Rate Too High
\begin{frame}[t]{Learning Rate Too High}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Problem}

When $\eta$ is too large:
\begin{itemize}
\item Steps overshoot the minimum
\item May jump to worse regions
\item Loss oscillates or explodes
\item Training diverges
\end{itemize}

\vspace{0.5em}
\textbf{Symptoms:}
\begin{itemize}
\item Loss goes up, not down
\item Loss becomes NaN
\item Weights grow very large
\item Erratic training curves
\end{itemize}

\column{0.48\textwidth}
\textbf{Finance Analogy}

\vspace{0.5em}
\textbf{Overtrading:}
\begin{itemize}
\item Adjusting positions too aggressively
\item Chasing every signal
\item Transaction costs accumulate
\item Portfolio becomes unstable
\end{itemize}

\vspace{0.5em}
\textbf{Solution:}

Reduce learning rate until stable.

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Rule of Thumb:}} If loss explodes, halve $\eta$.
\end{columns}
\bottomnote{Too big = overshoot the minimum}
\end{frame}

% Slide 27: Learning Rate Too Low
\begin{frame}[t]{Learning Rate Too Low}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Problem}

When $\eta$ is too small:
\begin{itemize}
\item Steps are tiny
\item Progress is slow
\item May get stuck in flat regions
\item Training takes forever
\end{itemize}

\vspace{0.5em}
\textbf{Symptoms:}
\begin{itemize}
\item Loss decreases very slowly
\item Many epochs with little improvement
\item May stop before reaching minimum
\item Wasted computation
\end{itemize}

\column{0.48\textwidth}
\textbf{Finance Analogy}

\vspace{0.5em}
\textbf{Underreacting:}
\begin{itemize}
\item Ignoring market signals
\item Missing opportunities
\item Portfolio drifts from target
\item Slow adaptation to regime changes
\end{itemize}

\vspace{0.5em}
\textbf{Solution:}

Increase learning rate or use adaptive methods.

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Modern Practice:}} Adaptive optimizers (Adam, RMSprop) adjust $\eta$ automatically.
\end{columns}
\bottomnote{Too small = converge too slowly}
\end{frame}

% Slide 28: Discussion Question 2
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``In trading, what's analogous to learning rate? What happens if you adjust positions too aggressively or too conservatively?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Position Sizing:}
\begin{itemize}
\item How much to trade per signal
\item Kelly criterion vs. fractional Kelly
\item Risk management constraints
\end{itemize}

\column{0.48\textwidth}
\textbf{Rebalancing Frequency:}
\begin{itemize}
\item How often to adjust
\item Transaction cost vs. tracking error
\item Market impact considerations
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Key Insight:}} Both trading and ML require balancing responsiveness against stability.
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% ==================== SECTION 5: BACKPROPAGATION (Slides 29-38) ====================
\section{Backpropagation: Credit Assignment}

% Slide 29: The Attribution Problem
\begin{frame}[t]{The Attribution Problem}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Challenge}

We know:
\begin{itemize}
\item The output was wrong
\item We need to update weights
\item There are thousands of weights
\end{itemize}

\textbf{The Question:}

\begin{center}
\textit{Which weights caused the error?}
\end{center}

\vspace{0.5em}
\textbf{Credit Assignment:}

Attributing output error to individual weights deep in the network.

\column{0.48\textwidth}
\textbf{Why Is This Hard?}

\vspace{0.5em}
\textbf{Direct Attribution:}
\begin{itemize}
\item Output layer weights: clear influence
\item Hidden layer weights: indirect
\item Early layers: very indirect
\end{itemize}

\textbf{The Chain of Influence:}

$w_1 \rightarrow h_1 \rightarrow h_2 \rightarrow \cdots \rightarrow \hat{y} \rightarrow \mathcal{L}$

Each weight affects the loss through many intermediate steps.
\end{columns}
\bottomnote{The output was wrong. Which weights caused it?}
\end{frame}

% Slide 30: Finance Analogy
\begin{frame}[t]{Finance Analogy: Post-Trade Analysis}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Attribution in Trading}

A portfolio lost money. Why?

\begin{enumerate}
\item Macro call wrong?
\item Sector allocation off?
\item Stock selection bad?
\item Timing poor?
\item Execution costly?
\end{enumerate}

\vspace{0.5em}
\textbf{Performance Attribution:}
\begin{itemize}
\item Decompose returns by factor
\item Trace P\&L to decisions
\item Learn which calls were wrong
\end{itemize}

\column{0.48\textwidth}
\textbf{Neural Network Attribution}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Trading} & \textbf{Neural Net} \\
\midrule
Macro view & Early layers \\
Sector allocation & Hidden layers \\
Stock picks & Later layers \\
Final trades & Output \\
P\&L & Loss \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Backpropagation} is the neural network's performance attribution algorithm.
\end{columns}
\bottomnote{``Which decisions led to this P\&L?''}
\end{frame}

% Slide 31: Backpropagation Definition
\begin{frame}[t]{Backpropagation: Blame Assignment}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Algorithm}

Backpropagation computes $\frac{\partial \mathcal{L}}{\partial w}$ for every weight $w$ in the network.

\vspace{0.5em}
\textbf{Key Idea:}

Work backward from output to input, propagating error attribution.

\vspace{0.5em}
\textbf{Two Passes:}
\begin{enumerate}
\item \textbf{Forward Pass:} Compute outputs
\item \textbf{Backward Pass:} Compute gradients
\end{enumerate}

\vspace{0.5em}
\textbf{Efficiency:}

Computes ALL gradients in time proportional to one forward pass.

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/backprop_computational_graph/backprop_computational_graph.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/backprop_computational_graph}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/backprop_computational_graph}{\includegraphics[width=0.6cm]{module3_training/charts/backprop_computational_graph/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/backprop_computational_graph}{\tiny\texttt{\textcolor{gray}{backprop\_computational\_graph}}}
};
\end{tikzpicture}

\bottomnote{Propagating error backward through the network}
\end{frame}

% Slide 32: The Chain Rule Intuition
\begin{frame}[t]{The Chain Rule: Intuition}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Core Mathematical Tool}

If $A$ affects $B$ and $B$ affects $C$:

$$\frac{\partial C}{\partial A} = \frac{\partial C}{\partial B} \cdot \frac{\partial B}{\partial A}$$

\vspace{0.5em}
\textbf{Example:}

Temperature $\rightarrow$ Ice cream sales $\rightarrow$ Profit

\vspace{0.5em}
How does temperature affect profit?

$$\frac{\partial \text{Profit}}{\partial \text{Temp}} = \frac{\partial \text{Profit}}{\partial \text{Sales}} \cdot \frac{\partial \text{Sales}}{\partial \text{Temp}}$$

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/chain_rule_visualization/chain_rule_visualization.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/chain_rule_visualization}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/chain_rule_visualization}{\includegraphics[width=0.6cm]{module3_training/charts/chain_rule_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/chain_rule_visualization}{\tiny\texttt{\textcolor{gray}{chain\_rule\_visualization}}}
};
\end{tikzpicture}

\bottomnote{``If A affects B and B affects C, how does A affect C?''}
\end{frame}

% Slide 33: Finance Chain Example
\begin{frame}[t]{Finance Chain Example}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Chain of Effects}

\begin{center}
Fed Rate $\rightarrow$ Mortgages $\rightarrow$ Housing $\rightarrow$ Banks $\rightarrow$ Portfolio
\end{center}

\vspace{0.5em}
\textbf{How does Fed rate affect your portfolio?}

$$\frac{\partial \text{Portfolio}}{\partial \text{Fed}} = \frac{\partial P}{\partial B} \cdot \frac{\partial B}{\partial H} \cdot \frac{\partial H}{\partial M} \cdot \frac{\partial M}{\partial F}$$

\vspace{0.5em}
\textbf{Each Link:}
\begin{itemize}
\item Fed $\rightarrow$ Mortgages: rate sensitivity
\item Mortgages $\rightarrow$ Housing: demand elasticity
\item Housing $\rightarrow$ Banks: credit exposure
\item Banks $\rightarrow$ Portfolio: position size
\end{itemize}

\column{0.48\textwidth}
\textbf{Neural Network Parallel}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Finance} & \textbf{Neural Net} \\
\midrule
Fed rate & Input $x$ \\
Mortgages & Hidden layer 1 \\
Housing & Hidden layer 2 \\
Banks & Hidden layer 3 \\
Portfolio & Output \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Backprop does this automatically:}

Chains together all the local sensitivities to get the total effect of each input/weight on the loss.
\end{columns}
\bottomnote{Effects propagate through chains of influence}
\end{frame}

% Slide 34: Output Layer Gradients
\begin{frame}[t]{Backprop: Output Layer}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{At the Output}

For output weight $w^{(L)}$:

$$\frac{\partial \mathcal{L}}{\partial w^{(L)}} = \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(L)}} \cdot \frac{\partial z^{(L)}}{\partial w^{(L)}}$$

\vspace{0.5em}
\textbf{Each Term:}
\begin{itemize}
\item $\frac{\partial \mathcal{L}}{\partial \hat{y}}$: How loss changes with output
\item $\frac{\partial \hat{y}}{\partial z^{(L)}}$: Activation derivative
\item $\frac{\partial z^{(L)}}{\partial w^{(L)}}$: Input from previous layer
\end{itemize}

\vspace{0.5em}
\textbf{For MSE + Sigmoid:}

$$\frac{\partial \mathcal{L}}{\partial w^{(L)}} = (\hat{y} - y) \cdot \hat{y}(1-\hat{y}) \cdot a^{(L-1)}$$

\column{0.48\textwidth}
\textbf{Output Error ($\delta^{(L)}$)}

\vspace{0.5em}
Define the ``error signal'':

$$\delta^{(L)} = \frac{\partial \mathcal{L}}{\partial z^{(L)}}$$

\vspace{0.5em}
For MSE loss + sigmoid:

$$\delta^{(L)} = (\hat{y} - y) \cdot \sigma'(z^{(L)})$$

\vspace{0.5em}
\textbf{Then:}

$$\frac{\partial \mathcal{L}}{\partial w^{(L)}} = \delta^{(L)} \cdot a^{(L-1)}$$

This is just error $\times$ input!
\end{columns}
\bottomnote{At the output, error attribution is straightforward}
\end{frame}

% Slide 35: Hidden Layer Gradients
\begin{frame}[t]{Backprop: Hidden Layers}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Key Insight}

Hidden layer error comes from downstream:

$$\delta^{(l)} = ((W^{(l+1)})^T \delta^{(l+1)}) \odot \sigma'(z^{(l)})$$

\vspace{0.5em}
\textbf{In Words:}
\begin{enumerate}
\item Take error from next layer ($\delta^{(l+1)}$)
\item Multiply by weights connecting to next layer
\item Scale by local activation derivative
\end{enumerate}

\vspace{0.5em}
\textbf{Error Flows Backward:}

Output $\rightarrow$ Last hidden $\rightarrow$ ... $\rightarrow$ First hidden

\column{0.48\textwidth}
\textbf{Why This Works}

\vspace{0.5em}
Chain rule connects layers:

$$\frac{\partial \mathcal{L}}{\partial z^{(l)}} = \sum_j \frac{\partial \mathcal{L}}{\partial z^{(l+1)}_j} \cdot \frac{\partial z^{(l+1)}_j}{\partial z^{(l)}}$$

\vspace{0.5em}
\textbf{Gradient for Hidden Weight:}

$$\frac{\partial \mathcal{L}}{\partial w^{(l)}} = \delta^{(l)} \cdot a^{(l-1)}$$

Same formula as output layer!
\end{columns}
\bottomnote{Hidden layer gradients require the chain rule}
\end{frame}

% Slide 36: The Complete Picture
\begin{frame}[t]{The Complete Training Loop}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{One Training Step}

\begin{enumerate}
\item \textbf{Forward Pass}
\begin{itemize}
\item Compute all activations
\item Get prediction $\hat{y}$
\end{itemize}

\item \textbf{Compute Loss}
\begin{itemize}
\item $\mathcal{L} = \ell(\hat{y}, y)$
\end{itemize}

\item \textbf{Backward Pass}
\begin{itemize}
\item Compute $\delta^{(L)}$ at output
\item Propagate backward to get all $\delta^{(l)}$
\item Compute all weight gradients
\end{itemize}

\item \textbf{Update Weights}
\begin{itemize}
\item $\mathbf{W} \leftarrow \mathbf{W} - \eta \nabla \mathcal{L}$
\end{itemize}
\end{enumerate}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/gradient_flow_mlp/gradient_flow_mlp.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/gradient_flow_mlp}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/gradient_flow_mlp}{\includegraphics[width=0.6cm]{module3_training/charts/gradient_flow_mlp/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/gradient_flow_mlp}{\tiny\texttt{\textcolor{gray}{gradient\_flow\_mlp}}}
};
\end{tikzpicture}

\bottomnote{Forward pass, compute loss, backward pass, update weights}
\end{frame}

% Slide 37: Why "Backpropagation"?
\begin{frame}[t]{Why ``Backpropagation''?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Name}

``Back-propagation of errors''

\vspace{0.5em}
\textbf{Information Flow:}

\textbf{Forward:}
\begin{itemize}
\item Data flows input $\rightarrow$ output
\item Activations computed layer by layer
\end{itemize}

\textbf{Backward:}
\begin{itemize}
\item Errors flow output $\rightarrow$ input
\item Gradients computed layer by layer
\end{itemize}

\vspace{0.5em}
\textbf{Symmetry:}

Each layer: one forward operation, one backward operation.

\column{0.48\textwidth}
\textbf{Historical Note}

\vspace{0.5em}
\textbf{The Algorithm:}
\begin{itemize}
\item Werbos (1974): first derivation
\item Rumelhart et al. (1986): popularized
\item Now standard in all deep learning
\end{itemize}

\textbf{Modern Perspective:}

Backprop is just automatic differentiation applied to neural networks.

\vspace{0.5em}
\textbf{Frameworks (PyTorch, TensorFlow):}

Compute gradients automatically -- you just specify the forward pass!
\end{columns}
\bottomnote{Error information flows from output to input}
\end{frame}

% Slide 38: Discussion Question 3
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``Why do deeper networks make training harder? What happens to gradients as they flow backward through many layers?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Vanishing Gradients:}
\begin{itemize}
\item Sigmoid: max derivative 0.25
\item Through 10 layers: $0.25^{10}$
\item Early layers get tiny gradients
\item Learn extremely slowly
\end{itemize}

\column{0.48\textwidth}
\textbf{Exploding Gradients:}
\begin{itemize}
\item If derivatives $>$ 1
\item Gradients grow exponentially
\item Weights become huge
\item Training diverges
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Solutions:}} ReLU, batch normalization, residual connections, careful initialization.
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% ==================== SECTION 6: TRAINING DYNAMICS (Slides 39-46) ====================
\section{Training Dynamics}

% Slide 39: Batch Gradient Descent
\begin{frame}[t]{Batch Gradient Descent}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Definition}

Use \textbf{all} training data to compute gradient:

$$\nabla \mathcal{L} = \frac{1}{m} \sum_{i=1}^{m} \nabla \ell(\hat{y}^{(i)}, y^{(i)})$$

Then update weights once.

\vspace{0.5em}
\textbf{Advantages:}
\begin{itemize}
\item[\textcolor{mlgreen}{+}] Stable gradient estimate
\item[\textcolor{mlgreen}{+}] Deterministic updates
\item[\textcolor{mlgreen}{+}] Guaranteed descent direction
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
\item[\textcolor{mlred}{-}] Slow for large datasets
\item[\textcolor{mlred}{-}] Must load all data in memory
\item[\textcolor{mlred}{-}] One update per full pass
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/batch_vs_stochastic/batch_vs_stochastic.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/batch_vs_stochastic}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/batch_vs_stochastic}{\includegraphics[width=0.6cm]{module3_training/charts/batch_vs_stochastic/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/batch_vs_stochastic}{\tiny\texttt{\textcolor{gray}{batch\_vs\_stochastic}}}
};
\end{tikzpicture}

\bottomnote{Compute gradient using the entire dataset}
\end{frame}

% Slide 40: Stochastic Gradient Descent
\begin{frame}[t]{Stochastic Gradient Descent (SGD)}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

Update after \textbf{each} single example:

$$\nabla \mathcal{L} \approx \nabla \ell(\hat{y}^{(i)}, y^{(i)})$$

One sample = one update.

\vspace{0.5em}
\textbf{Advantages:}
\begin{itemize}
\item[\textcolor{mlgreen}{+}] Very fast updates
\item[\textcolor{mlgreen}{+}] Can handle huge datasets
\item[\textcolor{mlgreen}{+}] Noise helps escape local minima
\item[\textcolor{mlgreen}{+}] Online learning possible
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
\item[\textcolor{mlred}{-}] Noisy gradient estimate
\item[\textcolor{mlred}{-}] Erratic convergence
\item[\textcolor{mlred}{-}] May not settle at minimum
\end{itemize}

\column{0.48\textwidth}
\textbf{Why ``Stochastic''?}

\vspace{0.5em}
Random sampling of training examples introduces randomness into gradient.

\vspace{0.5em}
\textbf{Expected Value:}

$$\mathbb{E}[\nabla \ell^{(i)}] = \nabla \mathcal{L}$$

On average, SGD points in the right direction.

\vspace{0.5em}
\textbf{Variance:}

Individual updates are noisy, but noise can help exploration.
\end{columns}
\bottomnote{Update after each single example}
\end{frame}

% Slide 41: Mini-Batch SGD
\begin{frame}[t]{Mini-Batch: The Sweet Spot}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

Use small batches of $B$ examples:

$$\nabla \mathcal{L} \approx \frac{1}{B} \sum_{i=1}^{B} \nabla \ell(\hat{y}^{(i)}, y^{(i)})$$

Typical $B$: 32, 64, 128, 256

\vspace{0.5em}
\textbf{Advantages:}
\begin{itemize}
\item[\textcolor{mlgreen}{+}] Reduced variance vs SGD
\item[\textcolor{mlgreen}{+}] GPU parallelization
\item[\textcolor{mlgreen}{+}] Reasonable memory usage
\item[\textcolor{mlgreen}{+}] Frequent updates
\end{itemize}

\textbf{The Modern Default}

\column{0.48\textwidth}
\textbf{Batch Size Trade-offs}

\vspace{0.5em}
\begin{tabular}{lll}
\toprule
\textbf{Size} & \textbf{Noise} & \textbf{Speed} \\
\midrule
1 (SGD) & High & Fast updates \\
32-256 & Medium & Best practice \\
Full batch & Low & Slow updates \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Large Batch Issues:}
\begin{itemize}
\item May converge to sharp minima
\item Worse generalization
\item Need learning rate scaling
\end{itemize}
\end{columns}
\bottomnote{Balance between efficiency and noise}
\end{frame}

% Slide 42: Epochs
\begin{frame}[t]{Epochs: Full Passes Through Data}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

\textbf{Epoch} = one complete pass through all training data.

\vspace{0.5em}
\textbf{With Mini-Batches:}
\begin{itemize}
\item 10,000 samples
\item Batch size 100
\item 100 updates per epoch
\end{itemize}

\vspace{0.5em}
\textbf{Typical Training:}
\begin{itemize}
\item 10-1000 epochs
\item Monitor loss curve
\item Stop when converged
\end{itemize}

\column{0.48\textwidth}
\textbf{Training Timeline}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Stage} & \textbf{Behavior} \\
\midrule
Early epochs & Loss drops quickly \\
Middle epochs & Progress slows \\
Late epochs & Diminishing returns \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{When to Stop?}
\begin{itemize}
\item Loss stops improving
\item Validation loss increases (overfitting!)
\item Resource constraints
\end{itemize}
\end{columns}
\bottomnote{Training typically requires multiple epochs}
\end{frame}

% Slide 43: Training Curves
\begin{frame}[t]{Training Curves}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{What to Plot}

\begin{itemize}
\item Training loss vs. epoch
\item Validation loss vs. epoch
\item Learning rate schedule
\item Gradient norms (debugging)
\end{itemize}

\vspace{0.5em}
\textbf{Healthy Training:}
\begin{itemize}
\item Both losses decrease
\item Validation tracks training
\item Smooth convergence
\end{itemize}

\textbf{Warning Signs:}
\begin{itemize}
\item Training drops, validation rises
\item Loss oscillates wildly
\item Loss becomes NaN
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/overfitting_curves/overfitting_curves.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/overfitting_curves}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/overfitting_curves}{\includegraphics[width=0.6cm]{module3_training/charts/overfitting_curves/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/overfitting_curves}{\tiny\texttt{\textcolor{gray}{overfitting\_curves}}}
};
\end{tikzpicture}

\bottomnote{Monitoring progress during training}
\end{frame}

% Slide 44: Worked Example
\begin{frame}[t]{Worked Example: One Training Step}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Simple 2-2-1 Network}

\textbf{Given:}
\begin{itemize}
\item Input: $\mathbf{x} = (0.5, 0.8)^T$
\item Target: $y = 1$
\item Current weights (simplified)
\end{itemize}

\textbf{Forward Pass:}
\begin{align*}
z^{(1)} &= W^{(1)}\mathbf{x} + b^{(1)} \\
a^{(1)} &= \sigma(z^{(1)}) \\
z^{(2)} &= W^{(2)}a^{(1)} + b^{(2)} \\
\hat{y} &= \sigma(z^{(2)}) = 0.62
\end{align*}

\column{0.48\textwidth}
\textbf{Loss and Backward}

\textbf{Loss:}
$$\mathcal{L} = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(1 - 0.62)^2 = 0.072$$

\textbf{Backward Pass:}
\begin{align*}
\delta^{(2)} &= (0.62 - 1) \cdot 0.62(1-0.62) \\
&= -0.089
\end{align*}

\textbf{Weight Gradient:}
$$\frac{\partial \mathcal{L}}{\partial W^{(2)}} = \delta^{(2)} \cdot a^{(1)}$$

\textbf{Update:}
$$W^{(2)} \leftarrow W^{(2)} - 0.1 \cdot \nabla W^{(2)}$$
\end{columns}
\bottomnote{Following the numbers through one training step}
\end{frame}

% Slide 45: Vanishing Gradients
\begin{frame}[t]{The Vanishing Gradient Problem}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Problem}

Gradients shrink as they flow backward:

$$\delta^{(l)} \propto \prod_{k=l}^{L-1} \sigma'(z^{(k)})$$

\vspace{0.5em}
For sigmoid: $\sigma'(z) \leq 0.25$

Through 10 layers: gradient $\times 10^{-6}$

\vspace{0.5em}
\textbf{Symptoms:}
\begin{itemize}
\item Early layers don't learn
\item Deep networks fail to train
\item Loss plateaus quickly
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module3_training/charts/vanishing_gradient_demo/vanishing_gradient_demo.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/vanishing_gradient_demo}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/vanishing_gradient_demo}{\includegraphics[width=0.6cm]{module3_training/charts/vanishing_gradient_demo/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/vanishing_gradient_demo}{\tiny\texttt{\textcolor{gray}{vanishing\_gradient\_demo}}}
};
\end{tikzpicture}

\bottomnote{Deep networks: gradients can become vanishingly small}
\end{frame}

% Slide 46: Appendix Reference
\begin{frame}[t]{Full Mathematical Derivation}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{This Module: Intuition}

We covered:
\begin{itemize}
\item Why backprop works (chain rule)
\item How errors flow backward
\item Update rule intuition
\item Training dynamics
\end{itemize}

\vspace{0.5em}
\textbf{What We Skipped:}
\begin{itemize}
\item Full mathematical derivation
\item Matrix calculus details
\item Vectorized implementations
\item Automatic differentiation theory
\end{itemize}

\column{0.48\textwidth}
\textbf{Appendix B Contains:}

\vspace{0.5em}
\begin{enumerate}
\item Chain rule setup
\item Output layer error derivation
\item Hidden layer recursion formula
\item Complete gradient equations
\item Weight and bias gradients
\item Algorithm pseudocode
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{For the mathematically curious:}}

The appendix provides the rigorous derivation with all matrix calculus steps.
\end{columns}
\bottomnote{See Appendix B for complete backpropagation derivation}
\end{frame}

% ==================== SECTION 7: OVERFITTING (Slides 47-54) ====================
\section{Overfitting: The Enemy of Generalization}

% Slide 47: What Is Overfitting?
\begin{frame}[t]{What Is Overfitting?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

\textbf{Overfitting:} When a model learns the training data too well, including its noise, and fails to generalize.

\vspace{0.5em}
\textbf{Analogy:}

A student who memorizes exam answers but doesn't understand the material.

\vspace{0.5em}
\textbf{Symptoms:}
\begin{itemize}
\item Training loss: very low
\item Test loss: high
\item Model is ``too confident''
\end{itemize}

\column{0.48\textwidth}
\textbf{Why It Happens}

\vspace{0.5em}
\textbf{Model Complexity:}
\begin{itemize}
\item Too many parameters
\item Can fit any training data perfectly
\item Including noise
\end{itemize}

\textbf{Limited Data:}
\begin{itemize}
\item Not enough examples
\item Training set not representative
\item Noise gets learned as signal
\end{itemize}

\textbf{Training Too Long:}
\begin{itemize}
\item Model eventually memorizes
\item Needs early stopping
\end{itemize}
\end{columns}
\bottomnote{When your model memorizes instead of learns}
\end{frame}

% Slide 48: Training vs Validation
\begin{frame}[t]{Training vs Validation Loss}
\begin{center}
\includegraphics[width=0.76\textwidth]{module3_training/charts/overfitting_curves/overfitting_curves.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/overfitting_curves}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/overfitting_curves}{\includegraphics[width=0.6cm]{module3_training/charts/overfitting_curves/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/overfitting_curves}{\tiny\texttt{\textcolor{gray}{overfitting\_curves}}}
};
\end{tikzpicture}

\bottomnote{Training loss decreases but validation increases}
\end{frame}

% Slide 49: The Backtest Trap
\begin{frame}[t]{Finance: The Backtest Trap}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Trap}

Every trading strategy looks good on historical data -- that's how you found it!

\vspace{0.5em}
\textbf{The Process:}
\begin{enumerate}
\item Try many strategies
\item Keep the one that worked best
\item By construction, it fits the past
\item Future performance? Unknown.
\end{enumerate}

\vspace{0.5em}
\textbf{Multiple Testing:}
\begin{itemize}
\item Try 1000 random strategies
\item Best one has Sharpe 2.0
\item Is it skill or luck?
\end{itemize}

\column{0.48\textwidth}
\textbf{Why Finance Overfits Easily}

\vspace{0.5em}
\begin{enumerate}
\item \textbf{Limited Data}
\begin{itemize}
\item 20 years = 5000 trading days
\item Few independent observations
\end{itemize}
\item \textbf{Low Signal-to-Noise}
\begin{itemize}
\item Markets are noisy
\item Easy to fit noise
\end{itemize}
\item \textbf{Non-Stationarity}
\begin{itemize}
\item Regimes change
\item Past may not predict future
\end{itemize}
\item \textbf{Look-Ahead Bias}
\begin{itemize}
\item Using future information
\item Subtle but deadly
\end{itemize}
\end{enumerate}
\end{columns}
\bottomnote{``Every strategy looks good on historical data''}
\end{frame}

% Slide 50: Why Finance Overfits Easily
\begin{frame}[t]{Why Finance Overfits So Easily}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Data Limitations}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Domain} & \textbf{Samples} \\
\midrule
ImageNet & 1,200,000 \\
MNIST & 60,000 \\
Stock returns (daily, 10y) & 2,520 \\
Stock returns (monthly, 50y) & 600 \\
Market crashes & $\sim$10 \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{The Problem:}

Neural networks have thousands of parameters but only thousands of data points.

\column{0.48\textwidth}
\textbf{Signal vs Noise}

\vspace{0.5em}
\textbf{Image Classification:}
\begin{itemize}
\item A cat is always a cat
\item Signal is strong and consistent
\item R$^2$ can reach 99\%+
\end{itemize}

\textbf{Stock Prediction:}
\begin{itemize}
\item Returns are mostly random
\item Signal is weak and changing
\item R$^2$ of 1\% is excellent!
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Implication:}}

Standard ML practices don't directly transfer to finance.
\end{columns}
\bottomnote{Limited data, high noise, non-stationary markets}
\end{frame}

% Slide 51: Detecting Overfitting
\begin{frame}[t]{Detecting Overfitting}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Train/Validation/Test Split}

\begin{enumerate}
\item \textbf{Training Set} (60-80\%)
\begin{itemize}
\item Used to fit weights
\end{itemize}
\item \textbf{Validation Set} (10-20\%)
\begin{itemize}
\item Used to tune hyperparameters
\item Monitor for overfitting
\end{itemize}
\item \textbf{Test Set} (10-20\%)
\begin{itemize}
\item Final evaluation only
\item Touch only once!
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textbf{Key Rule:}

Never use test data for decisions.

\column{0.48\textwidth}
\textbf{Warning Signs}

\vspace{0.5em}
\textbf{Overfitting Indicators:}
\begin{itemize}
\item Training loss $\ll$ validation loss
\item Validation loss starts increasing
\item Model predictions are ``too confident''
\item Performance degrades out-of-sample
\end{itemize}

\vspace{0.5em}
\textbf{For Finance:}
\begin{itemize}
\item Backtest Sharpe $\gg$ live Sharpe
\item Strategy ``stops working''
\item Drawdowns worse than expected
\end{itemize}
\end{columns}
\bottomnote{Always monitor out-of-sample performance}
\end{frame}

% Slide 52: Discussion Question 4
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``How would you know if your stock prediction model is overfitting? What specific symptoms would you look for?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{In Training:}
\begin{itemize}
\item Training/validation gap
\item Validation loss trend
\item Prediction confidence
\end{itemize}

\column{0.48\textwidth}
\textbf{In Production:}
\begin{itemize}
\item Live vs. backtest performance
\item Regime sensitivity
\item Transaction cost impact
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Best Practice:}} Always maintain a truly out-of-sample test set that you evaluate only once.
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 53: Preview: Regularization
\begin{frame}[t]{Preview: Fighting Overfitting}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Solutions (Module 4)}

\begin{enumerate}
\item \textbf{L1/L2 Regularization}
\begin{itemize}
\item Penalize large weights
\item Simpler models
\end{itemize}
\item \textbf{Dropout}
\begin{itemize}
\item Randomly disable neurons
\item Ensemble effect
\end{itemize}
\item \textbf{Early Stopping}
\begin{itemize}
\item Stop before overfitting
\item Use validation loss
\end{itemize}
\item \textbf{Data Augmentation}
\begin{itemize}
\item Create more training data
\item Finance: bootstrap?
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{Finance-Specific}

\vspace{0.5em}
\begin{enumerate}
\item \textbf{Walk-Forward Validation}
\begin{itemize}
\item Respect time ordering
\item Rolling windows
\end{itemize}
\item \textbf{Cross-Validation Variants}
\begin{itemize}
\item Purged CV
\item Combinatorial CV
\end{itemize}
\item \textbf{Ensemble Methods}
\begin{itemize}
\item Average multiple models
\item Reduce variance
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textit{Module 4 will cover these in detail.}}
\end{columns}
\bottomnote{Module 4 will cover solutions: regularization, dropout, early stopping}
\end{frame}

% Slide 54: Module 3 Summary Diagram
\begin{frame}[t]{Training Pipeline Overview}
\begin{center}
\includegraphics[width=0.62\textwidth]{module3_training/charts/module3_summary_diagram/module3_summary_diagram.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/module3_summary_diagram}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/module3_summary_diagram}{\includegraphics[width=0.6cm]{module3_training/charts/module3_summary_diagram/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module3_training/charts/module3_summary_diagram}{\tiny\texttt{\textcolor{gray}{module3\_summary\_diagram}}}
};
\end{tikzpicture}

\bottomnote{The complete neural network training process}
\end{frame}

% ==================== SECTION 8: SUMMARY (Slides 55-58) ====================
\section{Summary and Preview}

% Slide 55: Module 3 Key Takeaways
\begin{frame}[t]{Module 3: Key Takeaways}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What We Learned}

\begin{enumerate}
\item \textbf{Loss Functions}
\begin{itemize}
\item Measure prediction error
\item MSE, cross-entropy
\item Define what ``good'' means
\end{itemize}
\item \textbf{Gradient Descent}
\begin{itemize}
\item Follow the slope downhill
\item Learning rate matters
\item Batch vs stochastic
\end{itemize}
\item \textbf{Backpropagation}
\begin{itemize}
\item Chain rule for credit assignment
\item Error flows backward
\item Enables efficient gradient computation
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\begin{enumerate}
\setcounter{enumi}{3}
\item \textbf{Training Dynamics}
\begin{itemize}
\item Epochs and batches
\item Monitoring with curves
\item Vanishing gradients
\end{itemize}
\item \textbf{Overfitting}
\begin{itemize}
\item Memorizing vs learning
\item Train/val/test split
\item Finance-specific challenges
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textbf{The Big Picture:}

We can now train neural networks. But making them work well requires more...
\end{columns}
\bottomnote{From measuring error to updating weights}
\end{frame}

% Slide 56: What We've Built
\begin{frame}[t]{What We've Built So Far}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Modules 1-3 Foundation}

\begin{enumerate}
\item \textbf{Module 1: Architecture}
\begin{itemize}
\item Perceptron basics
\item Linear decision boundaries
\item Limitations (XOR)
\end{itemize}
\item \textbf{Module 2: MLPs}
\begin{itemize}
\item Hidden layers
\item Non-linear activation
\item Universal approximation
\end{itemize}
\item \textbf{Module 3: Training}
\begin{itemize}
\item Gradient descent
\item Backpropagation
\item Overfitting awareness
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{You Can Now:}

\vspace{0.5em}
\begin{itemize}
\item Explain how neural networks compute
\item Understand the training process
\item Recognize overfitting
\item Follow the math (or know where to look)
\end{itemize}

\vspace{0.5em}
\textbf{What's Missing:}

\begin{itemize}
\item Practical regularization
\item Real-world applications
\item Finance case studies
\item Modern developments
\end{itemize}
\end{columns}
\bottomnote{Modules 1-3: The complete neural network foundation}
\end{frame}

% Slide 57: Discussion Questions Review
\begin{frame}[t]{Key Questions for Reflection}
\textbf{Think about these as you move to Module 4:}

\vspace{0.5em}
\begin{enumerate}
\item \textbf{Loss vs. Profit:}

Why might minimizing MSE not maximize trading profit? What loss function would better align with trading goals?

\vspace{0.3em}
\item \textbf{Overfitting in Finance:}

With only 20 years of daily data, how many parameters can we safely learn? What's the ratio of samples to parameters you'd be comfortable with?

\vspace{0.3em}
\item \textbf{Non-Stationarity:}

If market regimes change, what does that mean for our training strategy? Should we weight recent data more heavily?

\vspace{0.3em}
\item \textbf{The Efficient Market Hypothesis:}

If markets are efficient, can neural networks find persistent patterns? What would success look like?
\end{enumerate}
\bottomnote{Reflect on the learning process}
\end{frame}

% Slide 58: Preview of Module 4
\begin{frame}[t]{Preview: Module 4}
\begin{center}
\Large
\textit{``Theory meets practice. How do we actually use neural networks in finance?''}
\end{center}

\vspace{1em}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Coming Up:}
\begin{itemize}
\item Regularization techniques
\begin{itemize}
\item L1/L2, dropout, early stopping
\end{itemize}
\item Financial data challenges
\begin{itemize}
\item Non-stationarity, noise
\end{itemize}
\item Complete case study
\begin{itemize}
\item Stock prediction end-to-end
\end{itemize}
\end{itemize}

\column{0.48\textwidth}
\textbf{Also:}
\begin{itemize}
\item Modern architectures overview
\begin{itemize}
\item CNN, RNN, Transformers
\end{itemize}
\item Limitations and ethics
\begin{itemize}
\item Black-box decisions
\item Regulatory concerns
\end{itemize}
\item Future directions
\begin{itemize}
\item Where the field is heading
\end{itemize}
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Mathematical details: See Appendix B-D for derivations}
\bottomnote{Next: Regularization, case studies, and modern developments}
\end{frame}



%% ============================================================================
%% MODULE4 APPLICATIONS
%% ============================================================================



% ==================== SECTION 1: OPENING (Slides 1-5) ====================
\section{Opening}

% Slide 1: Title
\begin{frame}[plain]
\titlepage
\end{frame}

% Slide 2: The Journey So Far
\begin{frame}[t]{The Journey So Far}
\begin{columns}[T]
\begin{column}{0.6\textwidth}
\textbf{What We've Covered:}
\begin{itemize}
    \item \textcolor{mlpurple}{\textbf{Module 1:}} The Perceptron
    \begin{itemize}
        \item Single neuron, decision boundaries
        \item XOR limitation $\rightarrow$ AI Winter
    \end{itemize}
    \item \textcolor{mlblue}{\textbf{Module 2:}} Multi-Layer Perceptrons
    \begin{itemize}
        \item Hidden layers, activation functions
        \item Universal Approximation Theorem
    \end{itemize}
    \item \textcolor{mlorange}{\textbf{Module 3:}} Training
    \begin{itemize}
        \item Gradient descent, backpropagation
        \item Overfitting warning signs
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.38\textwidth}
\begin{center}
\textbf{The Foundation is Complete}\\[3mm]
\includegraphics[width=0.95\textwidth]{module4_applications/charts/course_summary/course_summary.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/course_summary}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/course_summary}{\includegraphics[width=0.6cm]{module4_applications/charts/course_summary/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/course_summary}{\tiny\texttt{\textcolor{gray}{course\_summary}}}
};
\end{tikzpicture}

\bottomnote{Perceptron $\rightarrow$ MLP $\rightarrow$ Training: The complete foundation}
\end{frame}

% Slide 3: The Final Question
\begin{frame}[t]{The Final Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``How do we actually use this for stock prediction?''}}\\[1.5cm]
\textbf{From theory to practice:}
\begin{itemize}
    \item How do we prevent overfitting in finance?
    \item What makes financial data different?
    \item Does this actually work?
    \item What are the ethical considerations?
\end{itemize}
\end{center}
\bottomnote{Theory meets practice}
\end{frame}

% Slide 4: Module 4 Roadmap
\begin{frame}[t]{Module 4 Roadmap}
\begin{enumerate}
    \item \textbf{Historical Context} (2012-Present)
    \begin{itemize}
        \item The deep learning revolution
    \end{itemize}
    \item \textbf{Regularization Techniques}
    \begin{itemize}
        \item L1/L2, dropout, early stopping
    \end{itemize}
    \item \textbf{Financial Data Challenges}
    \begin{itemize}
        \item Non-stationarity, regime changes, biases
    \end{itemize}
    \item \textbf{Case Study: Stock Prediction}
    \begin{itemize}
        \item S\&P 500 direction prediction (realistic assessment)
    \end{itemize}
    \item \textbf{Modern Architectures}
    \begin{itemize}
        \item CNN, RNN, Transformer overview
    \end{itemize}
    \item \textbf{Limitations and Ethics}
    \begin{itemize}
        \item What neural networks can and cannot do
    \end{itemize}
\end{enumerate}
\bottomnote{From theory to real-world applications}
\end{frame}

% Slide 5: The Reality Check
\begin{frame}[t]{The Reality Check}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Theory is Clean:}
\begin{itemize}
    \item Data is stationary
    \item Training set represents test set
    \item Patterns persist
    \item No transaction costs
    \item Unlimited computing power
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Finance is Messy:}
\begin{itemize}
    \item Markets change constantly
    \item Past may not predict future
    \item Regime changes happen
    \item Costs eat into profits
    \item Latency matters
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\begin{center}
\textcolor{mlred}{\textbf{Warning:} Paper profits $\neq$ Real profits}
\end{center}
\bottomnote{``Theory is clean. Finance is messy.''}
\end{frame}

% ==================== SECTION 2: HISTORICAL CONTEXT (Slides 6-12) ====================
\section{Historical Context: 2012-Present}

% Slide 6: 2012 - AlexNet
\begin{frame}[t]{2012: The Deep Learning Revolution}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{AlexNet (Krizhevsky et al., 2012):}
\begin{itemize}
    \item ImageNet competition: 1.2M images, 1000 classes
    \item \textcolor{mlgreen}{\textbf{Error rate: 15.3\%}} (vs. 26.2\% second place)
    \item Deep convolutional neural network (8 layers)
\end{itemize}
\vspace{3mm}
\textbf{Why This Mattered:}
\begin{itemize}
    \item 10+ percentage points better than alternatives
    \item Proved deep learning works at scale
    \item GPU training (2x NVIDIA GTX 580)
    \item Started the deep learning ``gold rush''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/modern_architectures_timeline/modern_architectures_timeline.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/modern_architectures_timeline}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/modern_architectures_timeline}{\includegraphics[width=0.6cm]{module4_applications/charts/modern_architectures_timeline/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/modern_architectures_timeline}{\tiny\texttt{\textcolor{gray}{modern\_architectures\_timeline}}}
};
\end{tikzpicture}

\bottomnote{AlexNet: When deep learning proved its superiority}
\end{frame}

% Slide 7: What Made Deep Learning Work?
\begin{frame}[t]{What Made Deep Learning Work?}
\textbf{Three Factors Converged in the 2010s:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.32\textwidth}
\textbf{1. Big Data}
\begin{itemize}
    \item ImageNet: 14M+ images
    \item Internet scale data
    \item Labeled datasets
    \item In finance: tick data, alternative data
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{2. GPU Computing}
\begin{itemize}
    \item Parallel matrix operations
    \item 100x speedup vs CPU
    \item CUDA programming
    \item Cloud GPU access
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{3. Better Algorithms}
\begin{itemize}
    \item ReLU activation
    \item Dropout regularization
    \item Batch normalization
    \item Better optimizers (Adam)
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\begin{center}
\textcolor{mlpurple}{\textbf{All three were necessary; none was sufficient alone}}
\end{center}
\bottomnote{The convergence of data, compute, and algorithms}
\end{frame}

% Slide 8: 2017 - Transformers
\begin{frame}[t]{2017: Attention Is All You Need}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Transformer Architecture (Vaswani et al., 2017):}
\begin{itemize}
    \item Originally for machine translation
    \item Key innovation: \textcolor{mlblue}{\textbf{Self-attention mechanism}}
    \item No recurrence needed $\rightarrow$ parallelizable
\end{itemize}
\vspace{3mm}
\textbf{Self-Attention Intuition:}
\begin{itemize}
    \item Each position ``attends'' to all other positions
    \item Learns which inputs are relevant to each other
    \item ``The cat sat on the mat because \textit{it} was tired''
    \item Attention reveals that ``it'' refers to ``cat''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Attention Formula:}
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
\vspace{3mm}
\begin{itemize}
    \item $Q$: Query (what am I looking for?)
    \item $K$: Key (what do I have?)
    \item $V$: Value (what do I return?)
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Vaswani et al.: The architecture that changed everything}
\end{frame}

% Slide 9: GPT Era
\begin{frame}[t]{2020+: The GPT Era}
\textbf{Scaling Laws and Foundation Models:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Key Developments:}
\begin{itemize}
    \item GPT-2 (2019): 1.5B parameters
    \item GPT-3 (2020): 175B parameters
    \item GPT-4 (2023): rumored 1T+ parameters
    \item ChatGPT: Conversational interface
\end{itemize}
\vspace{3mm}
\textbf{Scaling Discovery:}
\begin{itemize}
    \item Performance scales predictably with:
    \begin{itemize}
        \item Model size
        \item Dataset size
        \item Compute budget
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Impact on Finance:}
\begin{itemize}
    \item Sentiment analysis from news/social media
    \item Document understanding (10-K filings)
    \item Natural language queries for data
    \item Automated research summarization
\end{itemize}
\vspace{3mm}
\textcolor{mlred}{\textbf{But:} LLMs don't predict stock prices}
\begin{itemize}
    \item Different problem domain
    \item Time series $\neq$ language patterns
\end{itemize}
\end{column}
\end{columns}
\bottomnote{From GPT-2 to GPT-4 and beyond}
\end{frame}

% Slide 10: Discussion Question 1
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1.5cm}
{\Large \textit{``Why did neural networks succeed in 2012 but not in 1990?}}\\[0.5cm]
{\Large \textit{What changed?''}}
\vspace{1cm}
\begin{itemize}
    \item Was it just computing power?
    \item What role did data play?
    \item Were the algorithms fundamentally different?
    \item Could we have predicted this breakthrough?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 11: AI in Finance Today
\begin{frame}[t]{AI in Finance Today}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Major Players:}
\begin{itemize}
    \item \textbf{Renaissance Technologies}
    \begin{itemize}
        \item Medallion Fund: 66\% avg. return (1988-2018)
        \item Highly secretive, physics/math PhDs
    \end{itemize}
    \item \textbf{Two Sigma}
    \begin{itemize}
        \item \$60B+ AUM
        \item Heavy ML/AI focus
    \end{itemize}
    \item \textbf{Citadel}
    \begin{itemize}
        \item Market making + hedge fund
        \item ML for high-frequency trading
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/ai_applications_finance/ai_applications_finance.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{2mm}
\textbf{Common Applications:} Signal generation, portfolio optimization, risk management, alternative data analysis

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/ai_applications_finance}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/ai_applications_finance}{\includegraphics[width=0.6cm]{module4_applications/charts/ai_applications_finance/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/ai_applications_finance}{\tiny\texttt{\textcolor{gray}{ai\_applications\_finance}}}
};
\end{tikzpicture}

\bottomnote{Renaissance, Two Sigma, Citadel: Industry adoption}
\end{frame}

% Slide 12: The Current Landscape
\begin{frame}[t]{The Current Landscape}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{What's Actually Working:}
\begin{itemize}
    \item Risk management and fraud detection
    \item High-frequency market making
    \item Alternative data processing
    \item Portfolio optimization
    \item Credit scoring
    \item Sentiment analysis
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{What's Mostly Hype:}
\begin{itemize}
    \item ``AI that beats the market consistently''
    \item Perfect stock price prediction
    \item Fully automated trading for retail
    \item ``Guaranteed returns'' from AI
\end{itemize}
\vspace{3mm}
\textcolor{mlred}{\textbf{Red Flag:}} If someone claims their AI consistently beats the market, ask why they're selling it instead of using it.
\end{column}
\end{columns}
\bottomnote{Separating reality from marketing}
\end{frame}

% ==================== SECTION 3: REGULARIZATION (Slides 13-24) ====================
\section{Regularization: Fighting Overfitting}

% Slide 13: The Overfitting Problem Revisited
\begin{frame}[t]{The Overfitting Problem Revisited}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Recall from Module 3:}
\begin{itemize}
    \item Model learns training data too well
    \item Memorizes noise instead of patterns
    \item Fails on new, unseen data
\end{itemize}
\vspace{3mm}
\textbf{In Finance, This Is Critical:}
\begin{itemize}
    \item Backtest shows 40\% annual returns
    \item Live trading shows -15\%
    \item \textcolor{mlred}{This happens constantly}
\end{itemize}
\vspace{3mm}
\textbf{Why Module 4 Focuses on This:}
\begin{itemize}
    \item Overfitting is the \#1 failure mode
    \item Financial data is especially prone
    \item Must master regularization techniques
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{The Overfitting Gap:}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/early_stopping/early_stopping.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/early_stopping}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/early_stopping}{\includegraphics[width=0.6cm]{module4_applications/charts/early_stopping/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/early_stopping}{\tiny\texttt{\textcolor{gray}{early\_stopping}}}
};
\end{tikzpicture}

\bottomnote{Overfitting: The greatest challenge in financial ML}
\end{frame}

% Slide 14: Why Finance Overfits
\begin{frame}[t]{Why Finance Overfits So Easily}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Limited Data:}
\begin{itemize}
    \item 20 years of daily data = 5,000 samples
    \item Compare to ImageNet: 14,000,000 images
    \item Regime changes reduce effective samples further
\end{itemize}
\vspace{3mm}
\textbf{High-Dimensional Features:}
\begin{itemize}
    \item 50 technical indicators $\times$ 10 lookbacks = 500 features
    \item More parameters than data points = guaranteed overfitting
\end{itemize}
\vspace{3mm}
\textbf{Low Signal-to-Noise:}
\begin{itemize}
    \item Daily stock returns: 95\%+ noise
    \item Real patterns are tiny
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/financial_data_challenges/financial_data_challenges.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/financial_data_challenges}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/financial_data_challenges}{\includegraphics[width=0.6cm]{module4_applications/charts/financial_data_challenges/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/financial_data_challenges}{\tiny\texttt{\textcolor{gray}{financial\_data\_challenges}}}
};
\end{tikzpicture}

\bottomnote{Limited data, high noise, changing regimes}
\end{frame}

% Slide 15: Solution 1 - L2 Regularization
\begin{frame}[t]{L2 Regularization (Ridge)}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea:} Add penalty for large weights
$$\mathcal{L}_{reg} = \mathcal{L} + \frac{\lambda}{2}\|\mathbf{W}\|_2^2 = \mathcal{L} + \frac{\lambda}{2}\sum_i w_i^2$$

\textbf{Effect on Optimization:}
\begin{itemize}
    \item Original gradient: $\nabla_w \mathcal{L}$
    \item With L2: $\nabla_w \mathcal{L} + \lambda w$
    \item Weights decay toward zero each update
    \item Also called ``weight decay''
\end{itemize}
\vspace{3mm}
\textbf{Hyperparameter $\lambda$:}
\begin{itemize}
    \item $\lambda = 0$: No regularization
    \item $\lambda$ large: All weights $\rightarrow$ 0
    \item Typical: $10^{-4}$ to $10^{-2}$
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/regularization_effect/regularization_effect.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/regularization_effect}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/regularization_effect}{\includegraphics[width=0.6cm]{module4_applications/charts/regularization_effect/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/regularization_effect}{\tiny\texttt{\textcolor{gray}{regularization\_effect}}}
};
\end{tikzpicture}

\bottomnote{Push weights to be small}
\end{frame}

% Slide 16: L2 Intuition
\begin{frame}[t]{L2 Intuition}
\textbf{Why Does Penalizing Large Weights Help?}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Mathematical View:}
\begin{itemize}
    \item Large weights $\rightarrow$ extreme predictions
    \item Small changes in input $\rightarrow$ big output changes
    \item High sensitivity = memorization
    \item L2 forces smoother functions
\end{itemize}
\vspace{3mm}
\textbf{Bayesian View:}
\begin{itemize}
    \item L2 = Gaussian prior on weights
    \item Prior belief: weights should be small
    \item More data $\rightarrow$ prior matters less
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Finance Analogy:}
\begin{itemize}
    \item Large weight on one feature = ``betting everything on one stock''
    \item Risky: what if that feature stops working?
    \item L2 forces diversification across features
    \item No single feature dominates the prediction
\end{itemize}
\vspace{3mm}
\textbf{Key Insight:}
\begin{itemize}
    \item L2 doesn't eliminate features
    \item Just reduces their influence
    \item All features contribute, but moderately
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Don't let any single feature dominate}
\end{frame}

% Slide 17: Solution 2 - L1 Regularization
\begin{frame}[t]{L1 Regularization (Lasso)}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea:} Penalty proportional to absolute value
$$\mathcal{L}_{reg} = \mathcal{L} + \lambda\|\mathbf{W}\|_1 = \mathcal{L} + \lambda\sum_i |w_i|$$

\textbf{Key Difference from L2:}
\begin{itemize}
    \item L1 pushes weights to \textbf{exactly zero}
    \item Creates sparse models (feature selection)
    \item Automatically identifies irrelevant features
\end{itemize}
\vspace{3mm}
\textbf{Why Sparsity?}
\begin{itemize}
    \item L1 gradient is $\pm\lambda$ (constant)
    \item Small weights get pushed to zero
    \item L2 gradient is $\lambda w$ (proportional)
    \item Small weights shrink slowly, never reach zero
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/l1_vs_l2/l1_vs_l2.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/l1_vs_l2}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/l1_vs_l2}{\includegraphics[width=0.6cm]{module4_applications/charts/l1_vs_l2/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/l1_vs_l2}{\tiny\texttt{\textcolor{gray}{l1\_vs\_l2}}}
};
\end{tikzpicture}

\bottomnote{Push some weights to exactly zero: feature selection}
\end{frame}

% Slide 18: L1 vs L2 Comparison
\begin{frame}[t]{L1 vs L2: Comparison}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{L1 (Lasso)} & \textbf{L2 (Ridge)} \\
\midrule
Penalty term & $\lambda\sum|w_i|$ & $\frac{\lambda}{2}\sum w_i^2$ \\
Effect on weights & Some become exactly 0 & All shrink toward 0 \\
Feature selection & Yes (automatic) & No \\
Correlated features & Picks one arbitrarily & Shares weight among them \\
Sparsity & Sparse solutions & Dense solutions \\
Computation & Non-differentiable at 0 & Smooth, differentiable \\
\midrule
\textbf{Use when} & Few features matter & All features may matter \\
\bottomrule
\end{tabular}
\end{center}
\vspace{5mm}
\textbf{Elastic Net:} Combine both: $\lambda_1\|W\|_1 + \lambda_2\|W\|_2^2$\\
Best of both worlds for correlated features
\bottomnote{L1 for sparsity, L2 for shrinkage}
\end{frame}

% Slide 19: Solution 3 - Dropout
\begin{frame}[t]{Dropout: Random Deactivation}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea (Hinton et al., 2012):}
\begin{itemize}
    \item During training: randomly ``drop'' neurons
    \item Each neuron has probability $p$ of being set to 0
    \item Typically $p = 0.5$ for hidden, $p = 0.2$ for input
\end{itemize}
\vspace{3mm}
\textbf{Training:}
\begin{itemize}
    \item Each mini-batch sees different network
    \item Forces redundancy in learned features
    \item No neuron can become a ``crutch''
\end{itemize}
\vspace{3mm}
\textbf{Inference:}
\begin{itemize}
    \item Use all neurons (no dropout)
    \item Scale outputs by $(1-p)$ or use ``inverted dropout''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/dropout_visualization/dropout_visualization.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/dropout_visualization}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/dropout_visualization}{\includegraphics[width=0.6cm]{module4_applications/charts/dropout_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/dropout_visualization}{\tiny\texttt{\textcolor{gray}{dropout\_visualization}}}
};
\end{tikzpicture}

\bottomnote{``No single neuron becomes a crutch''}
\end{frame}

% Slide 20: Discussion Question 2
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1.5cm}
{\Large \textit{``How is dropout like diversifying a portfolio?''}}
\vspace{1cm}
\begin{itemize}
    \item What happens if you bet everything on one stock?
    \item What happens if a neural network relies on one neuron?
    \item How does diversification protect against failure?
    \item How does dropout force the network to diversify?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 21: Dropout Intuition
\begin{frame}[t]{Dropout Intuition: Ensemble Learning}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Ensemble Interpretation:}
\begin{itemize}
    \item Network with $n$ neurons has $2^n$ possible subnetworks
    \item Dropout trains all subnetworks simultaneously
    \item Each mini-batch samples a different subnetwork
    \item Final prediction: average of all subnetworks
\end{itemize}
\vspace{3mm}
\textbf{Why Ensembles Work:}
\begin{itemize}
    \item Different models make different errors
    \item Averaging reduces variance
    \item More robust to noise
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Finance Parallel:}
\begin{itemize}
    \item One analyst: high variance predictions
    \item Committee of analysts: more stable
    \item Dropout = ``committee of networks''
\end{itemize}
\vspace{3mm}
\textbf{Practical Notes:}
\begin{itemize}
    \item Dropout slows convergence
    \item Needs more epochs to train
    \item Don't use with batch normalization (debate)
    \item Less common in CNNs today
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Dropout approximates training an ensemble of networks}
\end{frame}

% Slide 22: Solution 4 - Early Stopping
\begin{frame}[t]{Early Stopping}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Simplest Regularization:}
\begin{itemize}
    \item Monitor validation loss during training
    \item Stop when validation loss stops improving
    \item Use the model from the best epoch
\end{itemize}
\vspace{3mm}
\textbf{Implementation:}
\begin{itemize}
    \item Track best validation loss
    \item Patience: wait $k$ epochs before stopping
    \item Save checkpoint at each improvement
    \item Restore best checkpoint at end
\end{itemize}
\vspace{3mm}
\textbf{Why It Works:}
\begin{itemize}
    \item Early epochs: learning real patterns
    \item Later epochs: memorizing training noise
    \item Sweet spot: generalization peak
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/early_stopping/early_stopping.pdf}
\end{center}
\textbf{Typical patience:} 5-20 epochs
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/early_stopping}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/early_stopping}{\includegraphics[width=0.6cm]{module4_applications/charts/early_stopping/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/early_stopping}{\tiny\texttt{\textcolor{gray}{early\_stopping}}}
};
\end{tikzpicture}

\bottomnote{Stop training when validation loss stops improving}
\end{frame}

% Slide 23: Walk-Forward Validation
\begin{frame}[t]{Walk-Forward Validation for Time Series}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Standard Cross-Validation: WRONG for Time Series}
\begin{itemize}
    \item Random splits leak future information
    \item Model sees 2024 data, predicts 2023
    \item Guaranteed overfitting
\end{itemize}
\vspace{3mm}
\textbf{Walk-Forward Validation:}
\begin{itemize}
    \item Train on [2010-2015], validate on [2016]
    \item Train on [2010-2016], validate on [2017]
    \item Train on [2010-2017], validate on [2018]
    \item Always: train on past, validate on future
\end{itemize}
\vspace{3mm}
\textbf{Anchored vs Rolling Window:}
\begin{itemize}
    \item Anchored: always start from same date
    \item Rolling: fixed window slides forward
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/walk_forward_validation/walk_forward_validation.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/walk_forward_validation}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/walk_forward_validation}{\includegraphics[width=0.6cm]{module4_applications/charts/walk_forward_validation/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/walk_forward_validation}{\tiny\texttt{\textcolor{gray}{walk\_forward\_validation}}}
};
\end{tikzpicture}

\bottomnote{Train on past, validate on future (never the reverse)}
\end{frame}

% Slide 24: Regularization Summary
\begin{frame}[t]{Fighting Overfitting: Summary}
\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Technique} & \textbf{Mechanism} & \textbf{When to Use} \\
\midrule
L2 (Ridge) & Penalize large weights & Always (as baseline) \\
L1 (Lasso) & Push weights to zero & Feature selection needed \\
Dropout & Random neuron deactivation & Deep networks \\
Early Stopping & Stop before overfitting & Always (free) \\
Walk-Forward & Time-respecting validation & Time series only \\
\bottomrule
\end{tabular}
\end{center}
\vspace{5mm}
\textbf{Practical Recommendation for Finance:}
\begin{enumerate}
    \item Always use walk-forward validation
    \item Start with L2 regularization
    \item Add early stopping (patience=10)
    \item Try dropout (0.2-0.5) for deep networks
    \item Use L1 if you need interpretable feature importance
\end{enumerate}
\bottomnote{Multiple defenses against overfitting}
\end{frame}

% ==================== SECTION 4: FINANCIAL DATA CHALLENGES (Slides 25-32) ====================
\section{Financial Data Challenges}

% Slide 25: The Nature of Financial Data
\begin{frame}[t]{The Nature of Financial Data}
\textbf{Financial Data is Fundamentally Different:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Images/Text:}
\begin{itemize}
    \item Patterns are stable over time
    \item Cat in 2020 looks like cat in 2010
    \item English grammar doesn't change daily
    \item High signal-to-noise ratio
    \item Abundant labeled data
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Financial Markets:}
\begin{itemize}
    \item Patterns change constantly
    \item Strategies that work get arbitraged away
    \item Regime changes (bull/bear/crisis)
    \item Extremely low signal-to-noise
    \item Limited history, no ``labels'' for future
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\textbf{Key Insight:} Success in image recognition doesn't translate to finance.\\
The problems are fundamentally different.
\bottomnote{Financial data is fundamentally different from images or text}
\end{frame}

% Slide 26: Non-Stationarity
\begin{frame}[t]{Non-Stationarity}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Definition:}
\begin{itemize}
    \item Statistical properties change over time
    \item Mean, variance, correlations all shift
    \item Model trained on past may fail on future
\end{itemize}
\vspace{3mm}
\textbf{Causes in Finance:}
\begin{itemize}
    \item Central bank policy changes
    \item Market structure evolution (HFT, ETFs)
    \item Regulatory changes
    \item Technology disruption
    \item Global events (pandemics, wars)
\end{itemize}
\vspace{3mm}
\textbf{Implication:}
\begin{itemize}
    \item Models have ``shelf life''
    \item Need regular retraining
    \item ``What worked'' $\neq$ ``what will work''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/regime_changes/regime_changes.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/regime_changes}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/regime_changes}{\includegraphics[width=0.6cm]{module4_applications/charts/regime_changes/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/regime_changes}{\tiny\texttt{\textcolor{gray}{regime\_changes}}}
};
\end{tikzpicture}

\bottomnote{The patterns that worked yesterday may not work tomorrow}
\end{frame}

% Slide 27: Regime Changes
\begin{frame}[t]{Regime Changes}
\textbf{Markets Switch Between Fundamentally Different Behaviors:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.32\textwidth}
\textbf{Bull Market:}
\begin{itemize}
    \item Upward trend
    \item Low volatility
    \item Mean reversion works
    \item Risk-on behavior
    \item Correlations low
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{Bear Market:}
\begin{itemize}
    \item Downward trend
    \item High volatility
    \item Momentum works
    \item Risk-off behavior
    \item Correlations spike
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{Crisis:}
\begin{itemize}
    \item Extreme moves
    \item ``All correlations go to 1''
    \item Historical patterns break
    \item Liquidity disappears
    \item Fat tails dominate
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\textbf{Challenge:} You don't know which regime you're in until it's over.\\
\textbf{Solution:} Train separate models or use regime detection.
\bottomnote{Markets switch between fundamentally different behaviors}
\end{frame}

% Slide 28: Noise and Signal
\begin{frame}[t]{Noise vs Signal}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Signal-to-Noise Ratio (SNR):}
\begin{itemize}
    \item Daily stock returns: SNR $\approx$ 0.05
    \item Speech recognition: SNR $\approx$ 10-20
    \item \textcolor{mlred}{200-400x harder!}
\end{itemize}
\vspace{3mm}
\textbf{What This Means:}
\begin{itemize}
    \item 95\%+ of price movement is random
    \item True patterns are tiny
    \item Easy to find spurious patterns
    \item Need massive data to detect signal
\end{itemize}
\vspace{3mm}
\textbf{Example:}
\begin{itemize}
    \item Average daily return: 0.04\%
    \item Daily standard deviation: 1\%
    \item Signal = return / std = 0.04
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Implications for ML:}
\begin{itemize}
    \item Models will find patterns in noise
    \item Backtests look amazing
    \item Live performance disappoints
    \item Need extreme skepticism
\end{itemize}
\vspace{3mm}
\textbf{Reality Check:}
\begin{itemize}
    \item If returns were 50\% predictable, you'd be a billionaire in months
    \item Markets are efficient enough that small edges are huge
    \item 55\% accuracy is actually impressive
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Most price movement is noise, not signal}
\end{frame}

% Slide 29: Look-Ahead Bias
\begin{frame}[t]{Look-Ahead Bias}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Definition:} Using information that wasn't available at decision time.

\vspace{3mm}
\textbf{Common Mistakes:}
\begin{itemize}
    \item Using today's adjusted close to trade at today's open
    \item Normalizing with full dataset statistics
    \item Including stocks that didn't exist yet
    \item Using restated (revised) financial data
    \item Feature engineering with future data
\end{itemize}
\vspace{3mm}
\textbf{Example:}
\begin{itemize}
    \item Train on 2020-2023
    \item Normalize: subtract mean, divide by std
    \item \textcolor{mlred}{Problem:} Mean includes 2023!
    \item In 2020, you didn't know 2023 stats
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/look_ahead_bias/look_ahead_bias.pdf}
\end{center}
\textbf{Prevention:}
\begin{itemize}
    \item Point-in-time data
    \item Rolling normalization
    \item Careful feature engineering
\end{itemize}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/look_ahead_bias}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/look_ahead_bias}{\includegraphics[width=0.6cm]{module4_applications/charts/look_ahead_bias/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/look_ahead_bias}{\tiny\texttt{\textcolor{gray}{look\_ahead\_bias}}}
};
\end{tikzpicture}

\bottomnote{The silent killer of backtests}
\end{frame}

% Slide 30: Survivorship Bias
\begin{frame}[t]{Survivorship Bias}
\textbf{Definition:} Only successful companies remain in the dataset.
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{The Problem:}
\begin{itemize}
    \item S\&P 500 today has survivors
    \item Enron, Lehman, Bear Stearns are gone
    \item Your model never sees failures
    \item Learns patterns of survivors only
\end{itemize}
\vspace{3mm}
\textbf{Impact:}
\begin{itemize}
    \item Overstates historical returns
    \item ``Average stock returned 10\%/year''
    \item Actually includes only winners
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Real Example:}
\begin{itemize}
    \item Study: ``Value stocks beat growth''
    \item Used current value stocks list
    \item Many value stocks went bankrupt
    \item True effect was much smaller
\end{itemize}
\vspace{3mm}
\textbf{Solution:}
\begin{itemize}
    \item Point-in-time constituent lists
    \item Include delisted companies
    \item Use survivorship-bias-free databases
    \item Account for delisting returns
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Your dataset doesn't include the failures}
\end{frame}

% Slide 31: Discussion Question 3
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``What makes financial prediction harder than image recognition?''}}
\vspace{1cm}
\begin{itemize}
    \item A cat is always a cat. Is a bull market always a bull market?
    \item ImageNet has 14 million labeled images. How many ``market crashes'' exist?
    \item If everyone uses the same model, what happens?
    \item Does finding patterns in finance make them disappear?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 32: Data Preprocessing
\begin{frame}[t]{Data Preprocessing for Finance}
\textbf{Essential Preprocessing Steps:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Normalization:}
\begin{itemize}
    \item Z-score: $\frac{x - \mu}{\sigma}$
    \item Use rolling window (e.g., 252 days)
    \item Never use future data!
\end{itemize}
\vspace{3mm}
\textbf{Missing Data:}
\begin{itemize}
    \item Forward fill (most common)
    \item Linear interpolation
    \item Drop if too many missing
    \item \textcolor{mlred}{Never: backward fill}
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Outlier Handling:}
\begin{itemize}
    \item Winsorize at 1\%/99\% percentile
    \item Or use robust statistics (median)
    \item Don't remove outliers blindly!
    \item Crashes are real data
\end{itemize}
\vspace{3mm}
\textbf{Feature Engineering:}
\begin{itemize}
    \item Returns not prices (stationarity)
    \item Log returns for mathematical convenience
    \item Technical indicators as features
    \item Lag features appropriately
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Proper preprocessing is essential}
\end{frame}

% ==================== SECTION 5: STOCK PREDICTION CASE STUDY (Slides 33-44) ====================
\section{Case Study: Stock Prediction}

% Slide 33: Case Study Introduction
\begin{frame}[t]{Case Study: S\&P 500 Direction Prediction}
\textbf{A Realistic Example from Start to Finish}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Goal:}
\begin{itemize}
    \item Predict S\&P 500 next-day direction
    \item Binary: Up or Down?
    \item Use only information available at market close
\end{itemize}
\vspace{3mm}
\textbf{Why This Problem:}
\begin{itemize}
    \item Simple, well-defined target
    \item Abundant data
    \item Common industry problem
    \item Illustrates key challenges
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Our Approach:}
\begin{enumerate}
    \item Define features and target
    \item Choose architecture
    \item Set up walk-forward validation
    \item Train and evaluate
    \item Reality check the results
\end{enumerate}
\vspace{3mm}
\textbf{Spoiler:} Results will be modest.\\
That's the honest truth about financial ML.
\end{column}
\end{columns}
\bottomnote{A realistic example from start to finish}
\end{frame}

% Slide 34: Problem Definition
\begin{frame}[t]{Problem Definition}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Target Variable:}
$$y_t = \begin{cases} 1 & \text{if } R_{t+1} > 0 \\ 0 & \text{if } R_{t+1} \leq 0 \end{cases}$$
where $R_{t+1} = \frac{P_{t+1} - P_t}{P_t}$ is next-day return.

\vspace{3mm}
\textbf{Baseline:}
\begin{itemize}
    \item Random guess: 50\% accuracy
    \item Actual: S\&P 500 up 53\% of days (long-term)
    \item ``Always predict up'': 53\% accuracy
\end{itemize}
\vspace{3mm}
\textbf{Goal:}
\begin{itemize}
    \item Beat 53\% consistently
    \item Out-of-sample (not just backtest)
    \item After transaction costs
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Data:}
\begin{itemize}
    \item Period: 2000-2023 (24 years)
    \item Frequency: Daily
    \item Samples: $\sim$6,000 trading days
\end{itemize}
\vspace{3mm}
\textbf{Important Notes:}
\begin{itemize}
    \item This is harder than it sounds
    \item Small edge = big money
    \item Markets are highly efficient
    \item Most published research overfits
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Binary classification: Up or Down?}
\end{frame}

% Slide 35: Input Features
\begin{frame}[t]{Input Features}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Technical Indicators (15 features):}
\begin{itemize}
    \item Returns: 1-day, 5-day, 20-day
    \item Moving averages: 10/50/200-day ratios
    \item Volatility: 20-day rolling std
    \item RSI (14-day), MACD
    \item Bollinger Band position
    \item Volume ratio (vs 20-day avg)
\end{itemize}
\vspace{3mm}
\textbf{Market Factors (5 features):}
\begin{itemize}
    \item VIX level and change
    \item Treasury yield (10Y)
    \item Credit spread
    \item Put/Call ratio
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/case_study_features/case_study_features.pdf}
\end{center}
\textbf{Total: 20 input features}
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Preprocessing:} Rolling z-score (252-day window), clip at $\pm$3

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_features}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_features}{\includegraphics[width=0.6cm]{module4_applications/charts/case_study_features/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_features}{\tiny\texttt{\textcolor{gray}{case\_study\_features}}}
};
\end{tikzpicture}

\bottomnote{15 technical indicators + 5 market factors}
\end{frame}

% Slide 36: Architecture Choice
\begin{frame}[t]{Architecture Decision}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Network: 20-16-8-1}
\begin{itemize}
    \item Input: 20 features
    \item Hidden 1: 16 neurons (ReLU)
    \item Hidden 2: 8 neurons (ReLU)
    \item Output: 1 neuron (Sigmoid)
\end{itemize}
\vspace{3mm}
\textbf{Why This Architecture?}
\begin{itemize}
    \item Relatively shallow (avoid overfitting)
    \item Decreasing width (funnel shape)
    \item Total parameters: $\sim$500
    \item Parameters $<<$ samples (6,000)
\end{itemize}
\vspace{3mm}
\textbf{Regularization:}
\begin{itemize}
    \item L2: $\lambda = 0.001$
    \item Dropout: 0.2 (after each hidden layer)
    \item Early stopping: patience=10
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/case_study_architecture/case_study_architecture.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_architecture}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_architecture}{\includegraphics[width=0.6cm]{module4_applications/charts/case_study_architecture/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_architecture}{\tiny\texttt{\textcolor{gray}{case\_study\_architecture}}}
};
\end{tikzpicture}

\bottomnote{Balancing model capacity with overfitting risk}
\end{frame}

% Slide 37: Training Setup
\begin{frame}[t]{Training Setup}
\textbf{Walk-Forward Validation:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Data Split:}
\begin{itemize}
    \item Training: 10 years (2,500 days)
    \item Validation: 2 years (500 days)
    \item Test: 2 years (500 days)
    \item Roll forward by 1 year, retrain
\end{itemize}
\vspace{3mm}
\textbf{Training Details:}
\begin{itemize}
    \item Optimizer: Adam (lr=0.001)
    \item Loss: Binary cross-entropy
    \item Batch size: 64
    \item Max epochs: 200
    \item Early stopping: patience=10
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Walk-Forward Windows:}\\[3mm]
\begin{tabular}{ccc}
\toprule
Train & Valid & Test \\
\midrule
2000-09 & 2010-11 & 2012-13 \\
2001-10 & 2011-12 & 2013-14 \\
2002-11 & 2012-13 & 2014-15 \\
... & ... & ... \\
2010-19 & 2020-21 & 2022-23 \\
\bottomrule
\end{tabular}
\vspace{3mm}
\textbf{Total:} 10 test windows
\end{column}
\end{columns}
\bottomnote{10 years training, 2 years validation, 2 years test}
\end{frame}

% Slide 38: Training Curves
\begin{frame}[t]{Results: Training Progress}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Typical Training Run (2010-2019 $\rightarrow$ 2022-23):}
\begin{itemize}
    \item Training loss decreases smoothly
    \item Validation loss: decreases, then flat
    \item Early stopping at epoch 45-80
    \item Gap between train/val loss: moderate
\end{itemize}
\vspace{3mm}
\textbf{Observations:}
\begin{itemize}
    \item \textcolor{mlgreen}{Good:} Not severe overfitting
    \item \textcolor{mlgreen}{Good:} Validation loss improves
    \item \textcolor{mlorange}{Moderate:} Some train-val gap
    \item Training accuracy: 58-62\%
    \item Validation accuracy: 54-56\%
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/case_study_training/case_study_training.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_training}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_training}{\includegraphics[width=0.6cm]{module4_applications/charts/case_study_training/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_training}{\tiny\texttt{\textcolor{gray}{case\_study\_training}}}
};
\end{tikzpicture}

\bottomnote{Monitoring the training process}
\end{frame}

% Slide 39: Results - Accuracy
\begin{frame}[t]{Results: Accuracy}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Out-of-Sample Results (2012-2023):}
\begin{itemize}
    \item Average test accuracy: \textbf{54.2\%}
    \item Range across windows: 51.8\% - 56.7\%
    \item Baseline (always up): 53.1\%
    \item \textcolor{mlgreen}{Edge over baseline: +1.1\%}
\end{itemize}
\vspace{3mm}
\textbf{By Year:}
\begin{itemize}
    \item Best: 2017 (56.7\%) - low volatility
    \item Worst: 2020 (51.8\%) - COVID crash
    \item Average bull market: 55.1\%
    \item Average bear market: 52.4\%
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/case_study_results/case_study_results.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Is 54.2\% good?} It depends on costs and execution...

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_results}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_results}{\includegraphics[width=0.6cm]{module4_applications/charts/case_study_results/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/case_study_results}{\tiny\texttt{\textcolor{gray}{case\_study\_results}}}
};
\end{tikzpicture}

\bottomnote{54.2\% accuracy - is this good?}
\end{frame}

% Slide 40: Discussion Question 4
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``If a model is 54\% accurate at predicting direction,}}\\[0.3cm]
{\Large \textit{is it profitable?''}}
\vspace{1cm}
\begin{itemize}
    \item What if each trade costs 0.1\% in fees and slippage?
    \item What if you trade once per day vs once per month?
    \item Does accuracy equal profitability?
    \item What other metrics matter?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 41: Beyond Accuracy
\begin{frame}[t]{Beyond Accuracy: Risk-Adjusted Returns}
\textbf{Accuracy $\neq$ Profitability}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{What Accuracy Misses:}
\begin{itemize}
    \item Size of wins vs losses
    \item 54\% accuracy with small wins, large losses = loss
    \item Timing of predictions
    \item Risk taken to achieve returns
\end{itemize}
\vspace{3mm}
\textbf{Better Metrics:}
\begin{itemize}
    \item \textbf{Sharpe Ratio}: $\frac{\text{Return} - R_f}{\text{Volatility}}$
    \item \textbf{Max Drawdown}: Largest peak-to-trough loss
    \item \textbf{Win/Loss Ratio}: Avg win / Avg loss
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Our Case Study:}
\begin{itemize}
    \item Annual return: 8.2\% (vs 9.5\% buy-hold)
    \item Volatility: 12.1\% (vs 18.2\% buy-hold)
    \item Sharpe: 0.68 (vs 0.52 buy-hold)
    \item Max drawdown: 18\% (vs 34\% buy-hold)
\end{itemize}
\vspace{3mm}
\textbf{Interpretation:}
\begin{itemize}
    \item Lower return than buy-hold
    \item But much lower risk
    \item Better risk-adjusted performance
    \item Before costs!
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Accuracy is not the same as profitability}
\end{frame}

% Slide 42: Transaction Costs
\begin{frame}[t]{Reality Check: Transaction Costs}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Types of Costs:}
\begin{itemize}
    \item Commission: \$0-10 per trade (retail)
    \item Bid-ask spread: 0.01\%-0.1\%
    \item Market impact: depends on size
    \item Slippage: execution vs expected price
\end{itemize}
\vspace{3mm}
\textbf{Our Strategy:}
\begin{itemize}
    \item Trades: 252 days/year (daily)
    \item Round-trip cost: 0.1\% (conservative)
    \item Annual cost: 252 $\times$ 0.1\% = 25.2\%
    \item Gross return: 8.2\%
    \item \textcolor{mlred}{\textbf{Net return: -17\%}}
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/transaction_costs/transaction_costs.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Lesson:} Daily trading requires extremely high accuracy to be profitable.

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/transaction_costs}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/transaction_costs}{\includegraphics[width=0.6cm]{module4_applications/charts/transaction_costs/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/transaction_costs}{\tiny\texttt{\textcolor{gray}{transaction\_costs}}}
};
\end{tikzpicture}

\bottomnote{Costs can eliminate paper profits entirely}
\end{frame}

% Slide 43: The EMH Question
\begin{frame}[t]{The Efficient Market Hypothesis}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{EMH (Fama, 1970):}\\
``Prices fully reflect all available information''

\vspace{3mm}
\textbf{Three Forms:}
\begin{itemize}
    \item \textbf{Weak:} Can't profit from past prices
    \item \textbf{Semi-strong:} Can't profit from public info
    \item \textbf{Strong:} Can't profit from any info
\end{itemize}
\vspace{3mm}
\textbf{Implications for ML:}
\begin{itemize}
    \item If EMH true: all patterns are noise
    \item If EMH false: patterns exist but are small
    \item Reality: markets are ``mostly efficient''
    \item Small, temporary inefficiencies exist
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/emh_visualization/emh_visualization.pdf}
\end{center}
\textbf{Grossman-Stiglitz Paradox:}\\
If markets were perfectly efficient, no one would do research, so they couldn't be efficient.
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/emh_visualization}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/emh_visualization}{\includegraphics[width=0.6cm]{module4_applications/charts/emh_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/emh_visualization}{\tiny\texttt{\textcolor{gray}{emh\_visualization}}}
};
\end{tikzpicture}

\bottomnote{Markets are (mostly) efficient}
\end{frame}

% Slide 44: Honest Assessment
\begin{frame}[t]{What Works and What Doesn't}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{What Works (Maybe):}
\begin{itemize}
    \item Risk management and hedging
    \item Alternative data processing
    \item High-frequency market making
    \item Factor model enhancement
    \item Portfolio optimization
    \item Regime detection
\end{itemize}
\vspace{3mm}
\textbf{Where NNs Add Value:}
\begin{itemize}
    \item Complex non-linear relationships
    \item High-dimensional feature spaces
    \item Alternative data (satellite, NLP)
    \item Execution optimization
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{What Doesn't Work:}
\begin{itemize}
    \item ``Predicting stock prices'' (directly)
    \item Black-box trading systems
    \item Complex models on small data
    \item Ignoring transaction costs
    \item Overfitting to backtests
\end{itemize}
\vspace{3mm}
\textbf{Honest Expectations:}
\begin{itemize}
    \item Small edges are valuable
    \item 55\% accuracy is impressive
    \item Risk management $>$ alpha generation
    \item Domain knowledge essential
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Setting appropriate expectations for neural networks in finance}
\end{frame}

% ==================== SECTION 6: MODERN ARCHITECTURES (Slides 45-50) ====================
\section{Modern Architectures}

% Slide 45: Beyond MLPs
\begin{frame}[t]{Beyond MLPs: Modern Architectures}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The MLP Foundation:}
\begin{itemize}
    \item Everything we learned applies to modern architectures
    \item Backpropagation: same algorithm
    \item Activation functions: same choices
    \item Regularization: same techniques
\end{itemize}
\vspace{3mm}
\textbf{Key Modern Architectures:}
\begin{enumerate}
    \item \textbf{CNN}: Convolutional Neural Networks
    \item \textbf{RNN/LSTM}: Recurrent Networks
    \item \textbf{Transformer}: Attention-based
\end{enumerate}
\vspace{3mm}
\textbf{Common Thread:}\\
All contain feedforward (MLP) components!
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/architecture_family_tree/architecture_family_tree.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/architecture_family_tree}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/architecture_family_tree}{\includegraphics[width=0.6cm]{module4_applications/charts/architecture_family_tree/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/architecture_family_tree}{\tiny\texttt{\textcolor{gray}{architecture\_family\_tree}}}
};
\end{tikzpicture}

\bottomnote{MLPs are the foundation for everything that followed}
\end{frame}

% Slide 46: CNNs for Time Series
\begin{frame}[t]{Convolutional Neural Networks}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Key Idea:} Learnable pattern detectors
\vspace{3mm}
\begin{itemize}
    \item Convolutional filters slide over input
    \item Detect local patterns (edges, shapes)
    \item Weight sharing reduces parameters
    \item Hierarchical feature learning
\end{itemize}
\vspace{3mm}
\textbf{For Time Series:}
\begin{itemize}
    \item 1D convolutions over time
    \item Detect patterns in price/volume sequences
    \item Filter learns what to look for
    \item E.g., ``head and shoulders'' pattern
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Architecture:}
$$\text{Conv1D} \rightarrow \text{ReLU} \rightarrow \text{Pool}$$
$$\rightarrow \text{Conv1D} \rightarrow \text{ReLU} \rightarrow \text{Pool}$$
$$\rightarrow \text{Flatten} \rightarrow \text{MLP} \rightarrow \text{Output}$$

\vspace{3mm}
\textbf{Finance Use Cases:}
\begin{itemize}
    \item Technical pattern recognition
    \item Order book analysis
    \item Multi-asset correlation patterns
\end{itemize}
\end{column}
\end{columns}
\bottomnote{CNNs: Finding patterns with learnable filters}
\end{frame}

% Slide 47: RNNs and LSTMs
\begin{frame}[t]{Recurrent Neural Networks}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Key Idea:} Memory for sequences
\vspace{3mm}
\begin{itemize}
    \item Process sequences one step at a time
    \item Maintain hidden state (memory)
    \item Output depends on current + past inputs
    \item Natural for time series
\end{itemize}
\vspace{3mm}
\textbf{RNN Update:}
$$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b)$$

\textbf{Problem:} Vanishing gradients over long sequences
\end{column}
\begin{column}{0.43\textwidth}
\textbf{LSTM Solution (1997):}
\begin{itemize}
    \item Forget gate: what to discard
    \item Input gate: what to add
    \item Output gate: what to reveal
    \item Cell state: long-term memory
\end{itemize}
\vspace{3mm}
\textbf{Finance Use Cases:}
\begin{itemize}
    \item Time series forecasting
    \item Sequence-to-sequence (prices)
    \item Combining with attention
\end{itemize}
\end{column}
\end{columns}
\bottomnote{RNNs and LSTMs: Designed for sequences}
\end{frame}

% Slide 48: Transformers
\begin{frame}[t]{Transformers and Attention}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Key Innovation:} Self-attention
\vspace{3mm}
\begin{itemize}
    \item Each position attends to all others
    \item No recurrence needed
    \item Parallelizable (fast training)
    \item Captures long-range dependencies
\end{itemize}
\vspace{3mm}
\textbf{Components:}
\begin{itemize}
    \item Multi-head attention
    \item Feedforward layers (MLPs!)
    \item Layer normalization
    \item Positional encoding
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Attention Formula:}
$$\text{Attn} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

\vspace{3mm}
\textbf{Finance Use Cases:}
\begin{itemize}
    \item News/sentiment analysis (NLP)
    \item Document understanding
    \item Multi-asset attention
    \item Temporal attention for prices
\end{itemize}
\end{column}
\end{columns}
\bottomnote{The architecture behind GPT and modern NLP}
\end{frame}

% Slide 48b: RAG Formula with Venn Diagrams
\begin{frame}[t]{RAG Formula: A Concrete Example}
\begin{columns}[T]
\begin{column}{0.52\textwidth}
\textbf{Retrieval-Augmented Generation (RAG):}
\vspace{2mm}
\begin{itemize}
    \item Combines retrieval with generation
    \item Query x retrieves relevant documents z
    \item Model generates answer y conditioned on both
\end{itemize}
\vspace{3mm}
\textbf{The RAG Formula:}
$$P(y|x) = \sum_{z} P(y|x,z) \cdot P(z|x)$$
\vspace{2mm}
\begin{itemize}
    \item $P(z|x)$: Retriever finds relevant docs
    \item $P(y|x,z)$: Generator produces answer
    \item Marginalizes over retrieved documents
\end{itemize}
\end{column}
\begin{column}{0.46\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/rag_conditional_prob/rag_conditional_prob.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{2mm}
\textbf{Finance Application:} Retrieve relevant filings/news, then generate analysis conditioned on context.
% QUANTLET_BRANDING_START
\begin{tikzpicture}[remember picture, overlay]
  \node[anchor=south west, inner sep=0pt, opacity=0.9] at ([xshift=3mm, yshift=3mm]current page.south west) {\includegraphics[height=7mm]{quantlet_tools/logo/quantlet.png}};
  \node[anchor=south east, inner sep=0pt] at ([xshift=-3mm, yshift=3mm]current page.south east) {\includegraphics[height=10mm]{module4_applications/charts/rag_conditional_prob/qr_code.png}};
  \node[anchor=south, inner sep=0pt, font=\tiny\ttfamily, text=gray] at ([yshift=4mm]current page.south) {\href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/rag_conditional_prob}{github.com/QuantLet/.../rag\_conditional\_prob}};
\end{tikzpicture}
% QUANTLET_BRANDING_END
\end{frame}

% Slide 49: Connection to Feedforward
\begin{frame}[t]{The MLP Foundation}
\textbf{Every Modern Architecture Contains MLPs:}
\vspace{5mm}
\begin{columns}[T]
\begin{column}{0.32\textwidth}
\textbf{CNN:}
\begin{itemize}
    \item Conv layers: shared MLPs
    \item Final classifier: MLP
    \item Same activation functions
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{RNN/LSTM:}
\begin{itemize}
    \item Gate computations: MLPs
    \item Output layer: MLP
    \item Same backprop algorithm
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{Transformer:}
\begin{itemize}
    \item FFN after attention: MLP
    \item Position-wise: MLP
    \item 2/3 of parameters in MLPs!
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\begin{center}
\textbf{What you learned in this course is the foundation for all of deep learning.}\\[3mm]
Perceptron $\rightarrow$ MLP $\rightarrow$ CNN/RNN/Transformer
\end{center}
\bottomnote{Every modern architecture contains feedforward components}
\end{frame}

% Slide 50: Discussion Question 5
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``If you were building a financial AI startup today,}}\\[0.3cm]
{\Large \textit{what architecture and problem would you focus on?''}}
\vspace{1cm}
\begin{itemize}
    \item Direct price prediction vs risk management?
    \item Traditional features vs alternative data?
    \item Simple MLP vs complex Transformer?
    \item Retail product vs institutional tool?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% ==================== SECTION 7: LIMITATIONS AND ETHICS (Slides 51-56) ====================
\section{Limitations and Ethical Considerations}

% Slide 51: Black Box Problem
\begin{frame}[t]{The Black Box Problem}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Interpretability Challenge:}
\begin{itemize}
    \item Neural networks: millions of parameters
    \item No simple explanation for decisions
    \item ``Why did you sell?'' - ``Because weight 47,823 was 0.0032''
\end{itemize}
\vspace{3mm}
\textbf{Why This Matters in Finance:}
\begin{itemize}
    \item Regulatory requirements (explainability)
    \item Risk management needs understanding
    \item Client trust requires explanation
    \item Debugging requires insight
\end{itemize}
\vspace{3mm}
\textbf{Partial Solutions:}
\begin{itemize}
    \item SHAP values, LIME
    \item Attention visualization
    \item Simpler models where possible
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Trade-off:}
\begin{center}
\begin{tabular}{cc}
\toprule
Simple & Complex \\
\midrule
Interpretable & Black box \\
Linear & Non-linear \\
Stable & May overfit \\
Lower accuracy & Higher accuracy \\
\bottomrule
\end{tabular}
\end{center}
\vspace{3mm}
\textbf{Question:}\\
Is 1\% more accuracy worth losing all interpretability?
\end{column}
\end{columns}
\bottomnote{Neural networks are often difficult to interpret}
\end{frame}

% Slide 52: Regulatory Requirements
\begin{frame}[t]{Regulatory Requirements}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Key Regulations:}
\begin{itemize}
    \item \textbf{MiFID II} (EU): Best execution, transparency
    \item \textbf{GDPR}: Right to explanation for automated decisions
    \item \textbf{SR 11-7} (US): Model risk management
    \item \textbf{Basel III}: Capital requirements, risk models
\end{itemize}
\vspace{3mm}
\textbf{Explainability Mandates:}
\begin{itemize}
    \item Credit decisions must be explainable
    \item Trading algorithms need documentation
    \item Model validation required
    \item Audit trails essential
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{module4_applications/charts/ethical_considerations/ethical_considerations.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Trend:} Increasing regulation of algorithmic decision-making

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/ethical_considerations}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/ethical_considerations}{\includegraphics[width=0.6cm]{module4_applications/charts/ethical_considerations/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/ethical_considerations}{\tiny\texttt{\textcolor{gray}{ethical\_considerations}}}
};
\end{tikzpicture}

\bottomnote{Regulations increasingly demand explainable AI}
\end{frame}

% Slide 53: Systemic Risk
\begin{frame}[t]{Systemic Risk from Correlated AI}
\textbf{What if everyone uses similar models?}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{The Problem:}
\begin{itemize}
    \item Similar training data
    \item Similar architectures
    \item Similar features
    \item $\Rightarrow$ Similar predictions
    \item $\Rightarrow$ Correlated trades
    \item $\Rightarrow$ Amplified market moves
\end{itemize}
\vspace{3mm}
\textbf{Historical Example:}
\begin{itemize}
    \item August 2007: Quant meltdown
    \item Many funds used similar strategies
    \item All deleveraged simultaneously
    \item Massive losses in days
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Flash Crash Risk:}
\begin{itemize}
    \item May 6, 2010: Dow dropped 1000 points in minutes
    \item Algorithmic trading implicated
    \item Feedback loops between systems
\end{itemize}
\vspace{3mm}
\textbf{Mitigations:}
\begin{itemize}
    \item Circuit breakers
    \item Position limits
    \item Diversity requirements
    \item Human oversight
    \item Stress testing for crowding
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Correlated AI trading could amplify market instability}
\end{frame}

% Slide 54: Model Risk
\begin{frame}[t]{Model Risk Management}
\textbf{``All models are wrong, some are useful'' - George Box}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Types of Model Risk:}
\begin{itemize}
    \item \textbf{Specification risk}: Wrong model type
    \item \textbf{Implementation risk}: Coding bugs
    \item \textbf{Data risk}: Bad inputs
    \item \textbf{Usage risk}: Misapplication
\end{itemize}
\vspace{3mm}
\textbf{Famous Failures:}
\begin{itemize}
    \item LTCM (1998): Model assumptions failed
    \item Knight Capital (2012): \$440M in 45 minutes
    \item London Whale (2012): VAR model issues
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Governance Framework:}
\begin{itemize}
    \item Independent model validation
    \item Documentation requirements
    \item Regular backtesting
    \item Stress testing
    \item Change management
    \item Clear ownership
\end{itemize}
\vspace{3mm}
\textbf{Key Principle:}\\
Never deploy a model you don't understand well enough to know when it might fail.
\end{column}
\end{columns}
\bottomnote{Responsible deployment requires proper oversight}
\end{frame}

% Slide 55: Historical Lesson
\begin{frame}[t]{Historical Lesson: Responsible Innovation}
\textbf{AI Has Seen Hype Cycles Before:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{The Pattern:}
\begin{enumerate}
    \item Breakthrough discovery
    \item Excessive optimism/funding
    \item Over-promising
    \item Failure to deliver
    \item ``AI Winter'' backlash
    \item Quiet progress
    \item Next breakthrough...
\end{enumerate}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Examples:}
\begin{itemize}
    \item 1960s: ``Machines will think in 20 years''
    \item 1980s: Expert systems will replace experts
    \item 2010s: ``Deep learning solves everything''
    \item Today: ``AGI is imminent''
\end{itemize}
\vspace{3mm}
\textbf{Lesson:}\\
Hype damages the field. Responsible claims and honest assessment help it grow sustainably.
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Your Responsibility:} Be honest about what neural networks can and cannot do.
\bottomnote{Hype cycles damage the field; responsible claims help it grow}
\end{frame}

% Slide 56: Where NNs Add Real Value
\begin{frame}[t]{Where Neural Networks Add Value}
\textbf{Realistic Assessment of Neural Networks in Finance:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{High Value Applications:}
\begin{itemize}
    \item \textbf{Risk Management}
    \begin{itemize}
        \item Fraud detection
        \item Credit scoring
        \item Anomaly detection
    \end{itemize}
    \item \textbf{Alternative Data}
    \begin{itemize}
        \item Satellite imagery analysis
        \item News sentiment
        \item Social media signals
    \end{itemize}
    \item \textbf{Execution}
    \begin{itemize}
        \item Optimal order routing
        \item Market making
        \item Transaction cost analysis
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Lower Value (Often Overhyped):}
\begin{itemize}
    \item Direct price prediction
    \item ``AI-powered'' retail trading apps
    \item Fully automated strategies
    \item Complex models on limited data
\end{itemize}
\vspace{3mm}
\textbf{Key Insight:}\\
Neural networks work best when:
\begin{itemize}
    \item Abundant data available
    \item Clear signal exists
    \item Domain expertise integrated
    \item Proper validation done
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Risk management, alternative data, market making}
\end{frame}

% ==================== SECTION 8: SYNTHESIS AND FUTURE (Slides 57-60) ====================
\section{Synthesis and Future}

% Slide 57: The Complete Timeline
\begin{frame}[t]{Neural Networks: 1943-2024}
\begin{center}
\includegraphics[width=0.72\textwidth]{module4_applications/charts/full_timeline_1943_2024/full_timeline_1943_2024.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/full_timeline_1943_2024}{\includegraphics[width=0.8cm]{quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.5] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/full_timeline_1943_2024}{\includegraphics[width=0.6cm]{module4_applications/charts/full_timeline_1943_2024/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/NeuralNetworks/tree/main/module4_applications/charts/full_timeline_1943_2024}{\tiny\texttt{\textcolor{gray}{full\_timeline\_1943\_2024}}}
};
\end{tikzpicture}

\bottomnote{From McCulloch-Pitts to GPT: 80 years of progress}
\end{frame}

% Slide 58: Key Takeaways
\begin{frame}[t]{Course Summary: Key Takeaways}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Module 1 - Perceptron:}
\begin{itemize}
    \item Neuron as weighted voting
    \item Linear separability limits
    \item XOR problem $\rightarrow$ AI Winter
\end{itemize}
\vspace{2mm}
\textbf{Module 2 - MLPs:}
\begin{itemize}
    \item Hidden layers solve XOR
    \item Activation functions enable non-linearity
    \item Universal Approximation Theorem
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Module 3 - Training:}
\begin{itemize}
    \item Gradient descent finds minimum
    \item Backprop: efficient gradient computation
    \item Overfitting is the main enemy
\end{itemize}
\vspace{2mm}
\textbf{Module 4 - Practice:}
\begin{itemize}
    \item Regularization fights overfitting
    \item Financial data is uniquely challenging
    \item Honest assessment of capabilities
\end{itemize}
\end{column}
\end{columns}
\vspace{3mm}
\begin{center}
\textcolor{mlpurple}{\textbf{The Foundation:}} Everything in modern AI builds on these concepts.
\end{center}
\bottomnote{The essential concepts from all four modules}
\end{frame}

% Slide 59: Where to Go from Here
\begin{frame}[t]{Where to Go from Here}
\textbf{Suggested Learning Path:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Theory:}
\begin{itemize}
    \item Deep Learning (Goodfellow et al.)
    \item Neural Networks and Deep Learning (Nielsen) - free online
    \item Stanford CS231n (CNNs)
    \item Stanford CS224n (NLP)
\end{itemize}
\vspace{3mm}
\textbf{Practice:}
\begin{itemize}
    \item PyTorch or TensorFlow tutorials
    \item Kaggle competitions
    \item Personal projects
    \item Open-source contributions
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Finance-Specific:}
\begin{itemize}
    \item Advances in Financial ML (de Prado)
    \item Machine Learning for Asset Managers
    \item QuantConnect, Zipline (backtesting)
    \item Academic papers (SSRN, arXiv q-fin)
\end{itemize}
\vspace{3mm}
\textbf{Key Advice:}
\begin{itemize}
    \item Build things!
    \item Start simple, add complexity
    \item Focus on fundamentals
    \item Be skeptical of claims
    \item Domain knowledge matters
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Suggested resources for continued learning}
\end{frame}

% Slide 60: Closing
\begin{frame}[plain]
\begin{center}
\vspace{2cm}
{\LARGE \textcolor{mlpurple}{\textbf{Thank You}}}\\[1.5cm]
{\large Neural Networks for Finance}\\
{\large BSc Lecture Series}\\[1.5cm]
{\normalsize Questions?}\\[1cm]
{\small \textcolor{mlgray}{See Mathematical Appendix for full derivations}}\\[0.5cm]
{\footnotesize \textcolor{mlgray}{Module 4: From Theory to Practice}}
\end{center}
\end{frame}


\end{document}
