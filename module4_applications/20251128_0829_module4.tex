\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]
\setbeamersize{text margin left=5mm,text margin right=5mm}

\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

\title{Module 4: From Theory to Practice}
\subtitle{Neural Networks in Finance and Modern Developments (2012-Present)}
\author{Neural Networks for Finance}
\institute{BSc Lecture Series}
\date{\today}

\begin{document}

% ==================== SECTION 1: OPENING (Slides 1-5) ====================
\section{Opening}

% Slide 1: Title
\begin{frame}[plain]
\titlepage
\end{frame}

% Slide 2: The Journey So Far
\begin{frame}[t]{The Journey So Far}
\begin{columns}[T]
\begin{column}{0.6\textwidth}
\textbf{What We've Covered:}
\begin{itemize}
    \item \textcolor{mlpurple}{\textbf{Module 1:}} The Perceptron
    \begin{itemize}
        \item Single neuron, decision boundaries
        \item XOR limitation $\rightarrow$ AI Winter
    \end{itemize}
    \item \textcolor{mlblue}{\textbf{Module 2:}} Multi-Layer Perceptrons
    \begin{itemize}
        \item Hidden layers, activation functions
        \item Universal Approximation Theorem
    \end{itemize}
    \item \textcolor{mlorange}{\textbf{Module 3:}} Training
    \begin{itemize}
        \item Gradient descent, backpropagation
        \item Overfitting warning signs
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.38\textwidth}
\begin{center}
\textbf{The Foundation is Complete}\\[3mm]
\includegraphics[width=0.95\textwidth]{charts/course_summary/course_summary.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/course_summary}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/course_summary}{\includegraphics[width=0.6cm]{charts/course_summary/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/course_summary}{\tiny\texttt{\textcolor{gray}{course\_summary}}}
};
\end{tikzpicture}

\bottomnote{Perceptron $\rightarrow$ MLP $\rightarrow$ Training: The complete foundation}
\end{frame}

% Slide 3: The Final Question
\begin{frame}[t]{The Final Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``How do we actually use this for stock prediction?''}}\\[1.5cm]
\textbf{From theory to practice:}
\begin{itemize}
    \item How do we prevent overfitting in finance?
    \item What makes financial data different?
    \item Does this actually work?
    \item What are the ethical considerations?
\end{itemize}
\end{center}
\bottomnote{Theory meets practice}
\end{frame}

% Slide 4: Module 4 Roadmap
\begin{frame}[t]{Module 4 Roadmap}
\begin{enumerate}
    \item \textbf{Historical Context} (2012-Present)
    \begin{itemize}
        \item The deep learning revolution
    \end{itemize}
    \item \textbf{Regularization Techniques}
    \begin{itemize}
        \item L1/L2, dropout, early stopping
    \end{itemize}
    \item \textbf{Financial Data Challenges}
    \begin{itemize}
        \item Non-stationarity, regime changes, biases
    \end{itemize}
    \item \textbf{Case Study: Stock Prediction}
    \begin{itemize}
        \item S\&P 500 direction prediction (realistic assessment)
    \end{itemize}
    \item \textbf{Modern Architectures}
    \begin{itemize}
        \item CNN, RNN, Transformer overview
    \end{itemize}
    \item \textbf{Limitations and Ethics}
    \begin{itemize}
        \item What neural networks can and cannot do
    \end{itemize}
\end{enumerate}
\bottomnote{From theory to real-world applications}
\end{frame}

% Slide 5: The Reality Check
\begin{frame}[t]{The Reality Check}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Theory is Clean:}
\begin{itemize}
    \item Data is stationary
    \item Training set represents test set
    \item Patterns persist
    \item No transaction costs
    \item Unlimited computing power
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Finance is Messy:}
\begin{itemize}
    \item Markets change constantly
    \item Past may not predict future
    \item Regime changes happen
    \item Costs eat into profits
    \item Latency matters
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\begin{center}
\textcolor{mlred}{\textbf{Warning:} Paper profits $\neq$ Real profits}
\end{center}
\bottomnote{``Theory is clean. Finance is messy.''}
\end{frame}

% ==================== SECTION 2: HISTORICAL CONTEXT (Slides 6-12) ====================
\section{Historical Context: 2012-Present}

% Slide 6: 2012 - AlexNet
\begin{frame}[t]{2012: The Deep Learning Revolution}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{AlexNet (Krizhevsky et al., 2012):}
\begin{itemize}
    \item ImageNet competition: 1.2M images, 1000 classes
    \item \textcolor{mlgreen}{\textbf{Error rate: 15.3\%}} (vs. 26.2\% second place)
    \item Deep convolutional neural network (8 layers)
\end{itemize}
\vspace{3mm}
\textbf{Why This Mattered:}
\begin{itemize}
    \item 10+ percentage points better than alternatives
    \item Proved deep learning works at scale
    \item GPU training (2x NVIDIA GTX 580)
    \item Started the deep learning ``gold rush''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/modern_architectures_timeline/modern_architectures_timeline.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/modern_architectures_timeline}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/modern_architectures_timeline}{\includegraphics[width=0.6cm]{charts/modern_architectures_timeline/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/modern_architectures_timeline}{\tiny\texttt{\textcolor{gray}{modern\_architectures\_timeline}}}
};
\end{tikzpicture}

\bottomnote{AlexNet: When deep learning proved its superiority}
\end{frame}

% Slide 7: What Made Deep Learning Work?
\begin{frame}[t]{What Made Deep Learning Work?}
\textbf{Three Factors Converged in the 2010s:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.32\textwidth}
\textbf{1. Big Data}
\begin{itemize}
    \item ImageNet: 14M+ images
    \item Internet scale data
    \item Labeled datasets
    \item In finance: tick data, alternative data
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{2. GPU Computing}
\begin{itemize}
    \item Parallel matrix operations
    \item 100x speedup vs CPU
    \item CUDA programming
    \item Cloud GPU access
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{3. Better Algorithms}
\begin{itemize}
    \item ReLU activation
    \item Dropout regularization
    \item Batch normalization
    \item Better optimizers (Adam)
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\begin{center}
\textcolor{mlpurple}{\textbf{All three were necessary; none was sufficient alone}}
\end{center}
\bottomnote{The convergence of data, compute, and algorithms}
\end{frame}

% Slide 8: 2017 - Transformers
\begin{frame}[t]{2017: Attention Is All You Need}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Transformer Architecture (Vaswani et al., 2017):}
\begin{itemize}
    \item Originally for machine translation
    \item Key innovation: \textcolor{mlblue}{\textbf{Self-attention mechanism}}
    \item No recurrence needed $\rightarrow$ parallelizable
\end{itemize}
\vspace{3mm}
\textbf{Self-Attention Intuition:}
\begin{itemize}
    \item Each position ``attends'' to all other positions
    \item Learns which inputs are relevant to each other
    \item ``The cat sat on the mat because \textit{it} was tired''
    \item Attention reveals that ``it'' refers to ``cat''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Attention Formula:}
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
\vspace{3mm}
\begin{itemize}
    \item $Q$: Query (what am I looking for?)
    \item $K$: Key (what do I have?)
    \item $V$: Value (what do I return?)
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Vaswani et al.: The architecture that changed everything}
\end{frame}

% Slide 9: GPT Era
\begin{frame}[t]{2020+: The GPT Era}
\textbf{Scaling Laws and Foundation Models:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Key Developments:}
\begin{itemize}
    \item GPT-2 (2019): 1.5B parameters
    \item GPT-3 (2020): 175B parameters
    \item GPT-4 (2023): rumored 1T+ parameters
    \item ChatGPT: Conversational interface
\end{itemize}
\vspace{3mm}
\textbf{Scaling Discovery:}
\begin{itemize}
    \item Performance scales predictably with:
    \begin{itemize}
        \item Model size
        \item Dataset size
        \item Compute budget
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Impact on Finance:}
\begin{itemize}
    \item Sentiment analysis from news/social media
    \item Document understanding (10-K filings)
    \item Natural language queries for data
    \item Automated research summarization
\end{itemize}
\vspace{3mm}
\textcolor{mlred}{\textbf{But:} LLMs don't predict stock prices}
\begin{itemize}
    \item Different problem domain
    \item Time series $\neq$ language patterns
\end{itemize}
\end{column}
\end{columns}
\bottomnote{From GPT-2 to GPT-4 and beyond}
\end{frame}

% Slide 10: Discussion Question 1
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1.5cm}
{\Large \textit{``Why did neural networks succeed in 2012 but not in 1990?}}\\[0.5cm]
{\Large \textit{What changed?''}}
\vspace{1cm}
\begin{itemize}
    \item Was it just computing power?
    \item What role did data play?
    \item Were the algorithms fundamentally different?
    \item Could we have predicted this breakthrough?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 11: AI in Finance Today
\begin{frame}[t]{AI in Finance Today}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Major Players:}
\begin{itemize}
    \item \textbf{Renaissance Technologies}
    \begin{itemize}
        \item Medallion Fund: 66\% avg. return (1988-2018)
        \item Highly secretive, physics/math PhDs
    \end{itemize}
    \item \textbf{Two Sigma}
    \begin{itemize}
        \item \$60B+ AUM
        \item Heavy ML/AI focus
    \end{itemize}
    \item \textbf{Citadel}
    \begin{itemize}
        \item Market making + hedge fund
        \item ML for high-frequency trading
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/ai_applications_finance/ai_applications_finance.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{2mm}
\textbf{Common Applications:} Signal generation, portfolio optimization, risk management, alternative data analysis

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/ai_applications_finance}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/ai_applications_finance}{\includegraphics[width=0.6cm]{charts/ai_applications_finance/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/ai_applications_finance}{\tiny\texttt{\textcolor{gray}{ai\_applications\_finance}}}
};
\end{tikzpicture}

\bottomnote{Renaissance, Two Sigma, Citadel: Industry adoption}
\end{frame}

% Slide 12: The Current Landscape
\begin{frame}[t]{The Current Landscape}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{What's Actually Working:}
\begin{itemize}
    \item Risk management and fraud detection
    \item High-frequency market making
    \item Alternative data processing
    \item Portfolio optimization
    \item Credit scoring
    \item Sentiment analysis
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{What's Mostly Hype:}
\begin{itemize}
    \item ``AI that beats the market consistently''
    \item Perfect stock price prediction
    \item Fully automated trading for retail
    \item ``Guaranteed returns'' from AI
\end{itemize}
\vspace{3mm}
\textcolor{mlred}{\textbf{Red Flag:}} If someone claims their AI consistently beats the market, ask why they're selling it instead of using it.
\end{column}
\end{columns}
\bottomnote{Separating reality from marketing}
\end{frame}

% ==================== SECTION 3: REGULARIZATION (Slides 13-24) ====================
\section{Regularization: Fighting Overfitting}

% Slide 13: The Overfitting Problem Revisited
\begin{frame}[t]{The Overfitting Problem Revisited}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Recall from Module 3:}
\begin{itemize}
    \item Model learns training data too well
    \item Memorizes noise instead of patterns
    \item Fails on new, unseen data
\end{itemize}
\vspace{3mm}
\textbf{In Finance, This Is Critical:}
\begin{itemize}
    \item Backtest shows 40\% annual returns
    \item Live trading shows -15\%
    \item \textcolor{mlred}{This happens constantly}
\end{itemize}
\vspace{3mm}
\textbf{Why Module 4 Focuses on This:}
\begin{itemize}
    \item Overfitting is the \#1 failure mode
    \item Financial data is especially prone
    \item Must master regularization techniques
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{The Overfitting Gap:}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/early_stopping/early_stopping.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\includegraphics[width=0.6cm]{charts/early_stopping/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\tiny\texttt{\textcolor{gray}{early\_stopping}}}
};
\end{tikzpicture}

\bottomnote{Overfitting: The greatest challenge in financial ML}
\end{frame}

% Slide 14: Why Finance Overfits
\begin{frame}[t]{Why Finance Overfits So Easily}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Limited Data:}
\begin{itemize}
    \item 20 years of daily data = 5,000 samples
    \item Compare to ImageNet: 14,000,000 images
    \item Regime changes reduce effective samples further
\end{itemize}
\vspace{3mm}
\textbf{High-Dimensional Features:}
\begin{itemize}
    \item 50 technical indicators $\times$ 10 lookbacks = 500 features
    \item More parameters than data points = guaranteed overfitting
\end{itemize}
\vspace{3mm}
\textbf{Low Signal-to-Noise:}
\begin{itemize}
    \item Daily stock returns: 95\%+ noise
    \item Real patterns are tiny
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/financial_data_challenges/financial_data_challenges.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/financial_data_challenges}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/financial_data_challenges}{\includegraphics[width=0.6cm]{charts/financial_data_challenges/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/financial_data_challenges}{\tiny\texttt{\textcolor{gray}{financial\_data\_challenges}}}
};
\end{tikzpicture}

\bottomnote{Limited data, high noise, changing regimes}
\end{frame}

% Slide 15: Solution 1 - L2 Regularization
\begin{frame}[t]{L2 Regularization (Ridge)}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea:} Add penalty for large weights
$$\mathcal{L}_{reg} = \mathcal{L} + \frac{\lambda}{2}\|\mathbf{W}\|_2^2 = \mathcal{L} + \frac{\lambda}{2}\sum_i w_i^2$$

\textbf{Effect on Optimization:}
\begin{itemize}
    \item Original gradient: $\nabla_w \mathcal{L}$
    \item With L2: $\nabla_w \mathcal{L} + \lambda w$
    \item Weights decay toward zero each update
    \item Also called ``weight decay''
\end{itemize}
\vspace{3mm}
\textbf{Hyperparameter $\lambda$:}
\begin{itemize}
    \item $\lambda = 0$: No regularization
    \item $\lambda$ large: All weights $\rightarrow$ 0
    \item Typical: $10^{-4}$ to $10^{-2}$
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/regularization_effect/regularization_effect.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regularization_effect}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regularization_effect}{\includegraphics[width=0.6cm]{charts/regularization_effect/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regularization_effect}{\tiny\texttt{\textcolor{gray}{regularization\_effect}}}
};
\end{tikzpicture}

\bottomnote{Push weights to be small}
\end{frame}

% Slide 16: L2 Intuition
\begin{frame}[t]{L2 Intuition}
\textbf{Why Does Penalizing Large Weights Help?}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Mathematical View:}
\begin{itemize}
    \item Large weights $\rightarrow$ extreme predictions
    \item Small changes in input $\rightarrow$ big output changes
    \item High sensitivity = memorization
    \item L2 forces smoother functions
\end{itemize}
\vspace{3mm}
\textbf{Bayesian View:}
\begin{itemize}
    \item L2 = Gaussian prior on weights
    \item Prior belief: weights should be small
    \item More data $\rightarrow$ prior matters less
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Finance Analogy:}
\begin{itemize}
    \item Large weight on one feature = ``betting everything on one stock''
    \item Risky: what if that feature stops working?
    \item L2 forces diversification across features
    \item No single feature dominates the prediction
\end{itemize}
\vspace{3mm}
\textbf{Key Insight:}
\begin{itemize}
    \item L2 doesn't eliminate features
    \item Just reduces their influence
    \item All features contribute, but moderately
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Don't let any single feature dominate}
\end{frame}

% Slide 17: Solution 2 - L1 Regularization
\begin{frame}[t]{L1 Regularization (Lasso)}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea:} Penalty proportional to absolute value
$$\mathcal{L}_{reg} = \mathcal{L} + \lambda\|\mathbf{W}\|_1 = \mathcal{L} + \lambda\sum_i |w_i|$$

\textbf{Key Difference from L2:}
\begin{itemize}
    \item L1 pushes weights to \textbf{exactly zero}
    \item Creates sparse models (feature selection)
    \item Automatically identifies irrelevant features
\end{itemize}
\vspace{3mm}
\textbf{Why Sparsity?}
\begin{itemize}
    \item L1 gradient is $\pm\lambda$ (constant)
    \item Small weights get pushed to zero
    \item L2 gradient is $\lambda w$ (proportional)
    \item Small weights shrink slowly, never reach zero
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/l1_vs_l2/l1_vs_l2.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/l1_vs_l2}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/l1_vs_l2}{\includegraphics[width=0.6cm]{charts/l1_vs_l2/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/l1_vs_l2}{\tiny\texttt{\textcolor{gray}{l1\_vs\_l2}}}
};
\end{tikzpicture}

\bottomnote{Push some weights to exactly zero: feature selection}
\end{frame}

% Slide 18: L1 vs L2 Comparison
\begin{frame}[t]{L1 vs L2: Comparison}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{L1 (Lasso)} & \textbf{L2 (Ridge)} \\
\midrule
Penalty term & $\lambda\sum|w_i|$ & $\frac{\lambda}{2}\sum w_i^2$ \\
Effect on weights & Some become exactly 0 & All shrink toward 0 \\
Feature selection & Yes (automatic) & No \\
Correlated features & Picks one arbitrarily & Shares weight among them \\
Sparsity & Sparse solutions & Dense solutions \\
Computation & Non-differentiable at 0 & Smooth, differentiable \\
\midrule
\textbf{Use when} & Few features matter & All features may matter \\
\bottomrule
\end{tabular}
\end{center}
\vspace{5mm}
\textbf{Elastic Net:} Combine both: $\lambda_1\|W\|_1 + \lambda_2\|W\|_2^2$\\
Best of both worlds for correlated features
\bottomnote{L1 for sparsity, L2 for shrinkage}
\end{frame}

% Slide 19: Solution 3 - Dropout
\begin{frame}[t]{Dropout: Random Deactivation}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea (Hinton et al., 2012):}
\begin{itemize}
    \item During training: randomly ``drop'' neurons
    \item Each neuron has probability $p$ of being set to 0
    \item Typically $p = 0.5$ for hidden, $p = 0.2$ for input
\end{itemize}
\vspace{3mm}
\textbf{Training:}
\begin{itemize}
    \item Each mini-batch sees different network
    \item Forces redundancy in learned features
    \item No neuron can become a ``crutch''
\end{itemize}
\vspace{3mm}
\textbf{Inference:}
\begin{itemize}
    \item Use all neurons (no dropout)
    \item Scale outputs by $(1-p)$ or use ``inverted dropout''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/dropout_visualization/dropout_visualization.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/dropout_visualization}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/dropout_visualization}{\includegraphics[width=0.6cm]{charts/dropout_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/dropout_visualization}{\tiny\texttt{\textcolor{gray}{dropout\_visualization}}}
};
\end{tikzpicture}

\bottomnote{``No single neuron becomes a crutch''}
\end{frame}

% Slide 20: Discussion Question 2
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1.5cm}
{\Large \textit{``How is dropout like diversifying a portfolio?''}}
\vspace{1cm}
\begin{itemize}
    \item What happens if you bet everything on one stock?
    \item What happens if a neural network relies on one neuron?
    \item How does diversification protect against failure?
    \item How does dropout force the network to diversify?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 21: Dropout Intuition
\begin{frame}[t]{Dropout Intuition: Ensemble Learning}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Ensemble Interpretation:}
\begin{itemize}
    \item Network with $n$ neurons has $2^n$ possible subnetworks
    \item Dropout trains all subnetworks simultaneously
    \item Each mini-batch samples a different subnetwork
    \item Final prediction: average of all subnetworks
\end{itemize}
\vspace{3mm}
\textbf{Why Ensembles Work:}
\begin{itemize}
    \item Different models make different errors
    \item Averaging reduces variance
    \item More robust to noise
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Finance Parallel:}
\begin{itemize}
    \item One analyst: high variance predictions
    \item Committee of analysts: more stable
    \item Dropout = ``committee of networks''
\end{itemize}
\vspace{3mm}
\textbf{Practical Notes:}
\begin{itemize}
    \item Dropout slows convergence
    \item Needs more epochs to train
    \item Don't use with batch normalization (debate)
    \item Less common in CNNs today
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Dropout approximates training an ensemble of networks}
\end{frame}

% Slide 22: Solution 4 - Early Stopping
\begin{frame}[t]{Early Stopping}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Simplest Regularization:}
\begin{itemize}
    \item Monitor validation loss during training
    \item Stop when validation loss stops improving
    \item Use the model from the best epoch
\end{itemize}
\vspace{3mm}
\textbf{Implementation:}
\begin{itemize}
    \item Track best validation loss
    \item Patience: wait $k$ epochs before stopping
    \item Save checkpoint at each improvement
    \item Restore best checkpoint at end
\end{itemize}
\vspace{3mm}
\textbf{Why It Works:}
\begin{itemize}
    \item Early epochs: learning real patterns
    \item Later epochs: memorizing training noise
    \item Sweet spot: generalization peak
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/early_stopping/early_stopping.pdf}
\end{center}
\textbf{Typical patience:} 5-20 epochs
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\includegraphics[width=0.6cm]{charts/early_stopping/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\tiny\texttt{\textcolor{gray}{early\_stopping}}}
};
\end{tikzpicture}

\bottomnote{Stop training when validation loss stops improving}
\end{frame}

% Slide 23: Walk-Forward Validation
\begin{frame}[t]{Walk-Forward Validation for Time Series}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Standard Cross-Validation: WRONG for Time Series}
\begin{itemize}
    \item Random splits leak future information
    \item Model sees 2024 data, predicts 2023
    \item Guaranteed overfitting
\end{itemize}
\vspace{3mm}
\textbf{Walk-Forward Validation:}
\begin{itemize}
    \item Train on [2010-2015], validate on [2016]
    \item Train on [2010-2016], validate on [2017]
    \item Train on [2010-2017], validate on [2018]
    \item Always: train on past, validate on future
\end{itemize}
\vspace{3mm}
\textbf{Anchored vs Rolling Window:}
\begin{itemize}
    \item Anchored: always start from same date
    \item Rolling: fixed window slides forward
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/walk_forward_validation/walk_forward_validation.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/walk_forward_validation}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/walk_forward_validation}{\includegraphics[width=0.6cm]{charts/walk_forward_validation/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/walk_forward_validation}{\tiny\texttt{\textcolor{gray}{walk\_forward\_validation}}}
};
\end{tikzpicture}

\bottomnote{Train on past, validate on future (never the reverse)}
\end{frame}

% Slide 24: Regularization Summary
\begin{frame}[t]{Fighting Overfitting: Summary}
\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Technique} & \textbf{Mechanism} & \textbf{When to Use} \\
\midrule
L2 (Ridge) & Penalize large weights & Always (as baseline) \\
L1 (Lasso) & Push weights to zero & Feature selection needed \\
Dropout & Random neuron deactivation & Deep networks \\
Early Stopping & Stop before overfitting & Always (free) \\
Walk-Forward & Time-respecting validation & Time series only \\
\bottomrule
\end{tabular}
\end{center}
\vspace{5mm}
\textbf{Practical Recommendation for Finance:}
\begin{enumerate}
    \item Always use walk-forward validation
    \item Start with L2 regularization
    \item Add early stopping (patience=10)
    \item Try dropout (0.2-0.5) for deep networks
    \item Use L1 if you need interpretable feature importance
\end{enumerate}
\bottomnote{Multiple defenses against overfitting}
\end{frame}

% ==================== SECTION 4: FINANCIAL DATA CHALLENGES (Slides 25-32) ====================
\section{Financial Data Challenges}

% Slide 25: The Nature of Financial Data
\begin{frame}[t]{The Nature of Financial Data}
\textbf{Financial Data is Fundamentally Different:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Images/Text:}
\begin{itemize}
    \item Patterns are stable over time
    \item Cat in 2020 looks like cat in 2010
    \item English grammar doesn't change daily
    \item High signal-to-noise ratio
    \item Abundant labeled data
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Financial Markets:}
\begin{itemize}
    \item Patterns change constantly
    \item Strategies that work get arbitraged away
    \item Regime changes (bull/bear/crisis)
    \item Extremely low signal-to-noise
    \item Limited history, no ``labels'' for future
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\textbf{Key Insight:} Success in image recognition doesn't translate to finance.\\
The problems are fundamentally different.
\bottomnote{Financial data is fundamentally different from images or text}
\end{frame}

% Slide 26: Non-Stationarity
\begin{frame}[t]{Non-Stationarity}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Definition:}
\begin{itemize}
    \item Statistical properties change over time
    \item Mean, variance, correlations all shift
    \item Model trained on past may fail on future
\end{itemize}
\vspace{3mm}
\textbf{Causes in Finance:}
\begin{itemize}
    \item Central bank policy changes
    \item Market structure evolution (HFT, ETFs)
    \item Regulatory changes
    \item Technology disruption
    \item Global events (pandemics, wars)
\end{itemize}
\vspace{3mm}
\textbf{Implication:}
\begin{itemize}
    \item Models have ``shelf life''
    \item Need regular retraining
    \item ``What worked'' $\neq$ ``what will work''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/regime_changes/regime_changes.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regime_changes}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regime_changes}{\includegraphics[width=0.6cm]{charts/regime_changes/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regime_changes}{\tiny\texttt{\textcolor{gray}{regime\_changes}}}
};
\end{tikzpicture}

\bottomnote{The patterns that worked yesterday may not work tomorrow}
\end{frame}

% Slide 27: Regime Changes
\begin{frame}[t]{Regime Changes}
\textbf{Markets Switch Between Fundamentally Different Behaviors:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.32\textwidth}
\textbf{Bull Market:}
\begin{itemize}
    \item Upward trend
    \item Low volatility
    \item Mean reversion works
    \item Risk-on behavior
    \item Correlations low
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{Bear Market:}
\begin{itemize}
    \item Downward trend
    \item High volatility
    \item Momentum works
    \item Risk-off behavior
    \item Correlations spike
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{Crisis:}
\begin{itemize}
    \item Extreme moves
    \item ``All correlations go to 1''
    \item Historical patterns break
    \item Liquidity disappears
    \item Fat tails dominate
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\textbf{Challenge:} You don't know which regime you're in until it's over.\\
\textbf{Solution:} Train separate models or use regime detection.
\bottomnote{Markets switch between fundamentally different behaviors}
\end{frame}

% Slide 28: Noise and Signal
\begin{frame}[t]{Noise vs Signal}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Signal-to-Noise Ratio (SNR):}
\begin{itemize}
    \item Daily stock returns: SNR $\approx$ 0.05
    \item Speech recognition: SNR $\approx$ 10-20
    \item \textcolor{mlred}{200-400x harder!}
\end{itemize}
\vspace{3mm}
\textbf{What This Means:}
\begin{itemize}
    \item 95\%+ of price movement is random
    \item True patterns are tiny
    \item Easy to find spurious patterns
    \item Need massive data to detect signal
\end{itemize}
\vspace{3mm}
\textbf{Example:}
\begin{itemize}
    \item Average daily return: 0.04\%
    \item Daily standard deviation: 1\%
    \item Signal = return / std = 0.04
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Implications for ML:}
\begin{itemize}
    \item Models will find patterns in noise
    \item Backtests look amazing
    \item Live performance disappoints
    \item Need extreme skepticism
\end{itemize}
\vspace{3mm}
\textbf{Reality Check:}
\begin{itemize}
    \item If returns were 50\% predictable, you'd be a billionaire in months
    \item Markets are efficient enough that small edges are huge
    \item 55\% accuracy is actually impressive
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Most price movement is noise, not signal}
\end{frame}

% Slide 29: Look-Ahead Bias
\begin{frame}[t]{Look-Ahead Bias}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Definition:} Using information that wasn't available at decision time.

\vspace{3mm}
\textbf{Common Mistakes:}
\begin{itemize}
    \item Using today's adjusted close to trade at today's open
    \item Normalizing with full dataset statistics
    \item Including stocks that didn't exist yet
    \item Using restated (revised) financial data
    \item Feature engineering with future data
\end{itemize}
\vspace{3mm}
\textbf{Example:}
\begin{itemize}
    \item Train on 2020-2023
    \item Normalize: subtract mean, divide by std
    \item \textcolor{mlred}{Problem:} Mean includes 2023!
    \item In 2020, you didn't know 2023 stats
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/look_ahead_bias/look_ahead_bias.pdf}
\end{center}
\textbf{Prevention:}
\begin{itemize}
    \item Point-in-time data
    \item Rolling normalization
    \item Careful feature engineering
\end{itemize}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/look_ahead_bias}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/look_ahead_bias}{\includegraphics[width=0.6cm]{charts/look_ahead_bias/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/look_ahead_bias}{\tiny\texttt{\textcolor{gray}{look\_ahead\_bias}}}
};
\end{tikzpicture}

\bottomnote{The silent killer of backtests}
\end{frame}

% Slide 30: Survivorship Bias
\begin{frame}[t]{Survivorship Bias}
\textbf{Definition:} Only successful companies remain in the dataset.
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{The Problem:}
\begin{itemize}
    \item S\&P 500 today has survivors
    \item Enron, Lehman, Bear Stearns are gone
    \item Your model never sees failures
    \item Learns patterns of survivors only
\end{itemize}
\vspace{3mm}
\textbf{Impact:}
\begin{itemize}
    \item Overstates historical returns
    \item ``Average stock returned 10\%/year''
    \item Actually includes only winners
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Real Example:}
\begin{itemize}
    \item Study: ``Value stocks beat growth''
    \item Used current value stocks list
    \item Many value stocks went bankrupt
    \item True effect was much smaller
\end{itemize}
\vspace{3mm}
\textbf{Solution:}
\begin{itemize}
    \item Point-in-time constituent lists
    \item Include delisted companies
    \item Use survivorship-bias-free databases
    \item Account for delisting returns
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Your dataset doesn't include the failures}
\end{frame}

% Slide 31: Discussion Question 3
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``What makes financial prediction harder than image recognition?''}}
\vspace{1cm}
\begin{itemize}
    \item A cat is always a cat. Is a bull market always a bull market?
    \item ImageNet has 14 million labeled images. How many ``market crashes'' exist?
    \item If everyone uses the same model, what happens?
    \item Does finding patterns in finance make them disappear?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 32: Data Preprocessing
\begin{frame}[t]{Data Preprocessing for Finance}
\textbf{Essential Preprocessing Steps:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Normalization:}
\begin{itemize}
    \item Z-score: $\frac{x - \mu}{\sigma}$
    \item Use rolling window (e.g., 252 days)
    \item Never use future data!
\end{itemize}
\vspace{3mm}
\textbf{Missing Data:}
\begin{itemize}
    \item Forward fill (most common)
    \item Linear interpolation
    \item Drop if too many missing
    \item \textcolor{mlred}{Never: backward fill}
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Outlier Handling:}
\begin{itemize}
    \item Winsorize at 1\%/99\% percentile
    \item Or use robust statistics (median)
    \item Don't remove outliers blindly!
    \item Crashes are real data
\end{itemize}
\vspace{3mm}
\textbf{Feature Engineering:}
\begin{itemize}
    \item Returns not prices (stationarity)
    \item Log returns for mathematical convenience
    \item Technical indicators as features
    \item Lag features appropriately
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Proper preprocessing is essential}
\end{frame}

% ==================== SECTION 5: STOCK PREDICTION CASE STUDY (Slides 33-44) ====================
\section{Case Study: Stock Prediction}

% Slide 33: Case Study Introduction
\begin{frame}[t]{Case Study: S\&P 500 Direction Prediction}
\textbf{A Realistic Example from Start to Finish}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Goal:}
\begin{itemize}
    \item Predict S\&P 500 next-day direction
    \item Binary: Up or Down?
    \item Use only information available at market close
\end{itemize}
\vspace{3mm}
\textbf{Why This Problem:}
\begin{itemize}
    \item Simple, well-defined target
    \item Abundant data
    \item Common industry problem
    \item Illustrates key challenges
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Our Approach:}
\begin{enumerate}
    \item Define features and target
    \item Choose architecture
    \item Set up walk-forward validation
    \item Train and evaluate
    \item Reality check the results
\end{enumerate}
\vspace{3mm}
\textbf{Spoiler:} Results will be modest.\\
That's the honest truth about financial ML.
\end{column}
\end{columns}
\bottomnote{A realistic example from start to finish}
\end{frame}

% Slide 34: Problem Definition
\begin{frame}[t]{Problem Definition}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Target Variable:}
$$y_t = \begin{cases} 1 & \text{if } R_{t+1} > 0 \\ 0 & \text{if } R_{t+1} \leq 0 \end{cases}$$
where $R_{t+1} = \frac{P_{t+1} - P_t}{P_t}$ is next-day return.

\vspace{3mm}
\textbf{Baseline:}
\begin{itemize}
    \item Random guess: 50\% accuracy
    \item Actual: S\&P 500 up 53\% of days (long-term)
    \item ``Always predict up'': 53\% accuracy
\end{itemize}
\vspace{3mm}
\textbf{Goal:}
\begin{itemize}
    \item Beat 53\% consistently
    \item Out-of-sample (not just backtest)
    \item After transaction costs
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Data:}
\begin{itemize}
    \item Period: 2000-2023 (24 years)
    \item Frequency: Daily
    \item Samples: $\sim$6,000 trading days
\end{itemize}
\vspace{3mm}
\textbf{Important Notes:}
\begin{itemize}
    \item This is harder than it sounds
    \item Small edge = big money
    \item Markets are highly efficient
    \item Most published research overfits
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Binary classification: Up or Down?}
\end{frame}

% Slide 35: Input Features
\begin{frame}[t]{Input Features}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Technical Indicators (15 features):}
\begin{itemize}
    \item Returns: 1-day, 5-day, 20-day
    \item Moving averages: 10/50/200-day ratios
    \item Volatility: 20-day rolling std
    \item RSI (14-day), MACD
    \item Bollinger Band position
    \item Volume ratio (vs 20-day avg)
\end{itemize}
\vspace{3mm}
\textbf{Market Factors (5 features):}
\begin{itemize}
    \item VIX level and change
    \item Treasury yield (10Y)
    \item Credit spread
    \item Put/Call ratio
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/case_study_features/case_study_features.pdf}
\end{center}
\textbf{Total: 20 input features}
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Preprocessing:} Rolling z-score (252-day window), clip at $\pm$3

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_features}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_features}{\includegraphics[width=0.6cm]{charts/case_study_features/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_features}{\tiny\texttt{\textcolor{gray}{case\_study\_features}}}
};
\end{tikzpicture}

\bottomnote{15 technical indicators + 5 market factors}
\end{frame}

% Slide 36: Architecture Choice
\begin{frame}[t]{Architecture Decision}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Network: 20-16-8-1}
\begin{itemize}
    \item Input: 20 features
    \item Hidden 1: 16 neurons (ReLU)
    \item Hidden 2: 8 neurons (ReLU)
    \item Output: 1 neuron (Sigmoid)
\end{itemize}
\vspace{3mm}
\textbf{Why This Architecture?}
\begin{itemize}
    \item Relatively shallow (avoid overfitting)
    \item Decreasing width (funnel shape)
    \item Total parameters: $\sim$500
    \item Parameters $<<$ samples (6,000)
\end{itemize}
\vspace{3mm}
\textbf{Regularization:}
\begin{itemize}
    \item L2: $\lambda = 0.001$
    \item Dropout: 0.2 (after each hidden layer)
    \item Early stopping: patience=10
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/case_study_architecture/case_study_architecture.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_architecture}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_architecture}{\includegraphics[width=0.6cm]{charts/case_study_architecture/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_architecture}{\tiny\texttt{\textcolor{gray}{case\_study\_architecture}}}
};
\end{tikzpicture}

\bottomnote{Balancing model capacity with overfitting risk}
\end{frame}

% Slide 37: Training Setup
\begin{frame}[t]{Training Setup}
\textbf{Walk-Forward Validation:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Data Split:}
\begin{itemize}
    \item Training: 10 years (2,500 days)
    \item Validation: 2 years (500 days)
    \item Test: 2 years (500 days)
    \item Roll forward by 1 year, retrain
\end{itemize}
\vspace{3mm}
\textbf{Training Details:}
\begin{itemize}
    \item Optimizer: Adam (lr=0.001)
    \item Loss: Binary cross-entropy
    \item Batch size: 64
    \item Max epochs: 200
    \item Early stopping: patience=10
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Walk-Forward Windows:}\\[3mm]
\begin{tabular}{ccc}
\toprule
Train & Valid & Test \\
\midrule
2000-09 & 2010-11 & 2012-13 \\
2001-10 & 2011-12 & 2013-14 \\
2002-11 & 2012-13 & 2014-15 \\
... & ... & ... \\
2010-19 & 2020-21 & 2022-23 \\
\bottomrule
\end{tabular}
\vspace{3mm}
\textbf{Total:} 10 test windows
\end{column}
\end{columns}
\bottomnote{10 years training, 2 years validation, 2 years test}
\end{frame}

% Slide 38: Training Curves
\begin{frame}[t]{Results: Training Progress}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Typical Training Run (2010-2019 $\rightarrow$ 2022-23):}
\begin{itemize}
    \item Training loss decreases smoothly
    \item Validation loss: decreases, then flat
    \item Early stopping at epoch 45-80
    \item Gap between train/val loss: moderate
\end{itemize}
\vspace{3mm}
\textbf{Observations:}
\begin{itemize}
    \item \textcolor{mlgreen}{Good:} Not severe overfitting
    \item \textcolor{mlgreen}{Good:} Validation loss improves
    \item \textcolor{mlorange}{Moderate:} Some train-val gap
    \item Training accuracy: 58-62\%
    \item Validation accuracy: 54-56\%
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/case_study_training/case_study_training.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_training}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_training}{\includegraphics[width=0.6cm]{charts/case_study_training/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_training}{\tiny\texttt{\textcolor{gray}{case\_study\_training}}}
};
\end{tikzpicture}

\bottomnote{Monitoring the training process}
\end{frame}

% Slide 39: Results - Accuracy
\begin{frame}[t]{Results: Accuracy}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Out-of-Sample Results (2012-2023):}
\begin{itemize}
    \item Average test accuracy: \textbf{54.2\%}
    \item Range across windows: 51.8\% - 56.7\%
    \item Baseline (always up): 53.1\%
    \item \textcolor{mlgreen}{Edge over baseline: +1.1\%}
\end{itemize}
\vspace{3mm}
\textbf{By Year:}
\begin{itemize}
    \item Best: 2017 (56.7\%) - low volatility
    \item Worst: 2020 (51.8\%) - COVID crash
    \item Average bull market: 55.1\%
    \item Average bear market: 52.4\%
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/case_study_results/case_study_results.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Is 54.2\% good?} It depends on costs and execution...

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_results}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_results}{\includegraphics[width=0.6cm]{charts/case_study_results/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/case_study_results}{\tiny\texttt{\textcolor{gray}{case\_study\_results}}}
};
\end{tikzpicture}

\bottomnote{54.2\% accuracy - is this good?}
\end{frame}

% Slide 40: Discussion Question 4
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``If a model is 54\% accurate at predicting direction,}}\\[0.3cm]
{\Large \textit{is it profitable?''}}
\vspace{1cm}
\begin{itemize}
    \item What if each trade costs 0.1\% in fees and slippage?
    \item What if you trade once per day vs once per month?
    \item Does accuracy equal profitability?
    \item What other metrics matter?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 41: Beyond Accuracy
\begin{frame}[t]{Beyond Accuracy: Risk-Adjusted Returns}
\textbf{Accuracy $\neq$ Profitability}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{What Accuracy Misses:}
\begin{itemize}
    \item Size of wins vs losses
    \item 54\% accuracy with small wins, large losses = loss
    \item Timing of predictions
    \item Risk taken to achieve returns
\end{itemize}
\vspace{3mm}
\textbf{Better Metrics:}
\begin{itemize}
    \item \textbf{Sharpe Ratio}: $\frac{\text{Return} - R_f}{\text{Volatility}}$
    \item \textbf{Max Drawdown}: Largest peak-to-trough loss
    \item \textbf{Win/Loss Ratio}: Avg win / Avg loss
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Our Case Study:}
\begin{itemize}
    \item Annual return: 8.2\% (vs 9.5\% buy-hold)
    \item Volatility: 12.1\% (vs 18.2\% buy-hold)
    \item Sharpe: 0.68 (vs 0.52 buy-hold)
    \item Max drawdown: 18\% (vs 34\% buy-hold)
\end{itemize}
\vspace{3mm}
\textbf{Interpretation:}
\begin{itemize}
    \item Lower return than buy-hold
    \item But much lower risk
    \item Better risk-adjusted performance
    \item Before costs!
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Accuracy is not the same as profitability}
\end{frame}

% Slide 42: Transaction Costs
\begin{frame}[t]{Reality Check: Transaction Costs}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Types of Costs:}
\begin{itemize}
    \item Commission: \$0-10 per trade (retail)
    \item Bid-ask spread: 0.01\%-0.1\%
    \item Market impact: depends on size
    \item Slippage: execution vs expected price
\end{itemize}
\vspace{3mm}
\textbf{Our Strategy:}
\begin{itemize}
    \item Trades: 252 days/year (daily)
    \item Round-trip cost: 0.1\% (conservative)
    \item Annual cost: 252 $\times$ 0.1\% = 25.2\%
    \item Gross return: 8.2\%
    \item \textcolor{mlred}{\textbf{Net return: -17\%}}
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/transaction_costs/transaction_costs.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Lesson:} Daily trading requires extremely high accuracy to be profitable.

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/transaction_costs}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/transaction_costs}{\includegraphics[width=0.6cm]{charts/transaction_costs/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/transaction_costs}{\tiny\texttt{\textcolor{gray}{transaction\_costs}}}
};
\end{tikzpicture}

\bottomnote{Costs can eliminate paper profits entirely}
\end{frame}

% Slide 43: The EMH Question
\begin{frame}[t]{The Efficient Market Hypothesis}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{EMH (Fama, 1970):}\\
``Prices fully reflect all available information''

\vspace{3mm}
\textbf{Three Forms:}
\begin{itemize}
    \item \textbf{Weak:} Can't profit from past prices
    \item \textbf{Semi-strong:} Can't profit from public info
    \item \textbf{Strong:} Can't profit from any info
\end{itemize}
\vspace{3mm}
\textbf{Implications for ML:}
\begin{itemize}
    \item If EMH true: all patterns are noise
    \item If EMH false: patterns exist but are small
    \item Reality: markets are ``mostly efficient''
    \item Small, temporary inefficiencies exist
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/emh_visualization/emh_visualization.pdf}
\end{center}
\textbf{Grossman-Stiglitz Paradox:}\\
If markets were perfectly efficient, no one would do research, so they couldn't be efficient.
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/emh_visualization}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/emh_visualization}{\includegraphics[width=0.6cm]{charts/emh_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/emh_visualization}{\tiny\texttt{\textcolor{gray}{emh\_visualization}}}
};
\end{tikzpicture}

\bottomnote{Markets are (mostly) efficient}
\end{frame}

% Slide 44: Honest Assessment
\begin{frame}[t]{What Works and What Doesn't}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{What Works (Maybe):}
\begin{itemize}
    \item Risk management and hedging
    \item Alternative data processing
    \item High-frequency market making
    \item Factor model enhancement
    \item Portfolio optimization
    \item Regime detection
\end{itemize}
\vspace{3mm}
\textbf{Where NNs Add Value:}
\begin{itemize}
    \item Complex non-linear relationships
    \item High-dimensional feature spaces
    \item Alternative data (satellite, NLP)
    \item Execution optimization
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{What Doesn't Work:}
\begin{itemize}
    \item ``Predicting stock prices'' (directly)
    \item Black-box trading systems
    \item Complex models on small data
    \item Ignoring transaction costs
    \item Overfitting to backtests
\end{itemize}
\vspace{3mm}
\textbf{Honest Expectations:}
\begin{itemize}
    \item Small edges are valuable
    \item 55\% accuracy is impressive
    \item Risk management $>$ alpha generation
    \item Domain knowledge essential
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Setting appropriate expectations for neural networks in finance}
\end{frame}

% ==================== SECTION 6: MODERN ARCHITECTURES (Slides 45-50) ====================
\section{Modern Architectures}

% Slide 45: Beyond MLPs
\begin{frame}[t]{Beyond MLPs: Modern Architectures}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The MLP Foundation:}
\begin{itemize}
    \item Everything we learned applies to modern architectures
    \item Backpropagation: same algorithm
    \item Activation functions: same choices
    \item Regularization: same techniques
\end{itemize}
\vspace{3mm}
\textbf{Key Modern Architectures:}
\begin{enumerate}
    \item \textbf{CNN}: Convolutional Neural Networks
    \item \textbf{RNN/LSTM}: Recurrent Networks
    \item \textbf{Transformer}: Attention-based
\end{enumerate}
\vspace{3mm}
\textbf{Common Thread:}\\
All contain feedforward (MLP) components!
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/architecture_family_tree/architecture_family_tree.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/architecture_family_tree}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/architecture_family_tree}{\includegraphics[width=0.6cm]{charts/architecture_family_tree/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/architecture_family_tree}{\tiny\texttt{\textcolor{gray}{architecture\_family\_tree}}}
};
\end{tikzpicture}

\bottomnote{MLPs are the foundation for everything that followed}
\end{frame}

% Slide 46: CNNs for Time Series
\begin{frame}[t]{Convolutional Neural Networks}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Key Idea:} Learnable pattern detectors
\vspace{3mm}
\begin{itemize}
    \item Convolutional filters slide over input
    \item Detect local patterns (edges, shapes)
    \item Weight sharing reduces parameters
    \item Hierarchical feature learning
\end{itemize}
\vspace{3mm}
\textbf{For Time Series:}
\begin{itemize}
    \item 1D convolutions over time
    \item Detect patterns in price/volume sequences
    \item Filter learns what to look for
    \item E.g., ``head and shoulders'' pattern
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Architecture:}
$$\text{Conv1D} \rightarrow \text{ReLU} \rightarrow \text{Pool}$$
$$\rightarrow \text{Conv1D} \rightarrow \text{ReLU} \rightarrow \text{Pool}$$
$$\rightarrow \text{Flatten} \rightarrow \text{MLP} \rightarrow \text{Output}$$

\vspace{3mm}
\textbf{Finance Use Cases:}
\begin{itemize}
    \item Technical pattern recognition
    \item Order book analysis
    \item Multi-asset correlation patterns
\end{itemize}
\end{column}
\end{columns}
\bottomnote{CNNs: Finding patterns with learnable filters}
\end{frame}

% Slide 47: RNNs and LSTMs
\begin{frame}[t]{Recurrent Neural Networks}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Key Idea:} Memory for sequences
\vspace{3mm}
\begin{itemize}
    \item Process sequences one step at a time
    \item Maintain hidden state (memory)
    \item Output depends on current + past inputs
    \item Natural for time series
\end{itemize}
\vspace{3mm}
\textbf{RNN Update:}
$$h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b)$$

\textbf{Problem:} Vanishing gradients over long sequences
\end{column}
\begin{column}{0.43\textwidth}
\textbf{LSTM Solution (1997):}
\begin{itemize}
    \item Forget gate: what to discard
    \item Input gate: what to add
    \item Output gate: what to reveal
    \item Cell state: long-term memory
\end{itemize}
\vspace{3mm}
\textbf{Finance Use Cases:}
\begin{itemize}
    \item Time series forecasting
    \item Sequence-to-sequence (prices)
    \item Combining with attention
\end{itemize}
\end{column}
\end{columns}
\bottomnote{RNNs and LSTMs: Designed for sequences}
\end{frame}

% Slide 48: Transformers
\begin{frame}[t]{Transformers and Attention}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Key Innovation:} Self-attention
\vspace{3mm}
\begin{itemize}
    \item Each position attends to all others
    \item No recurrence needed
    \item Parallelizable (fast training)
    \item Captures long-range dependencies
\end{itemize}
\vspace{3mm}
\textbf{Components:}
\begin{itemize}
    \item Multi-head attention
    \item Feedforward layers (MLPs!)
    \item Layer normalization
    \item Positional encoding
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Attention Formula:}
$$\text{Attn} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

\vspace{3mm}
\textbf{Finance Use Cases:}
\begin{itemize}
    \item News/sentiment analysis (NLP)
    \item Document understanding
    \item Multi-asset attention
    \item Temporal attention for prices
\end{itemize}
\end{column}
\end{columns}
\bottomnote{The architecture behind GPT and modern NLP}
\end{frame}

% Slide 48b: RAG Formula with Venn Diagrams
\begin{frame}[t]{RAG Formula: A Concrete Example}
\begin{columns}[T]
\begin{column}{0.52\textwidth}
\textbf{Retrieval-Augmented Generation (RAG):}
\vspace{2mm}
\begin{itemize}
    \item Combines retrieval with generation
    \item Query x retrieves relevant documents z
    \item Model generates answer y conditioned on both
\end{itemize}
\vspace{3mm}
\textbf{The RAG Formula:}
$$P(y|x) = \sum_{z} P(y|x,z) \cdot P(z|x)$$
\vspace{2mm}
\begin{itemize}
    \item $P(z|x)$: Retriever finds relevant docs
    \item $P(y|x,z)$: Generator produces answer
    \item Marginalizes over retrieved documents
\end{itemize}
\end{column}
\begin{column}{0.46\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/rag_conditional_prob/rag_conditional_prob.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{2mm}
\textbf{Finance Application:} Retrieve relevant filings/news, then generate analysis conditioned on context.
% QUANTLET_BRANDING_START
\begin{tikzpicture}[remember picture, overlay]
  \node[anchor=south west, inner sep=0pt, opacity=0.9] at ([xshift=3mm, yshift=3mm]current page.south west) {\includegraphics[height=7mm]{../quantlet_tools/logo/quantlet.png}};
  \node[anchor=south east, inner sep=0pt] at ([xshift=-3mm, yshift=3mm]current page.south east) {\includegraphics[height=10mm]{charts/rag_conditional_prob/qr_code.png}};
  \node[anchor=south, inner sep=0pt, font=\tiny\ttfamily, text=gray] at ([yshift=4mm]current page.south) {\href{https://github.com/QuantLet/neural-networks-introduction/tree/main/rag_conditional_prob}{github.com/QuantLet/.../rag\_conditional\_prob}};
\end{tikzpicture}
% QUANTLET_BRANDING_END
\end{frame}

% Slide 49: Connection to Feedforward
\begin{frame}[t]{The MLP Foundation}
\textbf{Every Modern Architecture Contains MLPs:}
\vspace{5mm}
\begin{columns}[T]
\begin{column}{0.32\textwidth}
\textbf{CNN:}
\begin{itemize}
    \item Conv layers: shared MLPs
    \item Final classifier: MLP
    \item Same activation functions
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{RNN/LSTM:}
\begin{itemize}
    \item Gate computations: MLPs
    \item Output layer: MLP
    \item Same backprop algorithm
\end{itemize}
\end{column}
\begin{column}{0.32\textwidth}
\textbf{Transformer:}
\begin{itemize}
    \item FFN after attention: MLP
    \item Position-wise: MLP
    \item 2/3 of parameters in MLPs!
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\begin{center}
\textbf{What you learned in this course is the foundation for all of deep learning.}\\[3mm]
Perceptron $\rightarrow$ MLP $\rightarrow$ CNN/RNN/Transformer
\end{center}
\bottomnote{Every modern architecture contains feedforward components}
\end{frame}

% Slide 50: Discussion Question 5
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``If you were building a financial AI startup today,}}\\[0.3cm]
{\Large \textit{what architecture and problem would you focus on?''}}
\vspace{1cm}
\begin{itemize}
    \item Direct price prediction vs risk management?
    \item Traditional features vs alternative data?
    \item Simple MLP vs complex Transformer?
    \item Retail product vs institutional tool?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% ==================== SECTION 7: LIMITATIONS AND ETHICS (Slides 51-56) ====================
\section{Limitations and Ethical Considerations}

% Slide 51: Black Box Problem
\begin{frame}[t]{The Black Box Problem}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Interpretability Challenge:}
\begin{itemize}
    \item Neural networks: millions of parameters
    \item No simple explanation for decisions
    \item ``Why did you sell?'' - ``Because weight 47,823 was 0.0032''
\end{itemize}
\vspace{3mm}
\textbf{Why This Matters in Finance:}
\begin{itemize}
    \item Regulatory requirements (explainability)
    \item Risk management needs understanding
    \item Client trust requires explanation
    \item Debugging requires insight
\end{itemize}
\vspace{3mm}
\textbf{Partial Solutions:}
\begin{itemize}
    \item SHAP values, LIME
    \item Attention visualization
    \item Simpler models where possible
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Trade-off:}
\begin{center}
\begin{tabular}{cc}
\toprule
Simple & Complex \\
\midrule
Interpretable & Black box \\
Linear & Non-linear \\
Stable & May overfit \\
Lower accuracy & Higher accuracy \\
\bottomrule
\end{tabular}
\end{center}
\vspace{3mm}
\textbf{Question:}\\
Is 1\% more accuracy worth losing all interpretability?
\end{column}
\end{columns}
\bottomnote{Neural networks are often difficult to interpret}
\end{frame}

% Slide 52: Regulatory Requirements
\begin{frame}[t]{Regulatory Requirements}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Key Regulations:}
\begin{itemize}
    \item \textbf{MiFID II} (EU): Best execution, transparency
    \item \textbf{GDPR}: Right to explanation for automated decisions
    \item \textbf{SR 11-7} (US): Model risk management
    \item \textbf{Basel III}: Capital requirements, risk models
\end{itemize}
\vspace{3mm}
\textbf{Explainability Mandates:}
\begin{itemize}
    \item Credit decisions must be explainable
    \item Trading algorithms need documentation
    \item Model validation required
    \item Audit trails essential
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{charts/ethical_considerations/ethical_considerations.pdf}
\end{center}
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Trend:} Increasing regulation of algorithmic decision-making

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/ethical_considerations}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/ethical_considerations}{\includegraphics[width=0.6cm]{charts/ethical_considerations/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/ethical_considerations}{\tiny\texttt{\textcolor{gray}{ethical\_considerations}}}
};
\end{tikzpicture}

\bottomnote{Regulations increasingly demand explainable AI}
\end{frame}

% Slide 53: Systemic Risk
\begin{frame}[t]{Systemic Risk from Correlated AI}
\textbf{What if everyone uses similar models?}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{The Problem:}
\begin{itemize}
    \item Similar training data
    \item Similar architectures
    \item Similar features
    \item $\Rightarrow$ Similar predictions
    \item $\Rightarrow$ Correlated trades
    \item $\Rightarrow$ Amplified market moves
\end{itemize}
\vspace{3mm}
\textbf{Historical Example:}
\begin{itemize}
    \item August 2007: Quant meltdown
    \item Many funds used similar strategies
    \item All deleveraged simultaneously
    \item Massive losses in days
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Flash Crash Risk:}
\begin{itemize}
    \item May 6, 2010: Dow dropped 1000 points in minutes
    \item Algorithmic trading implicated
    \item Feedback loops between systems
\end{itemize}
\vspace{3mm}
\textbf{Mitigations:}
\begin{itemize}
    \item Circuit breakers
    \item Position limits
    \item Diversity requirements
    \item Human oversight
    \item Stress testing for crowding
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Correlated AI trading could amplify market instability}
\end{frame}

% Slide 54: Model Risk
\begin{frame}[t]{Model Risk Management}
\textbf{``All models are wrong, some are useful'' - George Box}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Types of Model Risk:}
\begin{itemize}
    \item \textbf{Specification risk}: Wrong model type
    \item \textbf{Implementation risk}: Coding bugs
    \item \textbf{Data risk}: Bad inputs
    \item \textbf{Usage risk}: Misapplication
\end{itemize}
\vspace{3mm}
\textbf{Famous Failures:}
\begin{itemize}
    \item LTCM (1998): Model assumptions failed
    \item Knight Capital (2012): \$440M in 45 minutes
    \item London Whale (2012): VAR model issues
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Governance Framework:}
\begin{itemize}
    \item Independent model validation
    \item Documentation requirements
    \item Regular backtesting
    \item Stress testing
    \item Change management
    \item Clear ownership
\end{itemize}
\vspace{3mm}
\textbf{Key Principle:}\\
Never deploy a model you don't understand well enough to know when it might fail.
\end{column}
\end{columns}
\bottomnote{Responsible deployment requires proper oversight}
\end{frame}

% Slide 55: Historical Lesson
\begin{frame}[t]{Historical Lesson: Responsible Innovation}
\textbf{AI Has Seen Hype Cycles Before:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{The Pattern:}
\begin{enumerate}
    \item Breakthrough discovery
    \item Excessive optimism/funding
    \item Over-promising
    \item Failure to deliver
    \item ``AI Winter'' backlash
    \item Quiet progress
    \item Next breakthrough...
\end{enumerate}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Examples:}
\begin{itemize}
    \item 1960s: ``Machines will think in 20 years''
    \item 1980s: Expert systems will replace experts
    \item 2010s: ``Deep learning solves everything''
    \item Today: ``AGI is imminent''
\end{itemize}
\vspace{3mm}
\textbf{Lesson:}\\
Hype damages the field. Responsible claims and honest assessment help it grow sustainably.
\end{column}
\end{columns}
\vspace{3mm}
\textbf{Your Responsibility:} Be honest about what neural networks can and cannot do.
\bottomnote{Hype cycles damage the field; responsible claims help it grow}
\end{frame}

% Slide 56: Where NNs Add Real Value
\begin{frame}[t]{Where Neural Networks Add Value}
\textbf{Realistic Assessment of Neural Networks in Finance:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{High Value Applications:}
\begin{itemize}
    \item \textbf{Risk Management}
    \begin{itemize}
        \item Fraud detection
        \item Credit scoring
        \item Anomaly detection
    \end{itemize}
    \item \textbf{Alternative Data}
    \begin{itemize}
        \item Satellite imagery analysis
        \item News sentiment
        \item Social media signals
    \end{itemize}
    \item \textbf{Execution}
    \begin{itemize}
        \item Optimal order routing
        \item Market making
        \item Transaction cost analysis
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Lower Value (Often Overhyped):}
\begin{itemize}
    \item Direct price prediction
    \item ``AI-powered'' retail trading apps
    \item Fully automated strategies
    \item Complex models on limited data
\end{itemize}
\vspace{3mm}
\textbf{Key Insight:}\\
Neural networks work best when:
\begin{itemize}
    \item Abundant data available
    \item Clear signal exists
    \item Domain expertise integrated
    \item Proper validation done
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Risk management, alternative data, market making}
\end{frame}

% ==================== SECTION 8: SYNTHESIS AND FUTURE (Slides 57-60) ====================
\section{Synthesis and Future}

% Slide 57: The Complete Timeline
\begin{frame}[t]{Neural Networks: 1943-2024}
\begin{center}
\includegraphics[width=0.72\textwidth]{charts/full_timeline_1943_2024/full_timeline_1943_2024.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/full_timeline_1943_2024}{\includegraphics[width=0.8cm]{../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/full_timeline_1943_2024}{\includegraphics[width=0.6cm]{charts/full_timeline_1943_2024/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/full_timeline_1943_2024}{\tiny\texttt{\textcolor{gray}{full\_timeline\_1943\_2024}}}
};
\end{tikzpicture}

\bottomnote{From McCulloch-Pitts to GPT: 80 years of progress}
\end{frame}

% Slide 58: Key Takeaways
\begin{frame}[t]{Course Summary: Key Takeaways}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Module 1 - Perceptron:}
\begin{itemize}
    \item Neuron as weighted voting
    \item Linear separability limits
    \item XOR problem $\rightarrow$ AI Winter
\end{itemize}
\vspace{2mm}
\textbf{Module 2 - MLPs:}
\begin{itemize}
    \item Hidden layers solve XOR
    \item Activation functions enable non-linearity
    \item Universal Approximation Theorem
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Module 3 - Training:}
\begin{itemize}
    \item Gradient descent finds minimum
    \item Backprop: efficient gradient computation
    \item Overfitting is the main enemy
\end{itemize}
\vspace{2mm}
\textbf{Module 4 - Practice:}
\begin{itemize}
    \item Regularization fights overfitting
    \item Financial data is uniquely challenging
    \item Honest assessment of capabilities
\end{itemize}
\end{column}
\end{columns}
\vspace{3mm}
\begin{center}
\textcolor{mlpurple}{\textbf{The Foundation:}} Everything in modern AI builds on these concepts.
\end{center}
\bottomnote{The essential concepts from all four modules}
\end{frame}

% Slide 59: Where to Go from Here
\begin{frame}[t]{Where to Go from Here}
\textbf{Suggested Learning Path:}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Theory:}
\begin{itemize}
    \item Deep Learning (Goodfellow et al.)
    \item Neural Networks and Deep Learning (Nielsen) - free online
    \item Stanford CS231n (CNNs)
    \item Stanford CS224n (NLP)
\end{itemize}
\vspace{3mm}
\textbf{Practice:}
\begin{itemize}
    \item PyTorch or TensorFlow tutorials
    \item Kaggle competitions
    \item Personal projects
    \item Open-source contributions
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Finance-Specific:}
\begin{itemize}
    \item Advances in Financial ML (de Prado)
    \item Machine Learning for Asset Managers
    \item QuantConnect, Zipline (backtesting)
    \item Academic papers (SSRN, arXiv q-fin)
\end{itemize}
\vspace{3mm}
\textbf{Key Advice:}
\begin{itemize}
    \item Build things!
    \item Start simple, add complexity
    \item Focus on fundamentals
    \item Be skeptical of claims
    \item Domain knowledge matters
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Suggested resources for continued learning}
\end{frame}

% Slide 60: Closing
\begin{frame}[plain]
\begin{center}
\vspace{2cm}
{\LARGE \textcolor{mlpurple}{\textbf{Thank You}}}\\[1.5cm]
{\large Neural Networks for Finance}\\
{\large BSc Lecture Series}\\[1.5cm]
{\normalsize Questions?}\\[1cm]
{\small \textcolor{mlgray}{See Mathematical Appendix for full derivations}}\\[0.5cm]
{\footnotesize \textcolor{mlgray}{Module 4: From Theory to Practice}}
\end{center}
\end{frame}

\end{document}
