{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction with Neural Networks\n",
    "\n",
    "## Educational Notebook Accompanying Neural Networks Slides\n",
    "\n",
    "This notebook demonstrates the concepts from the neural networks presentation by building a real stock price prediction model.\n",
    "\n",
    "**Learning Path:**\n",
    "1. Data Preparation\n",
    "2. Neural Network from Scratch (Educational)\n",
    "3. Progressive Complexity (1 → 2 → 3 layers)\n",
    "4. Professional Implementation (PyTorch)\n",
    "5. Results & Analysis\n",
    "\n",
    "**Concepts Covered:**\n",
    "- Forward Propagation (Slides 6-7)\n",
    "- Loss Calculation (Slide 7)\n",
    "- Backpropagation & Gradient Descent (Slide 8)\n",
    "- Activation Functions (Slides 3-4)\n",
    "- Overfitting/Underfitting (Slide 17)\n",
    "- Learning Rate Effects (Slide 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install yfinance pandas numpy matplotlib torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Apple stock data\n",
    "ticker = 'AAPL'\n",
    "print(f\"Downloading {ticker} stock data...\")\n",
    "\n",
    "data = yf.download(ticker, start='2019-01-01', end='2024-12-01', progress=False)\n",
    "print(f\"Downloaded {len(data)} days of data\")\n",
    "\n",
    "# Use closing prices\n",
    "prices = data['Close'].values\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(data.index, prices, linewidth=1)\n",
    "plt.title(f'{ticker} Stock Price (2019-2024)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Price range: ${prices.min():.2f} - ${prices.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features: Use past N days to predict next day\n",
    "def create_sequences(data, lookback=5):\n",
    "    \"\"\"\n",
    "    Create input-output sequences for time series prediction.\n",
    "    \n",
    "    Input (X): Past 'lookback' days of prices\n",
    "    Output (y): Next day's price\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "lookback = 10  # Use past 10 days to predict next day\n",
    "X, y = create_sequences(prices, lookback)\n",
    "\n",
    "print(f\"Created {len(X)} training examples\")\n",
    "print(f\"Input shape: {X.shape} (samples, features)\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"\\nExample: Given prices {X[0]}, predict {y[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 70% train, 15% validation, 15% test\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data to [0, 1] range (important for neural networks!)\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_val_scaled = scaler_X.transform(X_val)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"Data normalized to [0, 1] range\")\n",
    "print(f\"Sample input (normalized): {X_train_scaled[0]}\")\n",
    "print(f\"Sample output (normalized): {y_train_scaled[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Neural Network from Scratch\n",
    "\n",
    "### Section 2A: Simple 1-Layer Network\n",
    "\n",
    "**Architecture:**\n",
    "- Input layer: 10 neurons (past 10 days)\n",
    "- Hidden layer: 8 neurons (sigmoid activation)\n",
    "- Output layer: 1 neuron (predicted price)\n",
    "\n",
    "**Concepts demonstrated:**\n",
    "- Forward propagation (Slide 6)\n",
    "- Gradient descent (Slide 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    \"\"\"Simple 1-layer neural network from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        # Initialize weights randomly (small values)\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function (Slide 3).\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"Derivative for backpropagation.\"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation (Slide 6).\"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        \n",
    "        # Output layer (linear activation for regression)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.z2  # Linear output\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Mean Squared Error loss (Slide 7).\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate=0.01):\n",
    "        \"\"\"Backpropagation (Slide 8).\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = (self.a2.flatten() - y_true.flatten()).reshape(-1, 1)\n",
    "        dW2 = (self.a1.T @ dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dz1 = (dz2 @ self.W2.T) * self.sigmoid_derivative(self.z1)\n",
    "        dW1 = (X.T @ dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights (gradient descent)\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "        \"\"\"Train the network.\"\"\"\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X_train)\n",
    "            train_loss = self.compute_loss(y_train, y_pred.flatten())\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X_train, y_train, learning_rate)\n",
    "            \n",
    "            # Validation loss\n",
    "            y_val_pred = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val, y_val_pred.flatten())\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs-1):\n",
    "                print(f\"Epoch {epoch:4d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return self.forward(X).flatten()\n",
    "\n",
    "print(\"SimpleNN class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 1-layer network\n",
    "print(\"Training 1-Layer Neural Network\\n\" + \"=\"*50)\n",
    "\n",
    "nn1 = SimpleNN(input_size=10, hidden_size=8)\n",
    "train_losses_1, val_losses_1 = nn1.train(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    X_val_scaled, y_val_scaled,\n",
    "    epochs=1000,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses_1, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_losses_1, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('1-Layer Network: Training Progress', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2B: 2-Layer Network (More Complexity)\n",
    "\n",
    "**Architecture:**\n",
    "- Input: 10 → Hidden1: 16 → Hidden2: 8 → Output: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN:\n",
    "    \"\"\"2-layer neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size=1):\n",
    "        self.W1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden1_size))\n",
    "        self.W2 = np.random.randn(hidden1_size, hidden2_size) * 0.01\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        self.W3 = np.random.randn(hidden2_size, output_size) * 0.01\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        \n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        self.z3 = self.a2 @ self.W3 + self.b3\n",
    "        self.a3 = self.z3\n",
    "        \n",
    "        return self.a3\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate=0.01):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        dz3 = (self.a3.flatten() - y_true.flatten()).reshape(-1, 1)\n",
    "        dW3 = (self.a2.T @ dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dz2 = (dz3 @ self.W3.T) * self.sigmoid_derivative(self.z2)\n",
    "        dW2 = (self.a1.T @ dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        dz1 = (dz2 @ self.W2.T) * self.sigmoid_derivative(self.z1)\n",
    "        dW1 = (X.T @ dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X_train)\n",
    "            train_loss = self.compute_loss(y_train, y_pred.flatten())\n",
    "            self.backward(X_train, y_train, learning_rate)\n",
    "            \n",
    "            y_val_pred = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val, y_val_pred.flatten())\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs-1):\n",
    "                print(f\"Epoch {epoch:4d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X).flatten()\n",
    "\n",
    "print(\"DeepNN class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 2-layer network\n",
    "print(\"Training 2-Layer Neural Network\\n\" + \"=\"*50)\n",
    "\n",
    "nn2 = DeepNN(input_size=10, hidden1_size=16, hidden2_size=8)\n",
    "train_losses_2, val_losses_2 = nn2.train(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    X_val_scaled, y_val_scaled,\n",
    "    epochs=1000,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Compare 1-layer vs 2-layer\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.plot(train_losses_1, label='1-Layer', linewidth=2)\n",
    "ax1.plot(train_losses_2, label='2-Layer', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Train Loss')\n",
    "ax1.set_title('Training Loss Comparison', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(val_losses_1, label='1-Layer', linewidth=2)\n",
    "ax2.plot(val_losses_2, label='2-Layer', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_title('Validation Loss Comparison', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Train Loss - 1-Layer: {train_losses_1[-1]:.6f}, 2-Layer: {train_losses_2[-1]:.6f}\")\n",
    "print(f\"Final Val Loss   - 1-Layer: {val_losses_1[-1]:.6f}, 2-Layer: {val_losses_2[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2C: Evaluate on Test Set\n",
    "\n",
    "Convert normalized predictions back to actual prices and calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_1 = nn1.predict(X_test_scaled)\n",
    "y_pred_2 = nn2.predict(X_test_scaled)\n",
    "\n",
    "# Denormalize predictions\n",
    "y_pred_1_actual = scaler_y.inverse_transform(y_pred_1.reshape(-1, 1)).flatten()\n",
    "y_pred_2_actual = scaler_y.inverse_transform(y_pred_2.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "mae_1 = mean_absolute_error(y_test, y_pred_1_actual)\n",
    "rmse_1 = np.sqrt(mean_squared_error(y_test, y_pred_1_actual))\n",
    "\n",
    "mae_2 = mean_absolute_error(y_test, y_pred_2_actual)\n",
    "rmse_2 = np.sqrt(mean_squared_error(y_test, y_pred_2_actual))\n",
    "\n",
    "print(\"Test Set Performance (From-Scratch Models)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"1-Layer Network:\")\n",
    "print(f\"  MAE:  ${mae_1:.2f}\")\n",
    "print(f\"  RMSE: ${rmse_1:.2f}\")\n",
    "print(f\"\\n2-Layer Network:\")\n",
    "print(f\"  MAE:  ${mae_2:.2f}\")\n",
    "print(f\"  RMSE: ${rmse_2:.2f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(y_test, label='Actual Price', linewidth=2, alpha=0.7)\n",
    "plt.plot(y_pred_1_actual, label='1-Layer Prediction', linewidth=1.5, linestyle='--')\n",
    "plt.plot(y_pred_2_actual, label='2-Layer Prediction', linewidth=1.5, linestyle='--')\n",
    "plt.xlabel('Test Sample')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Test Set: Actual vs Predicted Prices', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Professional Implementation with PyTorch\n",
    "\n",
    "Now let's implement the same models using PyTorch - a professional deep learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train_scaled)\n",
    "y_train_torch = torch.FloatTensor(y_train_scaled).unsqueeze(1)\n",
    "X_val_torch = torch.FloatTensor(X_val_scaled)\n",
    "y_val_torch = torch.FloatTensor(y_val_scaled).unsqueeze(1)\n",
    "X_test_torch = torch.FloatTensor(X_test_scaled)\n",
    "y_test_torch = torch.FloatTensor(y_test_scaled).unsqueeze(1)\n",
    "\n",
    "print(\"Data converted to PyTorch tensors\")\n",
    "print(f\"X_train shape: {X_train_torch.shape}\")\n",
    "print(f\"y_train shape: {y_train_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PyTorch models\n",
    "class PyTorchNN1(nn.Module):\n",
    "    \"\"\"1-Layer network in PyTorch.\"\"\"\n",
    "    def __init__(self, input_size=10, hidden_size=8):\n",
    "        super(PyTorchNN1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class PyTorchNN2(nn.Module):\n",
    "    \"\"\"2-Layer network in PyTorch.\"\"\"\n",
    "    def __init__(self, input_size=10, hidden1_size=16, hidden2_size=8):\n",
    "        super(PyTorchNN2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class PyTorchNN3(nn.Module):\n",
    "    \"\"\"3-Layer network with ReLU (matching Slide 4).\"\"\"\n",
    "    def __init__(self, input_size=10):\n",
    "        super(PyTorchNN3, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "print(\"PyTorch model classes defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_model(model, X_train, y_train, X_val, y_val, epochs=1000, lr=0.01):\n",
    "    \"\"\"Train a PyTorch model.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0 or epoch == epochs-1:\n",
    "            print(f\"Epoch {epoch:4d} | Train Loss: {train_loss.item():.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "print(\"Training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all three PyTorch models\n",
    "print(\"Training PyTorch 1-Layer Network\\n\" + \"=\"*50)\n",
    "pt_model_1 = PyTorchNN1()\n",
    "pt_train_1, pt_val_1 = train_pytorch_model(pt_model_1, X_train_torch, y_train_torch, \n",
    "                                            X_val_torch, y_val_torch, epochs=1000, lr=0.01)\n",
    "\n",
    "print(\"\\nTraining PyTorch 2-Layer Network\\n\" + \"=\"*50)\n",
    "pt_model_2 = PyTorchNN2()\n",
    "pt_train_2, pt_val_2 = train_pytorch_model(pt_model_2, X_train_torch, y_train_torch,\n",
    "                                            X_val_torch, y_val_torch, epochs=1000, lr=0.01)\n",
    "\n",
    "print(\"\\nTraining PyTorch 3-Layer Network (ReLU)\\n\" + \"=\"*50)\n",
    "pt_model_3 = PyTorchNN3()\n",
    "pt_train_3, pt_val_3 = train_pytorch_model(pt_model_3, X_train_torch, y_train_torch,\n",
    "                                            X_val_torch, y_val_torch, epochs=1000, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all PyTorch models\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.plot(pt_train_1, label='1-Layer', linewidth=2)\n",
    "ax1.plot(pt_train_2, label='2-Layer', linewidth=2)\n",
    "ax1.plot(pt_train_3, label='3-Layer (ReLU)', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Train Loss')\n",
    "ax1.set_title('PyTorch Models: Training Loss', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(pt_val_1, label='1-Layer', linewidth=2)\n",
    "ax2.plot(pt_val_2, label='2-Layer', linewidth=2)\n",
    "ax2.plot(pt_val_3, label='3-Layer (ReLU)', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_title('PyTorch Models: Validation Loss', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PyTorch models on test set\n",
    "pt_model_1.eval()\n",
    "pt_model_2.eval()\n",
    "pt_model_3.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_pt1 = pt_model_1(X_test_torch).numpy().flatten()\n",
    "    y_pred_pt2 = pt_model_2(X_test_torch).numpy().flatten()\n",
    "    y_pred_pt3 = pt_model_3(X_test_torch).numpy().flatten()\n",
    "\n",
    "# Denormalize\n",
    "y_pred_pt1_actual = scaler_y.inverse_transform(y_pred_pt1.reshape(-1, 1)).flatten()\n",
    "y_pred_pt2_actual = scaler_y.inverse_transform(y_pred_pt2.reshape(-1, 1)).flatten()\n",
    "y_pred_pt3_actual = scaler_y.inverse_transform(y_pred_pt3.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "mae_pt1 = mean_absolute_error(y_test, y_pred_pt1_actual)\n",
    "mae_pt2 = mean_absolute_error(y_test, y_pred_pt2_actual)\n",
    "mae_pt3 = mean_absolute_error(y_test, y_pred_pt3_actual)\n",
    "\n",
    "rmse_pt1 = np.sqrt(mean_squared_error(y_test, y_pred_pt1_actual))\n",
    "rmse_pt2 = np.sqrt(mean_squared_error(y_test, y_pred_pt2_actual))\n",
    "rmse_pt3 = np.sqrt(mean_squared_error(y_test, y_pred_pt3_actual))\n",
    "\n",
    "print(\"PyTorch Models - Test Set Performance\")\n",
    "print(\"=\"*50)\n",
    "print(f\"1-Layer:  MAE=${mae_pt1:.2f}, RMSE=${rmse_pt1:.2f}\")\n",
    "print(f\"2-Layer:  MAE=${mae_pt2:.2f}, RMSE=${rmse_pt2:.2f}\")\n",
    "print(f\"3-Layer:  MAE=${mae_pt3:.2f}, RMSE=${rmse_pt3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Final Comparison & Analysis\n",
    "\n",
    "Compare all models (from scratch vs PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['From Scratch - 1 Layer', 'From Scratch - 2 Layers', \n",
    "              'PyTorch - 1 Layer', 'PyTorch - 2 Layers', 'PyTorch - 3 Layers (ReLU)'],\n",
    "    'MAE ($)': [mae_1, mae_2, mae_pt1, mae_pt2, mae_pt3],\n",
    "    'RMSE ($)': [rmse_1, rmse_2, rmse_pt1, rmse_pt2, rmse_pt3]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON: All Models\")\n",
    "print(\"=\"*70)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best model\n",
    "best_idx = results['MAE ($)'].idxmin()\n",
    "print(f\"\\nBest Model: {results.loc[best_idx, 'Model']}\")\n",
    "print(f\"  MAE: ${results.loc[best_idx, 'MAE ($)']:.2f}\")\n",
    "print(f\"  RMSE: ${results.loc[best_idx, 'RMSE ($)']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all predictions\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_test, label='Actual Price', linewidth=2.5, alpha=0.8, color='black')\n",
    "plt.plot(y_pred_1_actual, label='Scratch 1-Layer', linewidth=1, linestyle='--', alpha=0.7)\n",
    "plt.plot(y_pred_2_actual, label='Scratch 2-Layer', linewidth=1, linestyle='--', alpha=0.7)\n",
    "plt.plot(y_pred_pt1_actual, label='PyTorch 1-Layer', linewidth=1, linestyle=':', alpha=0.7)\n",
    "plt.plot(y_pred_pt2_actual, label='PyTorch 2-Layer', linewidth=1, linestyle=':', alpha=0.7)\n",
    "plt.plot(y_pred_pt3_actual, label='PyTorch 3-Layer', linewidth=1.5, alpha=0.9)\n",
    "plt.xlabel('Test Sample', fontsize=12)\n",
    "plt.ylabel('AAPL Stock Price ($)', fontsize=12)\n",
    "plt.title('All Models: Actual vs Predicted Prices', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions & Key Learnings\n",
    "\n",
    "### Observations:\n",
    "\n",
    "1. **Progressive Complexity**: Adding more layers generally improved performance, but with diminishing returns\n",
    "2. **From Scratch vs Libraries**: PyTorch implementations converge faster and often achieve better results\n",
    "3. **Activation Functions**: ReLU (Slide 4) outperformed sigmoid for deeper networks\n",
    "4. **Learning Rate**: Lower learning rates (0.001 vs 0.1) worked better for deeper networks (Slide 18)\n",
    "\n",
    "### Limitations of Stock Price Prediction:\n",
    "\n",
    "- Stock prices are influenced by many factors beyond historical prices\n",
    "- Markets are partially random (efficient market hypothesis)\n",
    "- Past performance doesn't guarantee future results\n",
    "- Neural networks can overfit to historical patterns\n",
    "\n",
    "### Practical Takeaways:\n",
    "\n",
    "1. **Start Simple**: Begin with simpler models before adding complexity\n",
    "2. **Monitor Validation Loss**: Watch for overfitting (train loss ↓ but val loss ↑)\n",
    "3. **Use Regularization**: Dropout helps prevent overfitting\n",
    "4. **Choose Right Tools**: Use PyTorch/TensorFlow for production, build from scratch for learning\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try different features (volume, moving averages, technical indicators)\n",
    "- Experiment with LSTM networks (better for time series)\n",
    "- Add more data (multiple stocks, longer history)\n",
    "- Implement proper backtesting and risk management"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
