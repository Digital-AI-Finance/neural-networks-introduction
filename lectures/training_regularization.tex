\documentclass[8pt,aspectratio=169]{beamer}
\usetheme{Madrid}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{amssymb}

% Color definitions
\definecolor{mlblue}{RGB}{0,102,204}
\definecolor{mlpurple}{RGB}{51,51,178}
\definecolor{mllavender}{RGB}{173,173,224}
\definecolor{mllavender2}{RGB}{193,193,232}
\definecolor{mllavender3}{RGB}{204,204,235}
\definecolor{mllavender4}{RGB}{214,214,239}
\definecolor{mlorange}{RGB}{255, 127, 14}
\definecolor{mlgreen}{RGB}{44, 160, 44}
\definecolor{mlred}{RGB}{214, 39, 40}
\definecolor{mlgray}{RGB}{127, 127, 127}
\definecolor{lightgray}{RGB}{240, 240, 240}
\definecolor{midgray}{RGB}{180, 180, 180}

% Apply custom colors to Madrid theme
\setbeamercolor{palette primary}{bg=mllavender3,fg=mlpurple}
\setbeamercolor{palette secondary}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{palette tertiary}{bg=mllavender,fg=white}
\setbeamercolor{palette quaternary}{bg=mlpurple,fg=white}
\setbeamercolor{structure}{fg=mlpurple}
\setbeamercolor{section in toc}{fg=mlpurple}
\setbeamercolor{subsection in toc}{fg=mlblue}
\setbeamercolor{title}{fg=mlpurple}
\setbeamercolor{frametitle}{fg=mlpurple,bg=mllavender3}
\setbeamercolor{block title}{bg=mllavender2,fg=mlpurple}
\setbeamercolor{block body}{bg=mllavender4,fg=black}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{enumerate items}[default]
\setbeamersize{text margin left=5mm,text margin right=5mm}

\newcommand{\bottomnote}[1]{%
\vfill
\vspace{-2mm}
\textcolor{mllavender2}{\rule{\textwidth}{0.4pt}}
\vspace{1mm}
\footnotesize
\textbf{#1}
}

\title{Training Dynamics and Regularization}
\subtitle{Neural Networks for Finance}
\author{Neural Networks for Finance}
\institute{BSc Lecture Series}
\date{\today}

\begin{document}

\section{Training Dynamics}


% Slide 39: Batch Gradient Descent
\begin{frame}[t]{Batch Gradient Descent}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{Definition}

Use \textbf{all} training data to compute gradient:

$$\nabla \mathcal{L} = \frac{1}{m} \sum_{i=1}^{m} \nabla \ell(\hat{y}^{(i)}, y^{(i)})$$

Then update weights once.

\vspace{0.5em}
\textbf{Advantages:}
\begin{itemize}
\item[\textcolor{mlgreen}{+}] Stable gradient estimate
\item[\textcolor{mlgreen}{+}] Deterministic updates
\item[\textcolor{mlgreen}{+}] Guaranteed descent direction
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
\item[\textcolor{mlred}{-}] Slow for large datasets
\item[\textcolor{mlred}{-}] Must load all data in memory
\item[\textcolor{mlred}{-}] One update per full pass
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module3_training/charts/batch_vs_stochastic/batch_vs_stochastic.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/batch_vs_stochastic}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/batch_vs_stochastic}{\includegraphics[width=0.6cm]{../module3_training/charts/batch_vs_stochastic/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/batch_vs_stochastic}{\tiny\texttt{\textcolor{gray}{batch\_vs\_stochastic}}}
};
\end{tikzpicture}

\bottomnote{Compute gradient using the entire dataset}
\end{frame}

% Slide 40: Stochastic Gradient Descent
\begin{frame}[t]{Stochastic Gradient Descent (SGD)}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

Update after \textbf{each} single example:

$$\nabla \mathcal{L} \approx \nabla \ell(\hat{y}^{(i)}, y^{(i)})$$

One sample = one update.

\vspace{0.5em}
\textbf{Advantages:}
\begin{itemize}
\item[\textcolor{mlgreen}{+}] Very fast updates
\item[\textcolor{mlgreen}{+}] Can handle huge datasets
\item[\textcolor{mlgreen}{+}] Noise helps escape local minima
\item[\textcolor{mlgreen}{+}] Online learning possible
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
\item[\textcolor{mlred}{-}] Noisy gradient estimate
\item[\textcolor{mlred}{-}] Erratic convergence
\item[\textcolor{mlred}{-}] May not settle at minimum
\end{itemize}

\column{0.48\textwidth}
\textbf{Why ``Stochastic''?}

\vspace{0.5em}
Random sampling of training examples introduces randomness into gradient.

\vspace{0.5em}
\textbf{Expected Value:}

$$\mathbb{E}[\nabla \ell^{(i)}] = \nabla \mathcal{L}$$

On average, SGD points in the right direction.

\vspace{0.5em}
\textbf{Variance:}

Individual updates are noisy, but noise can help exploration.
\end{columns}
\bottomnote{Update after each single example}
\end{frame}

% Slide 41: Mini-Batch SGD
\begin{frame}[t]{Mini-Batch: The Sweet Spot}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

Use small batches of $B$ examples:

$$\nabla \mathcal{L} \approx \frac{1}{B} \sum_{i=1}^{B} \nabla \ell(\hat{y}^{(i)}, y^{(i)})$$

Typical $B$: 32, 64, 128, 256

\vspace{0.5em}
\textbf{Advantages:}
\begin{itemize}
\item[\textcolor{mlgreen}{+}] Reduced variance vs SGD
\item[\textcolor{mlgreen}{+}] GPU parallelization
\item[\textcolor{mlgreen}{+}] Reasonable memory usage
\item[\textcolor{mlgreen}{+}] Frequent updates
\end{itemize}

\textbf{The Modern Default}

\column{0.48\textwidth}
\textbf{Batch Size Trade-offs}

\vspace{0.5em}
\begin{tabular}{lll}
\toprule
\textbf{Size} & \textbf{Noise} & \textbf{Speed} \\
\midrule
1 (SGD) & High & Fast updates \\
32-256 & Medium & Best practice \\
Full batch & Low & Slow updates \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{Large Batch Issues:}
\begin{itemize}
\item May converge to sharp minima
\item Worse generalization
\item Need learning rate scaling
\end{itemize}
\end{columns}
\bottomnote{Balance between efficiency and noise}
\end{frame}

% Slide 42: Epochs
\begin{frame}[t]{Epochs: Full Passes Through Data}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

\textbf{Epoch} = one complete pass through all training data.

\vspace{0.5em}
\textbf{With Mini-Batches:}
\begin{itemize}
\item 10,000 samples
\item Batch size 100
\item 100 updates per epoch
\end{itemize}

\vspace{0.5em}
\textbf{Typical Training:}
\begin{itemize}
\item 10-1000 epochs
\item Monitor loss curve
\item Stop when converged
\end{itemize}

\column{0.48\textwidth}
\textbf{Training Timeline}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Stage} & \textbf{Behavior} \\
\midrule
Early epochs & Loss drops quickly \\
Middle epochs & Progress slows \\
Late epochs & Diminishing returns \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{When to Stop?}
\begin{itemize}
\item Loss stops improving
\item Validation loss increases (overfitting!)
\item Resource constraints
\end{itemize}
\end{columns}
\bottomnote{Training typically requires multiple epochs}
\end{frame}

% Slide 43: Training Curves
\begin{frame}[t]{Training Curves}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{What to Plot}

\begin{itemize}
\item Training loss vs. epoch
\item Validation loss vs. epoch
\item Learning rate schedule
\item Gradient norms (debugging)
\end{itemize}

\vspace{0.5em}
\textbf{Healthy Training:}
\begin{itemize}
\item Both losses decrease
\item Validation tracks training
\item Smooth convergence
\end{itemize}

\textbf{Warning Signs:}
\begin{itemize}
\item Training drops, validation rises
\item Loss oscillates wildly
\item Loss becomes NaN
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module3_training/charts/overfitting_curves/overfitting_curves.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/overfitting_curves}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/overfitting_curves}{\includegraphics[width=0.6cm]{../module3_training/charts/overfitting_curves/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/overfitting_curves}{\tiny\texttt{\textcolor{gray}{overfitting\_curves}}}
};
\end{tikzpicture}

\bottomnote{Monitoring progress during training}
\end{frame}

% Slide 44: Worked Example
\begin{frame}[t]{Worked Example: One Training Step}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Simple 2-2-1 Network}

\textbf{Given:}
\begin{itemize}
\item Input: $\mathbf{x} = (0.5, 0.8)^T$
\item Target: $y = 1$
\item Current weights (simplified)
\end{itemize}

\textbf{Forward Pass:}
\begin{align*}
z^{(1)} &= W^{(1)}\mathbf{x} + b^{(1)} \\
a^{(1)} &= \sigma(z^{(1)}) \\
z^{(2)} &= W^{(2)}a^{(1)} + b^{(2)} \\
\hat{y} &= \sigma(z^{(2)}) = 0.62
\end{align*}

\column{0.48\textwidth}
\textbf{Loss and Backward}

\textbf{Loss:}
$$\mathcal{L} = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(1 - 0.62)^2 = 0.072$$

\textbf{Backward Pass:}
\begin{align*}
\delta^{(2)} &= (0.62 - 1) \cdot 0.62(1-0.62) \\
&= -0.089
\end{align*}

\textbf{Weight Gradient:}
$$\frac{\partial \mathcal{L}}{\partial W^{(2)}} = \delta^{(2)} \cdot a^{(1)}$$

\textbf{Update:}
$$W^{(2)} \leftarrow W^{(2)} - 0.1 \cdot \nabla W^{(2)}$$
\end{columns}
\bottomnote{Following the numbers through one training step}
\end{frame}

% Slide 45: Vanishing Gradients
\begin{frame}[t]{The Vanishing Gradient Problem}
\begin{columns}[T]
\column{0.45\textwidth}
\textbf{The Problem}

Gradients shrink as they flow backward:

$$\delta^{(l)} \propto \prod_{k=l}^{L-1} \sigma'(z^{(k)})$$

\vspace{0.5em}
For sigmoid: $\sigma'(z) \leq 0.25$

Through 10 layers: gradient $\times 10^{-6}$

\vspace{0.5em}
\textbf{Symptoms:}
\begin{itemize}
\item Early layers don't learn
\item Deep networks fail to train
\item Loss plateaus quickly
\end{itemize}

\column{0.52\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module3_training/charts/vanishing_gradient_demo/vanishing_gradient_demo.pdf}
\end{center}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/vanishing_gradient_demo}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/vanishing_gradient_demo}{\includegraphics[width=0.6cm]{../module3_training/charts/vanishing_gradient_demo/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/vanishing_gradient_demo}{\tiny\texttt{\textcolor{gray}{vanishing\_gradient\_demo}}}
};
\end{tikzpicture}

\bottomnote{Deep networks: gradients can become vanishingly small}
\end{frame}

% Slide 46: Appendix Reference
\begin{frame}[t]{Full Mathematical Derivation}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{This Module: Intuition}

We covered:
\begin{itemize}
\item Why backprop works (chain rule)
\item How errors flow backward
\item Update rule intuition
\item Training dynamics
\end{itemize}

\vspace{0.5em}
\textbf{What We Skipped:}
\begin{itemize}
\item Full mathematical derivation
\item Matrix calculus details
\item Vectorized implementations
\item Automatic differentiation theory
\end{itemize}

\column{0.48\textwidth}
\textbf{Appendix B Contains:}

\vspace{0.5em}
\begin{enumerate}
\item Chain rule setup
\item Output layer error derivation
\item Hidden layer recursion formula
\item Complete gradient equations
\item Weight and bias gradients
\item Algorithm pseudocode
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{For the mathematically curious:}}

The appendix provides the rigorous derivation with all matrix calculus steps.
\end{columns}
\bottomnote{See Appendix B for complete backpropagation derivation}
\end{frame}

% ==================== SECTION 7: OVERFITTING (Slides 47-54) ====================

\section{Overfitting: The Enemy of Generalization}


% Slide 47: What Is Overfitting?
\begin{frame}[t]{What Is Overfitting?}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Definition}

\textbf{Overfitting:} When a model learns the training data too well, including its noise, and fails to generalize.

\vspace{0.5em}
\textbf{Analogy:}

A student who memorizes exam answers but doesn't understand the material.

\vspace{0.5em}
\textbf{Symptoms:}
\begin{itemize}
\item Training loss: very low
\item Test loss: high
\item Model is ``too confident''
\end{itemize}

\column{0.48\textwidth}
\textbf{Why It Happens}

\vspace{0.5em}
\textbf{Model Complexity:}
\begin{itemize}
\item Too many parameters
\item Can fit any training data perfectly
\item Including noise
\end{itemize}

\textbf{Limited Data:}
\begin{itemize}
\item Not enough examples
\item Training set not representative
\item Noise gets learned as signal
\end{itemize}

\textbf{Training Too Long:}
\begin{itemize}
\item Model eventually memorizes
\item Needs early stopping
\end{itemize}
\end{columns}
\bottomnote{When your model memorizes instead of learns}
\end{frame}

% Slide 48: Training vs Validation
\begin{frame}[t]{Training vs Validation Loss}
\begin{center}
\includegraphics[width=0.76\textwidth]{../module3_training/charts/overfitting_curves/overfitting_curves.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/overfitting_curves}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/overfitting_curves}{\includegraphics[width=0.6cm]{../module3_training/charts/overfitting_curves/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/overfitting_curves}{\tiny\texttt{\textcolor{gray}{overfitting\_curves}}}
};
\end{tikzpicture}

\bottomnote{Training loss decreases but validation increases}
\end{frame}

% Slide 49: The Backtest Trap
\begin{frame}[t]{Finance: The Backtest Trap}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{The Trap}

Every trading strategy looks good on historical data -- that's how you found it!

\vspace{0.5em}
\textbf{The Process:}
\begin{enumerate}
\item Try many strategies
\item Keep the one that worked best
\item By construction, it fits the past
\item Future performance? Unknown.
\end{enumerate}

\vspace{0.5em}
\textbf{Multiple Testing:}
\begin{itemize}
\item Try 1000 random strategies
\item Best one has Sharpe 2.0
\item Is it skill or luck?
\end{itemize}

\column{0.48\textwidth}
\textbf{Why Finance Overfits Easily}

\vspace{0.5em}
\begin{enumerate}
\item \textbf{Limited Data}
\begin{itemize}
\item 20 years = 5000 trading days
\item Few independent observations
\end{itemize}
\item \textbf{Low Signal-to-Noise}
\begin{itemize}
\item Markets are noisy
\item Easy to fit noise
\end{itemize}
\item \textbf{Non-Stationarity}
\begin{itemize}
\item Regimes change
\item Past may not predict future
\end{itemize}
\item \textbf{Look-Ahead Bias}
\begin{itemize}
\item Using future information
\item Subtle but deadly
\end{itemize}
\end{enumerate}
\end{columns}
\bottomnote{``Every strategy looks good on historical data''}
\end{frame}

% Slide 50: Why Finance Overfits Easily
\begin{frame}[t]{Why Finance Overfits So Easily}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Data Limitations}

\vspace{0.5em}
\begin{tabular}{ll}
\toprule
\textbf{Domain} & \textbf{Samples} \\
\midrule
ImageNet & 1,200,000 \\
MNIST & 60,000 \\
Stock returns (daily, 10y) & 2,520 \\
Stock returns (monthly, 50y) & 600 \\
Market crashes & $\sim$10 \\
\bottomrule
\end{tabular}

\vspace{0.5em}
\textbf{The Problem:}

Neural networks have thousands of parameters but only thousands of data points.

\column{0.48\textwidth}
\textbf{Signal vs Noise}

\vspace{0.5em}
\textbf{Image Classification:}
\begin{itemize}
\item A cat is always a cat
\item Signal is strong and consistent
\item R$^2$ can reach 99\%+
\end{itemize}

\textbf{Stock Prediction:}
\begin{itemize}
\item Returns are mostly random
\item Signal is weak and changing
\item R$^2$ of 1\% is excellent!
\end{itemize}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Implication:}}

Standard ML practices don't directly transfer to finance.
\end{columns}
\bottomnote{Limited data, high noise, non-stationary markets}
\end{frame}

% Slide 51: Detecting Overfitting
\begin{frame}[t]{Detecting Overfitting}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Train/Validation/Test Split}

\begin{enumerate}
\item \textbf{Training Set} (60-80\%)
\begin{itemize}
\item Used to fit weights
\end{itemize}
\item \textbf{Validation Set} (10-20\%)
\begin{itemize}
\item Used to tune hyperparameters
\item Monitor for overfitting
\end{itemize}
\item \textbf{Test Set} (10-20\%)
\begin{itemize}
\item Final evaluation only
\item Touch only once!
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textbf{Key Rule:}

Never use test data for decisions.

\column{0.48\textwidth}
\textbf{Warning Signs}

\vspace{0.5em}
\textbf{Overfitting Indicators:}
\begin{itemize}
\item Training loss $\ll$ validation loss
\item Validation loss starts increasing
\item Model predictions are ``too confident''
\item Performance degrades out-of-sample
\end{itemize}

\vspace{0.5em}
\textbf{For Finance:}
\begin{itemize}
\item Backtest Sharpe $\gg$ live Sharpe
\item Strategy ``stops working''
\item Drawdowns worse than expected
\end{itemize}
\end{columns}
\bottomnote{Always monitor out-of-sample performance}
\end{frame}

% Slide 52: Discussion Question 4
\begin{frame}[t]{Discussion Question}
\begin{center}
\Large
\textit{``How would you know if your stock prediction model is overfitting? What specific symptoms would you look for?''}
\end{center}

\vspace{1em}
\textbf{Consider:}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{In Training:}
\begin{itemize}
\item Training/validation gap
\item Validation loss trend
\item Prediction confidence
\end{itemize}

\column{0.48\textwidth}
\textbf{In Production:}
\begin{itemize}
\item Live vs. backtest performance
\item Regime sensitivity
\item Transaction cost impact
\end{itemize}
\end{columns}

\vspace{0.5em}
\textcolor{mlpurple}{\textbf{Best Practice:}} Always maintain a truly out-of-sample test set that you evaluate only once.
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 53: Preview: Regularization
\begin{frame}[t]{Preview: Fighting Overfitting}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Solutions (Module 4)}

\begin{enumerate}
\item \textbf{L1/L2 Regularization}
\begin{itemize}
\item Penalize large weights
\item Simpler models
\end{itemize}
\item \textbf{Dropout}
\begin{itemize}
\item Randomly disable neurons
\item Ensemble effect
\end{itemize}
\item \textbf{Early Stopping}
\begin{itemize}
\item Stop before overfitting
\item Use validation loss
\end{itemize}
\item \textbf{Data Augmentation}
\begin{itemize}
\item Create more training data
\item Finance: bootstrap?
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{Finance-Specific}

\vspace{0.5em}
\begin{enumerate}
\item \textbf{Walk-Forward Validation}
\begin{itemize}
\item Respect time ordering
\item Rolling windows
\end{itemize}
\item \textbf{Cross-Validation Variants}
\begin{itemize}
\item Purged CV
\item Combinatorial CV
\end{itemize}
\item \textbf{Ensemble Methods}
\begin{itemize}
\item Average multiple models
\item Reduce variance
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textcolor{mlpurple}{\textit{Module 4 will cover these in detail.}}
\end{columns}
\bottomnote{Module 4 will cover solutions: regularization, dropout, early stopping}
\end{frame}

% Slide 54: Module 3 Summary Diagram
\begin{frame}[t]{Training Pipeline Overview}
\begin{center}
\includegraphics[width=0.62\textwidth]{../module3_training/charts/module3_summary_diagram/module3_summary_diagram.pdf}
\end{center}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/module3_summary_diagram}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/module3_summary_diagram}{\includegraphics[width=0.6cm]{../module3_training/charts/module3_summary_diagram/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/module3_summary_diagram}{\tiny\texttt{\textcolor{gray}{module3\_summary\_diagram}}}
};
\end{tikzpicture}

\bottomnote{The complete neural network training process}
\end{frame}

% ==================== SECTION 8: SUMMARY (Slides 55-58) ====================

\section{Summary and Preview}


% Slide 55: Module 3 Key Takeaways
\begin{frame}[t]{Module 3: Key Takeaways}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{What We Learned}

\begin{enumerate}
\item \textbf{Loss Functions}
\begin{itemize}
\item Measure prediction error
\item MSE, cross-entropy
\item Define what ``good'' means
\end{itemize}
\item \textbf{Gradient Descent}
\begin{itemize}
\item Follow the slope downhill
\item Learning rate matters
\item Batch vs stochastic
\end{itemize}
\item \textbf{Backpropagation}
\begin{itemize}
\item Chain rule for credit assignment
\item Error flows backward
\item Enables efficient gradient computation
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\begin{enumerate}
\setcounter{enumi}{3}
\item \textbf{Training Dynamics}
\begin{itemize}
\item Epochs and batches
\item Monitoring with curves
\item Vanishing gradients
\end{itemize}
\item \textbf{Overfitting}
\begin{itemize}
\item Memorizing vs learning
\item Train/val/test split
\item Finance-specific challenges
\end{itemize}
\end{enumerate}

\vspace{0.5em}
\textbf{The Big Picture:}

We can now train neural networks. But making them work well requires more...
\end{columns}
\bottomnote{From measuring error to updating weights}
\end{frame}

% Slide 56: What We've Built
\begin{frame}[t]{What We've Built So Far}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Modules 1-3 Foundation}

\begin{enumerate}
\item \textbf{Module 1: Architecture}
\begin{itemize}
\item Perceptron basics
\item Linear decision boundaries
\item Limitations (XOR)
\end{itemize}
\item \textbf{Module 2: MLPs}
\begin{itemize}
\item Hidden layers
\item Non-linear activation
\item Universal approximation
\end{itemize}
\item \textbf{Module 3: Training}
\begin{itemize}
\item Gradient descent
\item Backpropagation
\item Overfitting awareness
\end{itemize}
\end{enumerate}

\column{0.48\textwidth}
\textbf{You Can Now:}

\vspace{0.5em}
\begin{itemize}
\item Explain how neural networks compute
\item Understand the training process
\item Recognize overfitting
\item Follow the math (or know where to look)
\end{itemize}

\vspace{0.5em}
\textbf{What's Missing:}

\begin{itemize}
\item Practical regularization
\item Real-world applications
\item Finance case studies
\item Modern developments
\end{itemize}
\end{columns}
\bottomnote{Modules 1-3: The complete neural network foundation}
\end{frame}

% Slide 57: Discussion Questions Review
\begin{frame}[t]{Key Questions for Reflection}
\textbf{Think about these as you move to Module 4:}

\vspace{0.5em}
\begin{enumerate}
\item \textbf{Loss vs. Profit:}

Why might minimizing MSE not maximize trading profit? What loss function would better align with trading goals?

\vspace{0.3em}
\item \textbf{Overfitting in Finance:}

With only 20 years of daily data, how many parameters can we safely learn? What's the ratio of samples to parameters you'd be comfortable with?

\vspace{0.3em}
\item \textbf{Non-Stationarity:}

If market regimes change, what does that mean for our training strategy? Should we weight recent data more heavily?

\vspace{0.3em}
\item \textbf{The Efficient Market Hypothesis:}

If markets are efficient, can neural networks find persistent patterns? What would success look like?
\end{enumerate}
\bottomnote{Reflect on the learning process}
\end{frame}

% Slide 58: Preview of Module 4
\begin{frame}[t]{Preview: Module 4}
\begin{center}
\Large
\textit{``Theory meets practice. How do we actually use neural networks in finance?''}
\end{center}

\vspace{1em}
\begin{columns}[T]
\column{0.48\textwidth}
\textbf{Coming Up:}
\begin{itemize}
\item Regularization techniques
\begin{itemize}
\item L1/L2, dropout, early stopping
\end{itemize}
\item Financial data challenges
\begin{itemize}
\item Non-stationarity, noise
\end{itemize}
\item Complete case study
\begin{itemize}
\item Stock prediction end-to-end
\end{itemize}
\end{itemize}

\column{0.48\textwidth}
\textbf{Also:}
\begin{itemize}
\item Modern architectures overview
\begin{itemize}
\item CNN, RNN, Transformers
\end{itemize}
\item Limitations and ethics
\begin{itemize}
\item Black-box decisions
\item Regulatory concerns
\end{itemize}
\item Future directions
\begin{itemize}
\item Where the field is heading
\end{itemize}
\end{itemize}
\end{columns}

\vspace{0.5em}
\textbf{Mathematical details: See Appendix B-D for derivations}
\bottomnote{Next: Regularization, case studies, and modern developments}
\end{frame}


\section{Opening}


% Slide 1: Title
\begin{frame}[plain]
\titlepage
\end{frame}

% Slide 2: The Journey So Far
\begin{frame}[t]{The Journey So Far}
\begin{columns}[T]
\begin{column}{0.6\textwidth}
\textbf{What We've Covered:}
\begin{itemize}
    \item \textcolor{mlpurple}{\textbf{Module 1:}} The Perceptron
    \begin{itemize}
        \item Single neuron, decision boundaries
        \item XOR limitation $\rightarrow$ AI Winter
    \end{itemize}
    \item \textcolor{mlblue}{\textbf{Module 2:}} Multi-Layer Perceptrons
    \begin{itemize}
        \item Hidden layers, activation functions
        \item Universal Approximation Theorem
    \end{itemize}
    \item \textcolor{mlorange}{\textbf{Module 3:}} Training
    \begin{itemize}
        \item Gradient descent, backpropagation
        \item Overfitting warning signs
    \end{itemize}
\end{itemize}
\end{column}
\begin{column}{0.38\textwidth}
\begin{center}
\textbf{The Foundation is Complete}\\[3mm]
\includegraphics[width=0.95\textwidth]{../module4_applications/charts/course_summary/course_summary.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/course_summary}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/course_summary}{\includegraphics[width=0.6cm]{../module4_applications/charts/course_summary/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/course_summary}{\tiny\texttt{\textcolor{gray}{course\_summary}}}
};
\end{tikzpicture}

\bottomnote{Perceptron $\rightarrow$ MLP $\rightarrow$ Training: The complete foundation}
\end{frame}

% Slide 3: The Final Question
\begin{frame}[t]{The Final Question}
\begin{center}
\vspace{1cm}
{\Large \textit{``How do we actually use this for stock prediction?''}}\\[1.5cm]
\textbf{From theory to practice:}
\begin{itemize}
    \item How do we prevent overfitting in finance?
    \item What makes financial data different?
    \item Does this actually work?
    \item What are the ethical considerations?
\end{itemize}
\end{center}
\bottomnote{Theory meets practice}
\end{frame}

% Slide 4: Module 4 Roadmap
\begin{frame}[t]{Module 4 Roadmap}
\begin{enumerate}
    \item \textbf{Historical Context} (2012-Present)
    \begin{itemize}
        \item The deep learning revolution
    \end{itemize}
    \item \textbf{Regularization Techniques}
    \begin{itemize}
        \item L1/L2, dropout, early stopping
    \end{itemize}
    \item \textbf{Financial Data Challenges}
    \begin{itemize}
        \item Non-stationarity, regime changes, biases
    \end{itemize}
    \item \textbf{Case Study: Stock Prediction}
    \begin{itemize}
        \item S\&P 500 direction prediction (realistic assessment)
    \end{itemize}
    \item \textbf{Modern Architectures}
    \begin{itemize}
        \item CNN, RNN, Transformer overview
    \end{itemize}
    \item \textbf{Limitations and Ethics}
    \begin{itemize}
        \item What neural networks can and cannot do
    \end{itemize}
\end{enumerate}
\bottomnote{From theory to real-world applications}
\end{frame}

% Slide 5: The Reality Check
\begin{frame}[t]{The Reality Check}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Theory is Clean:}
\begin{itemize}
    \item Data is stationary
    \item Training set represents test set
    \item Patterns persist
    \item No transaction costs
    \item Unlimited computing power
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Finance is Messy:}
\begin{itemize}
    \item Markets change constantly
    \item Past may not predict future
    \item Regime changes happen
    \item Costs eat into profits
    \item Latency matters
\end{itemize}
\end{column}
\end{columns}
\vspace{5mm}
\begin{center}
\textcolor{mlred}{\textbf{Warning:} Paper profits $\neq$ Real profits}
\end{center}
\bottomnote{``Theory is clean. Finance is messy.''}
\end{frame}

% ==================== SECTION 2: HISTORICAL CONTEXT (Slides 6-12) ====================

\section{Regularization: Fighting Overfitting}


% Slide 13: The Overfitting Problem Revisited
\begin{frame}[t]{The Overfitting Problem Revisited}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Recall from Module 3:}
\begin{itemize}
    \item Model learns training data too well
    \item Memorizes noise instead of patterns
    \item Fails on new, unseen data
\end{itemize}
\vspace{3mm}
\textbf{In Finance, This Is Critical:}
\begin{itemize}
    \item Backtest shows 40\% annual returns
    \item Live trading shows -15\%
    \item \textcolor{mlred}{This happens constantly}
\end{itemize}
\vspace{3mm}
\textbf{Why Module 4 Focuses on This:}
\begin{itemize}
    \item Overfitting is the \#1 failure mode
    \item Financial data is especially prone
    \item Must master regularization techniques
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{The Overfitting Gap:}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module4_applications/charts/early_stopping/early_stopping.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\includegraphics[width=0.6cm]{../module4_applications/charts/early_stopping/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\tiny\texttt{\textcolor{gray}{early\_stopping}}}
};
\end{tikzpicture}

\bottomnote{Overfitting: The greatest challenge in financial ML}
\end{frame}

% Slide 14: Why Finance Overfits
\begin{frame}[t]{Why Finance Overfits So Easily}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Limited Data:}
\begin{itemize}
    \item 20 years of daily data = 5,000 samples
    \item Compare to ImageNet: 14,000,000 images
    \item Regime changes reduce effective samples further
\end{itemize}
\vspace{3mm}
\textbf{High-Dimensional Features:}
\begin{itemize}
    \item 50 technical indicators $\times$ 10 lookbacks = 500 features
    \item More parameters than data points = guaranteed overfitting
\end{itemize}
\vspace{3mm}
\textbf{Low Signal-to-Noise:}
\begin{itemize}
    \item Daily stock returns: 95\%+ noise
    \item Real patterns are tiny
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module4_applications/charts/financial_data_challenges/financial_data_challenges.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/financial_data_challenges}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/financial_data_challenges}{\includegraphics[width=0.6cm]{../module4_applications/charts/financial_data_challenges/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/financial_data_challenges}{\tiny\texttt{\textcolor{gray}{financial\_data\_challenges}}}
};
\end{tikzpicture}

\bottomnote{Limited data, high noise, changing regimes}
\end{frame}

% Slide 15: Solution 1 - L2 Regularization
\begin{frame}[t]{L2 Regularization (Ridge)}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea:} Add penalty for large weights
$$\mathcal{L}_{reg} = \mathcal{L} + \frac{\lambda}{2}\|\mathbf{W}\|_2^2 = \mathcal{L} + \frac{\lambda}{2}\sum_i w_i^2$$

\textbf{Effect on Optimization:}
\begin{itemize}
    \item Original gradient: $\nabla_w \mathcal{L}$
    \item With L2: $\nabla_w \mathcal{L} + \lambda w$
    \item Weights decay toward zero each update
    \item Also called ``weight decay''
\end{itemize}
\vspace{3mm}
\textbf{Hyperparameter $\lambda$:}
\begin{itemize}
    \item $\lambda = 0$: No regularization
    \item $\lambda$ large: All weights $\rightarrow$ 0
    \item Typical: $10^{-4}$ to $10^{-2}$
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module4_applications/charts/regularization_effect/regularization_effect.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regularization_effect}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regularization_effect}{\includegraphics[width=0.6cm]{../module4_applications/charts/regularization_effect/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/regularization_effect}{\tiny\texttt{\textcolor{gray}{regularization\_effect}}}
};
\end{tikzpicture}

\bottomnote{Push weights to be small}
\end{frame}

% Slide 16: L2 Intuition
\begin{frame}[t]{L2 Intuition}
\textbf{Why Does Penalizing Large Weights Help?}
\vspace{3mm}
\begin{columns}[T]
\begin{column}{0.48\textwidth}
\textbf{Mathematical View:}
\begin{itemize}
    \item Large weights $\rightarrow$ extreme predictions
    \item Small changes in input $\rightarrow$ big output changes
    \item High sensitivity = memorization
    \item L2 forces smoother functions
\end{itemize}
\vspace{3mm}
\textbf{Bayesian View:}
\begin{itemize}
    \item L2 = Gaussian prior on weights
    \item Prior belief: weights should be small
    \item More data $\rightarrow$ prior matters less
\end{itemize}
\end{column}
\begin{column}{0.48\textwidth}
\textbf{Finance Analogy:}
\begin{itemize}
    \item Large weight on one feature = ``betting everything on one stock''
    \item Risky: what if that feature stops working?
    \item L2 forces diversification across features
    \item No single feature dominates the prediction
\end{itemize}
\vspace{3mm}
\textbf{Key Insight:}
\begin{itemize}
    \item L2 doesn't eliminate features
    \item Just reduces their influence
    \item All features contribute, but moderately
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Don't let any single feature dominate}
\end{frame}

% Slide 17: Solution 2 - L1 Regularization
\begin{frame}[t]{L1 Regularization (Lasso)}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea:} Penalty proportional to absolute value
$$\mathcal{L}_{reg} = \mathcal{L} + \lambda\|\mathbf{W}\|_1 = \mathcal{L} + \lambda\sum_i |w_i|$$

\textbf{Key Difference from L2:}
\begin{itemize}
    \item L1 pushes weights to \textbf{exactly zero}
    \item Creates sparse models (feature selection)
    \item Automatically identifies irrelevant features
\end{itemize}
\vspace{3mm}
\textbf{Why Sparsity?}
\begin{itemize}
    \item L1 gradient is $\pm\lambda$ (constant)
    \item Small weights get pushed to zero
    \item L2 gradient is $\lambda w$ (proportional)
    \item Small weights shrink slowly, never reach zero
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module4_applications/charts/l1_vs_l2/l1_vs_l2.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/l1_vs_l2}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/l1_vs_l2}{\includegraphics[width=0.6cm]{../module4_applications/charts/l1_vs_l2/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/l1_vs_l2}{\tiny\texttt{\textcolor{gray}{l1\_vs\_l2}}}
};
\end{tikzpicture}

\bottomnote{Push some weights to exactly zero: feature selection}
\end{frame}

% Slide 18: L1 vs L2 Comparison
\begin{frame}[t]{L1 vs L2: Comparison}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Property} & \textbf{L1 (Lasso)} & \textbf{L2 (Ridge)} \\
\midrule
Penalty term & $\lambda\sum|w_i|$ & $\frac{\lambda}{2}\sum w_i^2$ \\
Effect on weights & Some become exactly 0 & All shrink toward 0 \\
Feature selection & Yes (automatic) & No \\
Correlated features & Picks one arbitrarily & Shares weight among them \\
Sparsity & Sparse solutions & Dense solutions \\
Computation & Non-differentiable at 0 & Smooth, differentiable \\
\midrule
\textbf{Use when} & Few features matter & All features may matter \\
\bottomrule
\end{tabular}
\end{center}
\vspace{5mm}
\textbf{Elastic Net:} Combine both: $\lambda_1\|W\|_1 + \lambda_2\|W\|_2^2$\\
Best of both worlds for correlated features
\bottomnote{L1 for sparsity, L2 for shrinkage}
\end{frame}

% Slide 19: Solution 3 - Dropout
\begin{frame}[t]{Dropout: Random Deactivation}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Idea (Hinton et al., 2012):}
\begin{itemize}
    \item During training: randomly ``drop'' neurons
    \item Each neuron has probability $p$ of being set to 0
    \item Typically $p = 0.5$ for hidden, $p = 0.2$ for input
\end{itemize}
\vspace{3mm}
\textbf{Training:}
\begin{itemize}
    \item Each mini-batch sees different network
    \item Forces redundancy in learned features
    \item No neuron can become a ``crutch''
\end{itemize}
\vspace{3mm}
\textbf{Inference:}
\begin{itemize}
    \item Use all neurons (no dropout)
    \item Scale outputs by $(1-p)$ or use ``inverted dropout''
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module4_applications/charts/dropout_visualization/dropout_visualization.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/dropout_visualization}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/dropout_visualization}{\includegraphics[width=0.6cm]{../module4_applications/charts/dropout_visualization/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/dropout_visualization}{\tiny\texttt{\textcolor{gray}{dropout\_visualization}}}
};
\end{tikzpicture}

\bottomnote{``No single neuron becomes a crutch''}
\end{frame}

% Slide 20: Discussion Question 2
\begin{frame}[t]{Discussion Question}
\begin{center}
\vspace{1.5cm}
{\Large \textit{``How is dropout like diversifying a portfolio?''}}
\vspace{1cm}
\begin{itemize}
    \item What happens if you bet everything on one stock?
    \item What happens if a neural network relies on one neuron?
    \item How does diversification protect against failure?
    \item How does dropout force the network to diversify?
\end{itemize}
\end{center}
\bottomnote{Think-Pair-Share: 3 minutes}
\end{frame}

% Slide 21: Dropout Intuition
\begin{frame}[t]{Dropout Intuition: Ensemble Learning}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Ensemble Interpretation:}
\begin{itemize}
    \item Network with $n$ neurons has $2^n$ possible subnetworks
    \item Dropout trains all subnetworks simultaneously
    \item Each mini-batch samples a different subnetwork
    \item Final prediction: average of all subnetworks
\end{itemize}
\vspace{3mm}
\textbf{Why Ensembles Work:}
\begin{itemize}
    \item Different models make different errors
    \item Averaging reduces variance
    \item More robust to noise
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\textbf{Finance Parallel:}
\begin{itemize}
    \item One analyst: high variance predictions
    \item Committee of analysts: more stable
    \item Dropout = ``committee of networks''
\end{itemize}
\vspace{3mm}
\textbf{Practical Notes:}
\begin{itemize}
    \item Dropout slows convergence
    \item Needs more epochs to train
    \item Don't use with batch normalization (debate)
    \item Less common in CNNs today
\end{itemize}
\end{column}
\end{columns}
\bottomnote{Dropout approximates training an ensemble of networks}
\end{frame}

% Slide 22: Solution 4 - Early Stopping
\begin{frame}[t]{Early Stopping}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{The Simplest Regularization:}
\begin{itemize}
    \item Monitor validation loss during training
    \item Stop when validation loss stops improving
    \item Use the model from the best epoch
\end{itemize}
\vspace{3mm}
\textbf{Implementation:}
\begin{itemize}
    \item Track best validation loss
    \item Patience: wait $k$ epochs before stopping
    \item Save checkpoint at each improvement
    \item Restore best checkpoint at end
\end{itemize}
\vspace{3mm}
\textbf{Why It Works:}
\begin{itemize}
    \item Early epochs: learning real patterns
    \item Later epochs: memorizing training noise
    \item Sweet spot: generalization peak
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module4_applications/charts/early_stopping/early_stopping.pdf}
\end{center}
\textbf{Typical patience:} 5-20 epochs
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\includegraphics[width=0.6cm]{../module4_applications/charts/early_stopping/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/early_stopping}{\tiny\texttt{\textcolor{gray}{early\_stopping}}}
};
\end{tikzpicture}

\bottomnote{Stop training when validation loss stops improving}
\end{frame}

% Slide 23: Walk-Forward Validation
\begin{frame}[t]{Walk-Forward Validation for Time Series}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
\textbf{Standard Cross-Validation: WRONG for Time Series}
\begin{itemize}
    \item Random splits leak future information
    \item Model sees 2024 data, predicts 2023
    \item Guaranteed overfitting
\end{itemize}
\vspace{3mm}
\textbf{Walk-Forward Validation:}
\begin{itemize}
    \item Train on [2010-2015], validate on [2016]
    \item Train on [2010-2016], validate on [2017]
    \item Train on [2010-2017], validate on [2018]
    \item Always: train on past, validate on future
\end{itemize}
\vspace{3mm}
\textbf{Anchored vs Rolling Window:}
\begin{itemize}
    \item Anchored: always start from same date
    \item Rolling: fixed window slides forward
\end{itemize}
\end{column}
\begin{column}{0.43\textwidth}
\begin{center}
\includegraphics[width=0.98\textwidth]{../module4_applications/charts/walk_forward_validation/walk_forward_validation.pdf}
\end{center}
\end{column}
\end{columns}

% Quantlet branding (auto-generated)
\begin{tikzpicture}[remember picture,overlay]
% Logo (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.6cm,opacity=1.0] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/walk_forward_validation}{\includegraphics[width=0.8cm]{../../quantlet_tools/logo/quantlet.png}}
};
% QR Code (clickable)
\node[anchor=south east,xshift=-1.3cm,yshift=0.6cm,opacity=0.8] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/walk_forward_validation}{\includegraphics[width=0.6cm]{../module4_applications/charts/walk_forward_validation/qr_code.png}}
};
% URL text (clickable)
\node[anchor=south east,xshift=-0.3cm,yshift=0.2cm] at (current page.south east) {
  \href{https://github.com/QuantLet/neural-networks-introduction/tree/main/walk_forward_validation}{\tiny\texttt{\textcolor{gray}{walk\_forward\_validation}}}
};
\end{tikzpicture}

\bottomnote{Train on past, validate on future (never the reverse)}
\end{frame}

% Slide 24: Regularization Summary
\begin{frame}[t]{Fighting Overfitting: Summary}
\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Technique} & \textbf{Mechanism} & \textbf{When to Use} \\
\midrule
L2 (Ridge) & Penalize large weights & Always (as baseline) \\
L1 (Lasso) & Push weights to zero & Feature selection needed \\
Dropout & Random neuron deactivation & Deep networks \\
Early Stopping & Stop before overfitting & Always (free) \\
Walk-Forward & Time-respecting validation & Time series only \\
\bottomrule
\end{tabular}
\end{center}
\vspace{5mm}
\textbf{Practical Recommendation for Finance:}
\begin{enumerate}
    \item Always use walk-forward validation
    \item Start with L2 regularization
    \item Add early stopping (patience=10)
    \item Try dropout (0.2-0.5) for deep networks
    \item Use L1 if you need interpretable feature importance
\end{enumerate}
\bottomnote{Multiple defenses against overfitting}
\end{frame}

% ==================== SECTION 4: FINANCIAL DATA CHALLENGES (Slides 25-32) ====================

\end{document}
